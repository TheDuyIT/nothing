name,par,href,summary,goals,prerequisites
A* search,Symbolic AI,https://metacademy.org/graphs/concepts/a_star_search,"A* search is a search algorithm which makes use of a heuristic which estimates the distance from a node to a goal state. It iteratively expands nodes in order by their path cost plus their heuristic value. If the heuristic satisfies certain ""admissibility"" conditions, then A* is guaranteed to find an optimal path.","Know some terminology: evaluation function, heuristic function. Know what A* search uses as its evaluation function. What is an admissible heuristic?. Show that A* search with an admissible heuristic finds an optimal solution. Give an example where it finds a suboptimal solution when the heuristic is inadmissible. Give some examples of admissible heuristics. If possible, explain how they can be interpreted in terms of loosening problem constraints.. How does A* behave if the heuristic returns the exact distance to the goal? If it always returns zero?. What does it mean for an admissible heuristic A to dominate another admissible heuristic B, and why should A be preferred in this case?","search problems,uninformed search"
abstract data types,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/abstract_data_types,"An abstract data type is a specification of the behavior of a data type (often a class) without reference to its underlying implementation. Data abstraction is one of the fundamental principles of software engineering, since it facilitates the decomposition of a problem into independent chunks and allows the implementation of the data type to change without breaking the client code.","Understand the benefits of data abstraction:\nthat one need not understand how the data type is implemented in order to use it\nit gives more flexibility for changing the implementation, so long as it obeys the specification\nthat the user can replace one implementation with another if it proves to be more effective. In a programming language such as Java, C++, or Python, understand how the specifications of the standard container types abstract away the implementation. Be able to implement an abstract data type that meets a specification. Be able to design a specification of an ADT based on the anticipated needs of the user (e.g. what should be included in the specification and what should be left to the implementation?)","specifications (programming),classes (programming)"
AdaBoost,Machine Learning,https://metacademy.org/graphs/concepts/ada_boost,"AdaBoost is an example of a boosting algorithm, where the goal is to take a ""weak classifier"" (one which performs slightly above chance) and make it into a ""strong classifier"" (one which performs well on the training set). It is widely used in data mining, especially in conjunction with decision trees, because of its simplicity and effectiveness.","Understand the steps in the AdaBoost algorithm. Be aware of the underlying motivation, namely taking a ""weak classifier"" which performs slightly above chance, and producing a ""strong classifier,"" which classifies the whole training set correctly.",
adaptive rejection sampling,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/adaptive_rejection_sampling,"Adaptive rejection sampling (ARS) is a technique used to automatically choose and refine an envelope distribution, q(z), for rejection sampling of distribution p(z). ARS initially forms an envelope distribution, q(z), as a piecewise combination of intersecting line segments that outline p(z). These line segments are the tangents of log(p(z)) evaluated along a set of initial grid points and are guaranteed to enclose p(z) if p(z) is log-concave (the derivative of log(p(z)) is nonincreasing) -- this property holds for a large number of distributions. ARS then proceeds like rejection sampling, though if a sampled point is rejected at point x* then the tangent of log(p(x*)) is added to the envelope distributions, which shrinks the proposal distribution around the true distribution and leads to fewer rejected samples later on.",Be able to implement the adaptive rejection sampling algorithm.. What is the advantage compared to standard rejection sampling?,"rejection sampling,exponential distribution"
Akaike information criterion,Machine Learning,https://metacademy.org/graphs/concepts/akaike_information_criterion,"The Akaike Information Criterion (AIC) is a frequentist model selection criterion typically used to regularize maximum likelihood estimators. The AIC provides a relative estimate of the quality of the tested models (so it is necessary to report the AIC differences between models), but the AIC will not indicate if all tested models poorly describe the data. The quality score is a relative estimate of the kl-divergence between the given model and the true model.",Know the definition of the AIC.. How is the AIC justified in terms of performance on held-out data?,"maximum likelihood,generalization"
alpha-beta pruning,Symbolic AI,https://metacademy.org/graphs/concepts/alpha_beta_pruning,Alpha-beta pruning is a method for improving the efficiency of minimax search by pruning out branches which can't possibly be played by optimal players. It works by maintaining the best value either player would be able to achieve by playing a move which does not lead to the current part of the search tree. A branch of the search tree can be pruned if its value is determined to be outside the range of these values.,"What are the values alpha and beta maintained by the alpha-beta pruning procedure?. When is a subtree pruned, and how do we know it's safe to prune the subtree?. Why is it advantageous to explore the (seemingly) best move first at each node?",minimax search
analyzing recursive algorithms,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/analyzing_recursive_algorithms,"Many important algorithms, such as binary search and merge sort, are most simply described in terms of a recursive procedure. Analyzing the running time of such algorithms requires solving recurrence equations. There are tricks for solving or analyzing these equations, including the Master Theorem, which covers most of the common cases.","Be able to write down the recurrence equation which gives the running time of a recursive algorithm. Be able to solve some common kinds of recurrences, or at least give bounds on their solutions. Know the statement of the Master Theorem","asymptotic complexity,recursion (programming)"
annealed importance sampling,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/annealed_importance_sampling,"Annealed importance sampling (AIS) is a Monte Carlo algorithm based on sampling from a sequence of distributions which interpolate between a tractable initial distribution and the intractable target distribution. It returns a set of weighted samples, and in the limit of infinitely many intermediate distributions, the variance of the weights approahces zero. The most common use is in estimating partition functions.",Know the steps the AIS algorithm.. Know how to obtain weighted samples and estimates of the partition function from the algorithm's outputs.. Show that the variance of the weights approaches zero in the limit of infinitely many intermediate distributions (assuming the transition operator returns perfect samples).,"Markov chain Monte Carlo,importance sampling,inference in MRFs,Central Limit Theorem"
arc consistency,Symbolic AI,https://metacademy.org/graphs/concepts/arc_consistency,"Arc consistency is a heuristic for pruning out possible values for the variables in a CSP which cannot possibly be part of a consistent solution. The set of values for all variables is arc consistent if for each value d of a variable X, and each neighbor Y of X, there is some value of Y which is consistent with d (in terms of the constraint between X and Y). The AC3 procedure iteratively removes values until the graph is arc consistent.",Know what it means for the domains of the variables in a CSP to be arc consistent. Know how the AC3 algorithm enforces arc consistency. Analyze the running time of AC3. What is the tradeoff between using AC3 versus simple forward checking as part of the backtracking procedure?,"constraint satisfaction problems,queue,asymptotic complexity"
asymptotic complexity,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/asymptotic_complexity,"The asymptotic (time) complexity of an algorithm refers to the scaling of the running time of an algorithm as a function of the input size. The time complexity is typically given in terms of big-O notation, where the running time is bounded up to a multiplicative constant.","Know the precise definitions of big-O, big-Theta, and big-Omega notations. Be able to analyze the asymptotic complexity of simple algorithms. Be aware of the distinction between average case and worst case complexity",
asymptotics of maximum likelihood,Frequentist Statistics,https://metacademy.org/graphs/concepts/asymptotics_of_maximum_likelihood,"Under certain regularity conditions, the maximum likelihood estimator is consistent, i.e. it asymptotically approaches the true value. Its sampling distribution (when rescaled appropriately) approaches a normal distribution whose variance is determined by the Fisher information. Because of the Cramer-Rao bound, this is the best we can do. The asymptotic analysis is useful for constructing confidence intervals for parameter estimates.","Understand basic properties of maximum likelihood estimators:\nthey are consistent (they approach the correct value in the limit)\nasymptotically, their sampling distribution (rescaled appropriately) approaches a normal distribution whose variance is the inverse Fisher information\nthey are efficient (no unbiased estimator has smaller variance asymptotically). Note: for the multivariate version of the asymptotic normality result, you'll want to know about the Fisher information matrix .","maximum likelihood,Fisher information,Gaussian distribution"
automatic differentiation,Optimization,https://metacademy.org/graphs/concepts/automatic_differentiation,"Automatic differentiation is a method for computing partial derivatives or directional derivatives of a function, given a program which computes the function. It is based on the chain rule for derivatives, but shares computations between different entries of the result.","Be able to compute directional derivatives using forward accumulation. Be able to compute gradients using reverse accumulation. Why does reverse accumulation, but not forward accumulation, require storing the computation graph?","partial derivatives,Chain Rule"
autoregressive generative models,Uncategorized,https://metacademy.org/graphs/concepts/autoregressive_generative_models,"Autoregressive generative models implicitly define a distribution over sequences using the Chain Rule for Conditional Probability, whereby in each step the distribution of the next sequence element is predicted given the previous elements. The main autoregressive architectures are RNNs and causal conv nets. Important examples include PixelRNN, PixelCNN, and WaveNet.",Know how to train an autoregressive generative model with maximum likelihood.. Know how to generate samples from a trained autoregressive model.. Be familiar with both convolutional and RNN autoregressive architectures.. Understand what causal convolution is and why it's important.. Be able to reason about the tradeoffs of convolutional vs. recurrent autoregressive models.,"neural probabilistic language models,convolutional neural nets,recurrent neural networks"
AVL trees,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/avl_trees,"AVL trees, named after their inventors, are a kind of binary search tree which automatically maintains balance in order to support efficient search, insertion, and deletion. This data structure is based on maintaining the invariant that the heights of the left and right subtrees of a given node can differ by at most 1.",Know what invariant is maintained by an AVL tree. Know what additional information must be stored in order to efficiently maintain the invariant. Know what the rotation operator refers to and why it preserves the ordering property. Be able to implement the insertion and deletion operations such that the invariant is maintained. Analyze the running times of insertion and deletion,binary search trees
Axiom of Choice,"Logic,Set Theory",https://metacademy.org/graphs/concepts/axiom_of_choice,"The Axiom of Choice states that for any set A of sets, there exists a choice function which picks a single element of each x in A. While intuitive, it has some surprising consequences, such as the Banach-Tarski Paradox. It is logically independent of the Zermelo-Frankl axioms, so one may choose whether or not to include it.","Know the statement of the Axiom of Choice. Be aware that it is controversial (e.g. because of the Banach-Tarski paradox), and it is often noted specifically when a theorem depends on it. Be aware that it is independent of the Zermelo-Frankl axioms",Zermelo-Frankl axioms
backpropagation,Machine Learning,https://metacademy.org/graphs/concepts/backpropagation,"Backpropagation is the standard algorithm for training supervised feed-forward neural nets. More precisely, it isn't actually a learning algorithm, but a way of computing the gradient of the loss function with respect to the network parameters. Mathematically, it's just an instance of the chain rule for derivatives, but it has an intuitive interpretation in terms of passing messages between the units.","Know what the computation graph represents, and be able to draw it for simple neural net architectures.. Be able to derive the backprop computations for a neural net architecture.. Be able to write the backprop computations in matrix form.","stochastic gradient descent,feed-forward neural nets,Chain Rule"
backpropagation for second-order methods,Machine Learning,https://metacademy.org/graphs/concepts/backpropagation_second_order,"Backpropagation is normally used to propagate first-order derivatives (gradients). However, it can also be used to propagate second-order derivatives, at least approximately.",,"backpropagation,matrix inverse"
bagging,Machine Learning,https://metacademy.org/graphs/concepts/bagging,Bagging is a technique for reducing the variance of a learning algorithm by averaging the predictions obtained from random resamplings of the training data. It can improve the performance of unstable algorithms such as decision trees.,Know what the bagging procedure is.. What is the motivation behind bagging? For what sorts of algorithms would you expect it to improve performance?,"decision trees,generalization"
bases,Linear Algebra,https://metacademy.org/graphs/concepts/bases,"A basis is a set of linearly independent vectors that define a coordinate system for a vector space, where the basis vectors span the entire vector space (meaning that any vector in this space can be represented as a linear combination of the basis vectors).","Know the definition of linearly independent vectors. Be able to determine if a set of vectors is linearly independent. Know the definition of a basis. Understand how bases can be used as coordinate systems. Show that all bases for a given space have the same number of elements (and hence, dimension is a well-defined concept). Know the definition of dimension",subspaces
basis function expansions,Machine Learning,https://metacademy.org/graphs/concepts/basis_function_expansions,"A basis function expansion augments/replaces the attributes of a dataset with transformations of these attributes. For instance, given an input attribute X, a basis function expansion could map this attribute to three features: 1, X, X^2---a ""polynomial basis."" This mapping allows various learning algorithms and statistical procedures to capture nonlinear trends in the data while still using linear models to analyze these transformed attributes. For instance, using the polynomial basis functions with linear regression allows linear regression to find polynomial (nonlinear) trends in the data; this is commonly called ""polynomial regression."" The process of selecting the particular mapping (basis functions) is typically referred to as ""feature engineering.""",,linear regression
Baum-Welch algorithm,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/baum_welch_algorithm,"The Baum-Welch algorithm is an algorithm for maximum likelihood learning in hidden Markov models (HMMs). It is a special case of expectation-maximization (EM), and alternates between inferring the posterior marginals and maximizing the expected log-likelihood given those posterior marginals.",Derive the Baum-Welch algorithm as a special case of EM.,"hidden Markov models,Expectation-Maximization algorithm,forward-backward algorithm"
Bayes Ball,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/bayes_ball,"D-separation gives a way of determining conditional independence properties of a Bayes net from the graphical representation, but unfortunately the definition itself doesn't give a practical algorithm. Bayes ball is an efficient algorithm for computing d-separation by passing simple messages between nodes of the graph. The name ""Bayes Ball"" stems from the idea of balls bouncing around a directed graph, where if a ball cannot bounce between two nodes then they are [conditionally] independent.","Memorize the ten Bayes' ball ""bouncing rules"" (noting symmetries makes this easy).. You should be able to prove the validity of the bouncing rules using the definition of conditional probability.. Understand how ""marking"" the nodes depending on the direction the ball is traveling when visiting the node leads to a linear-time algorithm for determining the [conditional] independencies in a Bayes net.",d-separation
Bayes net parameter learning,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/bayes_net_parameter_learning,"The parameters of a Bayes net can be estimated using maximum likelihood. In the most general parameterization, when the data are fully observed, the ML estimation problem decomposes into independent subproblems associated with each CPT.","Know how to determine the maximum likelihood estimate for the parameters in a Bayes net when all of the variables are fully observed.. In particular, understand why the problem decomposes into independent parameter learning subproblems associated with each CPT, and why the assumption of full observations is necessary.\nThe decomposition into independent terms isn't just used for maximum likelihood estimation -- it's the basis behind a number of other algorithms for learning Bayes nets.. How does the maximum likelihood solution change when parameters are shared between different CPTs?","maximum likelihood,Bayesian networks,optimization problems"
Bayes net structure learning,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/bayes_net_structure_learning,"If the structure of a Bayes net (in particular, the set of edges) is not known, we may wish to learn it from data. This requires trading off the degree of fit with the complexity of the network. The Bayesian score gives a simple and efficient way of evaluating Bayes net structures.","How are the requirements for structure learning different if your goal is knowledge discovery or density modeling?\nIn particular, if the goal is density modeling, why might you want a graph which is sparser than the true one?. Why isn't the maximum likelihood score appropriate for evaluating graph structures?. Be able to derive the Bayesian (marginal likelihood) score for evaluating Bayes nets.\nWhat assumptions about the prior are needed for the solution to have a convenient closed form?\nThe Bayesian score implicitly penalizes complex graphs. Why does this happen? (Hint: it's not the prior over graph structures, as you might expect!). Give an example where the graph is not identifiable, i.e. there are multiple graph structures which yield the same set of distributions.. Give an example where the graph structure is identifiable.","Bayesian estimation of Bayes net parameters,Bayesian model comparison,generalization,gamma function"
Bayes' rule,"Bayesian Statistics,Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/bayes_rule,"Bayes' rule is a formula for combining prior beliefs with observed evidence to obtain a ""posterior"" distribution. It is central to Bayesian statistics, where one infers a posterior over the parameters of a statistical model given the observed data.","Know the statement of Bayes' Rule. Be able to use it to combine prior information with evidence. Derive Bayes' Rule from the definition of conditional probability. Know terminology: prior, posterior. Be able to reason intuitively about Bayes' Rule in terms of odds ratios",conditional probability
Bayesian decision theory,Machine Learning,https://metacademy.org/graphs/concepts/bayesian_decision_theory,"When we use Bayesian parameter estimation techniques, often it's because we want to make a decision. In Bayesian decision theory, we make the choice which minimizes the expected loss under the posterior. When we compute a statistic like the mode or the mean of the predictive distribution, this can be interpreted as the decision theoretic solution under a particular loss function.",Know how the optimal decision is defined (in terms of minimizing expected loss with respect to the posterior). Derive the form of the estimator for some particular loss functions:\n0-1 loss\nquadratic loss\nabsolute loss,"Bayesian parameter estimation,loss function,expectation and variance"
Bayesian estimation of Bayes net parameters,"Bayesian Statistics,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/bayesian_estimation_bayes_net_params,Bayesian parameter estimation techniques can be applied to learning Bayes net parameters. This leads to more stable estimates in situations with limited data and can improve generalization performance.,"Derive the formulas for Bayesian estimation of Bayes net parameters, and for the predictive distribution over new data, in the simplest case where all variables are fully observed.. In particular, see why posterior inference and prediction can both decompose into independent problems associated with each CPT. What has to be true of the prior for the problem to decompose this way?. Be able to represent the Bayesian parameter estimation problem itself as a Bayes net, i.e. build a Bayes net where the parameters and data are represented as separate sets of variables. This ties together the problems of learning and inference.","Bayes net parameter learning,Bayesian parameter estimation"
Bayesian information criterion,Frequentist Statistics,https://metacademy.org/graphs/concepts/bayesian_information_criterion,"The Bayesian information criterion (BIC) is a rough approximation to the marginal likelihood, based on the asymptotic behavior of the Laplace approximation as more data is observed.",Know the formula for the BIC. Derive the formula in terms of the Laplace approximation,"Bayesian model comparison,Laplace approximation"
Bayesian linear regression,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_linear_regression,"By interpreting linear regression as a Bayesian model, we can automatically infer the prior variance and the noise variance, and make calibrated predictions. Bayesian linear regression is a useful component in fancier probabilistic models.","Know the form of the Bayesian linear regression model. Be familiar with visualizations of the prior, evidence, and posterior. Derive the predictive distribution. Be familiar with visualizations of the posterior distribution. Be able to infer the variance parameters (with the evidence approximation or a conjugate prior)","linear regression as maximum likelihood,Bayesian parameter estimation,Bayesian parameter estimation: multivariate Gaussians,computations on multivariate Gaussians"
Bayesian logistic regression,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_logistic_regression,A Bayesian version of logistic regression.,Know the form of the Bayesian logistic regression model. Be able to estimate the parameters of the model computationally (e.g. with the Laplace approximation or EP). Be able to approximate the predictive distribution computationally (e.g. with sampling or the probit approximation),"logistic regression,Bayesian parameter estimation,Bayesian linear regression,Laplace approximation,evidence approximation,Bayesian decision theory,probit regression"
Bayesian model averaging,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_model_averaging,"In model selection, we typically select a single ""best"" model from a set of candidate models (based upon some selection criteria, such as an AIC score) and then use this model for prediction. Instead of selecting a single ""best"" model and using it for prediction, Bayesian Model Averaging BMA uses a weighted average of each model's individual prediction for the final predicted value, where the weight is the posterior probability of the model given the data.",,Bayesian model comparison
Bayesian model comparison,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_model_comparison,"The framework of Bayesian model comparison evaluates probabilistic models based on the marginal likelihood, or the probability they assign a dataset with all the parameters marginalized out. The marginalization of model parameters implements a sort of ""Occam's razor"" effect. Marginal likelihoods can also be used to compute a posterior over model classes using Bayes' rule.","Know what the marginal likelihood of a model refers to. Motivate the marginal likelihood in terms of Bayes factors. Understand the basis for the ""Bayesian Occam's razor"" effect (hint: it's not primarily a result of assigning lower prior probability to models with more parameters, as many people believe). Derive the Bayes factor for a simple example (e.g. a beta-Bernoulli model)","Bayesian parameter estimation,Bayes rule"
Bayesian naive Bayes,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/bayesian_naive_bayes,A Bayesian version of naive Bayes in which we place a prior distribution on the latent class assignment parameter.,,"naive Bayes,Bayesian parameter estimation: multinomial distribution"
Bayesian networks,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/bayesian_networks,"Bayesian networks are a graphical formalism for representing the structure of a probabilistic model, i.e. the ways in which the random variables may depend on each other. Intuitively, they are good at representing domains with a causal structure, and the edges in the graph determine which variables directly influence which other variables. They can be equivalently viewed as representing a factorization structure of the joint probability distribution, or as encoding a set of conditional independence assumptions about the distribution.",,"random variables,conditional independence,Bayes rule"
Bayesian parameter estimation,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_parameter_estimation,"In the Bayesian framework, we treat the parameters of a statistical model as random variables. The model is specified by a prior distribution over the values of the variables, as well as an evidence model which determines how the parameters influence the observed data. When we condition on the observations, we get the posterior distribution over parameters. The term ``Bayesian parameter estimation'' is deceptive, because often we can skip the parameter estimation step entirely. Rather, we integrate out the parameters and directly make predictions about future observables.","Know what the terms ""prior"" and ""likelihood function"" refer to. Be able to compute the posterior distribution using Bayes' Rule. Know what the posterior predictive distribution is and how to compute it analytically for a simple example (e.g. a beta-Bernoulli model). What is a conjugate prior, and why is it useful?. Why can the posterior distribution be given in terms of pseudocounts when a conjugate prior is used?. What is the maximum a-posteriori (MAP) approximation? Give an example where the predictions differ between the MAP parameters and the posterior predictive distribution.","Bayes rule,multivariate distributions,conditional distributions,expectation and variance,beta distribution"
Bayesian parameter estimation in exponential families,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayes_param_exp_fam,"Exponential families are convenient for Bayesian parameter estimation because the conjugate priors often have a convenient form, and there is a simple form for the posterior.","How do you derive the conjugate prior for an exponential family distribution?. Show that the posterior can be computed in terms of the sufficient statistics.. Work through a simple example, such as the beta-Bernoulli model.","exponential families,Bayesian parameter estimation"
Bayesian parameter estimation: Gaussian distribution,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_parameter_estimation_gaussian,"Using the Bayesian framework, we can infer the mean parameter of a Gaussian distribution, the scale parameter, or both. Since Gaussians are widely used in probabilistic modeling, the computations that go into this are common motifs in Bayesian machine learning more generally.","Derive the conjugate prior, the posterior distribution, and the posterior predictive distribution for a Gaussian distribution with unknown mean and known variance. Do the same for known mean and unknown variance. Do the same for unknown mean and unknown variance","Gaussian distribution,Bayesian parameter estimation,gamma distribution,Student-t distribution"
Bayesian parameter estimation: multinomial distribution,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayesian_parameter_estimation_multinomial,"Suppose we observe a set of draws from a multinomial distribution with unknown parameters and we're trying to predict the distribution over subsequent draws. If we put a Dirichlet prior over the probabilities, we can analytically integrate out the parameters to get the posterior predictive distribution. This has a very simple form: adding fake counts and then normalizing. These ideas are used more generally in Bayesian models involving discrete variables.",Know how the Dirichlet distribution is defined and what the parameters represent. Know what the Dirichlet-multinomial model is. Derive the posterior distribution and posterior predictive distribution,"multinomial distribution,Bayesian parameter estimation"
Bayesian parameter estimation: multivariate Gaussians,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bayes_param_multivariate_gaussian,"Using the Bayesian framework, we can infer the posterior over the mean vector of a multivariate Gaussian, the covariance matrix, or both. Since multivariate Gaussians are widely used in probabilistic modeling, the computations that go into this are common motifs in Bayesian machine learning more generally.",Derive the conjugate prior and the posterior distribution for a multivariate Gaussian with unknown mean and known covariance. Do the same for known mean and unknown covariance. Do the same for unknown mean and unknown covariance,"information form for multivariate Gaussians,computations on multivariate Gaussians,Bayesian parameter estimation,Wishart distribution,Student-t distribution"
Bayesian PCA,Machine Learning,https://metacademy.org/graphs/concepts/bayesian_pca,"By formulating PCA as a Bayesian model, we can auotmatically choose a latent dimensionality by maximizing the (approximate) marginal likelihood of the model.",Know the definition of the Bayesian PCA model. How can it be used to select the dimension of the latent space?. Know of a way to approximate the marginal likelihood (e.g. the evidence approximation),"probabilistic PCA,Bayesian linear regression,evidence approximation,Bayesian parameter estimation: multivariate Gaussians,Laplace approximation"
Bellman equations,Reinforcement Learning,https://metacademy.org/graphs/concepts/bellman_equations,"The Bellman equations are a system of equations that provide a recursive definition of optimality associated with dynamic programming. Informally, the Bellman equations state that optimality is achieved by taking an optimal first action and recursively taking optimal subsequent actions.",,"optimization problems,functions of several variables"
Bellman-Ford algorithm,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/bellman_ford_algorithm,"The Bellman-Ford algorithm is an algorithm for computing the shortest path between two points in a weighted graph. Unlike with Dijkstra's algorithm, the edge weights are allowed to be negative. On the down side, it has a larger running time, O(VE).",Know what the Bellman-Ford algorithm is. Prove the correctness of the algorithm. Analyze its running time. Know how Bellman-Ford can be used to detect negative cycles,graph representations
beta distribution,Probability Theory,https://metacademy.org/graphs/concepts/beta_distribution,The beta distribution is a probability distribution over the unit interval. It is most commonly used in Bayesian statistics as the conjugate prior for the Bernoulli distribution.,Know the PDF of the beta distribution. Be able to compute the expectation of a beta random variable. Express the uniform distribution as a special case of the beta distribution,"random variables,gamma function,expectation and variance"
beta process,"Bayesian Statistics,Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/beta_process,"The beta process is a random discrete measure that is completely described by a countably infinite set of atoms, where each atom has a finite mass determined from a stick-breaking process. Unlike the Dirichlet process, the weights of the atoms do not have to sum to one, but the masses must be between [0,1], and the marginals of the beta process are not beta distributed. The beta process can be used as a base measure for a Bernoulli process, i.e. to yield a stochastic process for binary random variables.",,"Dirichlet process,Indian buffet process"
BFGS,Optimization,https://metacademy.org/graphs/concepts/bfgs,"The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is a quasi-Newton optimization method, and one of the most commonly used optimization algorithms in practice. It approximates the Newton updates by iteratively updating an approximation to the inverse Hessian matrix.","Know the steps of the BFGS algorithm. What is the curvature condition, and why must it be satisfied in order to obtain a valid udpate? Why might it not be satisfied without a careful line search?. What is the optimization problem used to derive the update to the inverse Hessian?. Why does this optimization problem use a weighted Frobenius norm rather than a Frobenius norm?. What is the computational complexity of a single iteration of the algorithm (not counting the cost of function and gradient evaluations)?","matrix norms,Newtons method (optimization),optimization problems"
bias-variance decomposition,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/bias_variance_decomposition,"The bias-variance decomposition (often referred to as the bias-variance tradeoff) is a frequentist analysis of the generalization capability of an estimator, i.e. a learning algorithm.",,"linear regression,linear regression: closed-form solution,generalization"
binary linear classifiers,Machine Learning,https://metacademy.org/graphs/concepts/binary_linear_classifiers,"A linear classifier makes a classification decision for a given observation based on the value of a linear combination of the observation's features. In a ``binary'' linear classifier, the observation is classified into one of two possible classes using a linear boundary in the input feature space.",,linear regression
binary search,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/binary_search,Binary search is a logarithmic time algorithm for locating an element in a sorted array. It is an example of a divide-and-conquer algorithm.,Be able to implement binary search. Analyze the running time of binary search,"recursion (programming),asymptotic complexity"
binary search trees,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/binary_search_trees,"A binary search tree is a data structure which supports efficient search, insertion, and deletion operations. In particular, it is a binary tree where each left child is less than its parent, and each right child is greater than its parent. Search, insertion, and deletion can be done in logarithmic time, assuming the tree is somehow kept balanced.","Know what a binary search tree is and what invariant is maintained. Be able to implement the search operator. Be able to implement the naive versions of insertion and deletion (i.e. algorithms which don't attempt to maintain balance). Analyze the running times of search, insertion, and deletion in terms of the depth of the tree. Understand why maintaining balance is important for efficiency","recursion (programming),tree (data structure),binary search,asymptotic complexity"
binomial distribution,Probability Theory,https://metacademy.org/graphs/concepts/binomial_distribution,"The binomial distribution describes the number of successes in a fixed number of independent trials, each with the same success probability. When the number of trials is large, the distribution is approximately bell-shaped.",Know the PMF of the binomial distribution. Interpret the distribution in terms of i.i.d. Bernoulli random variables. Describe the shape of the distribution. Derive the expectation and variance of a binomial random variable,"random variables,independent events,expectation and variance"
Bloom filter,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/bloom_filter,"A bloom filter is a space-efficient data structure for determining if a given item is in a set. It has a certain (small) probability of a false positive, but no chance of a false negative.","Be able to implement a bloom filter. Assuming good hash functions, analyze the probability of a false positive given the number of bits, the number of hash functions, and the number of objects","hash tables,probability,independent events"
Boltzmann machines,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/boltzmann_machines,"Boltzmann machines are a kind of probabilistic neural network used in density modeling. They can be viewed as an MRF with only pairwise connections between units, and where the units are typically binary-valued. Restricted Boltzmann machines (RBMs) are a widely used special case.",Know the definition of a Boltzmann machine (i.e. what distribution it represents). Be able to (approximately) sample from a Boltzmann machine using Gibbs sampling.. Derive the fact that the model correlations must match the data correlations at the maximum likelihood solution.. Why can it be beneficial to add hidden units to the network?. Be aware of the analogies between Boltzmann machine updates and Hopfield network updates.,"Gibbs sampling,maximum likelihood,gradient,Hopfield networks"
Boolean algebras,"Logic,Set Theory",https://metacademy.org/graphs/concepts/boolean_algebras,"Boolean algebras are a mathematical structure which shares the algebraic properties of propositional formulas. Canonical examples include propositional formulas and the power set of a set (with set union, intersection, and complement playing the roles of the propositional connectives). Boolean algebras are used in topology, model theory, and social choice theory.","Define a lattice. Define a Boolean algebra (as a complemented distributed lattice). Be able to prove simple facts about Boolean algebras. Show that the power set of a set, with the standard set operations, forms a Boolean algebra. Give an example of a Boolean algebra which is not a power set. Define an atom. Be aware of the principle of duality (that Boolean algebras are symmetric with respect to meet and join)","propositional logic,order relations,set operations,countable sets"
boosting as optimization,Machine Learning,https://metacademy.org/graphs/concepts/boosting_as_optimization,AdaBoost can be interpreted as a sequential procedure for minimizing the exponential loss on the training set with respect to the coefficients of a particular basis function expansion. This leads to generalizations of the algorithm to different loss functions.,"Derive AdaBoost as a sequential procedure to minimize the exponential loss on the training set.. Based on this analysis, why might AdaBoost be especially sensitive to mislabeled training examples?. Understand how the basic boosting procedure can be generalized to other loss function.s\nWhy do we often re-estimate the weights of the base classifiers for more general boosting algorithms, but not for AdaBoost?","AdaBoost,optimization problems,basis function expansions"
bootstrap,Frequentist Statistics,https://metacademy.org/graphs/concepts/bootstrap,"The bootstrap is a Monte Carlo technique for estimating variances or confidence intervals of statistical estimators. It uses the empirical distribution as a proxy for the true distribution, and measures the accuracy of the estimator on datasets resampled from the empirical distribution. It is widely applicable and doesn't require assuming a parametric form for the true distribution.","Know the procedures for both the parametric and nonparametric bootstrap\nWhen would you choose one over the other?\nNote: for the parametric bootstrap, it may help to know about a point estimator such as maximum likelihood , but you can treat this as a black box.. Be able to use the bootstrap to:\nestimate the variance of an estimator\ncompute a confidence interval for an estimator. The nonparametric bootstrap introduces two sources of error: using the empirical distribution as a proxy for the true distribution, and repeatedly simulating from the empirical distribution. Which of these would you expect to be a larger source of error?","expectation and variance,Monte Carlo estimation"
breadth-first search,"Data Structures & Algorithms,Programming,Symbolic AI",https://metacademy.org/graphs/concepts/breadth_first_search,Breadth-first search is a kind of graph search where nodes are explored in the order of distance from the starting node. It can be used to compute shortest paths in a graph.,Be able to implement breadth-first search using the adjacency list representation of a graph. Analyze the running time of BFS. Know how BFS can be used to compute shortest paths. Know what the predecessor subgraph (or search graph) refers to and why it can be represented as a tree,"graph representations,queue,tree (data structure)"
b-trees,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/b_trees,"B-trees are a data structure analogous to binary search trees, but where a given node may have more than two children. They are a fundamental data structure in databases, since they can be used to search extremely large datasets with only a handful of disk accesses.","Be aware of the motivation of b-trees in terms of minimizing disk accesses. Know what a b-tree is and what invariants it maintains. Be able to implement the search, insert, and delete operators","red-black trees,binary search trees,memory hierarchy"
completeness of first-order logic,Logic,https://metacademy.org/graphs/concepts/completeness_of_first_order_logic,"Godel's Completeness Theorem shows that there is a complete (and sound) deductive calculus for first-order logic. In other words, if some set of sentences is consistent (one cannot derive a contradiction from them), then there is some model in which all the sentences are satisfied. This result is significant in that it unifies the syntax and semantics of first-order logic.","Know what it means for a first-order deductive calculus to be complete. Know the statement of Godel's Completeness Theorem. Prove Godel's Completeness Theorem (Henkin's proof in particular). Using the Completeness Theorem, prove the Compactness Theorem for first-order logic.","first-order logic,semantics of first-order logic,proofs in first-order logic,countable sets"
complex numbers,Uncategorized,https://metacademy.org/graphs/concepts/complex_numbers,"Complex numbers are numbers expressible as a + bi, where i^2 = -1. They are often more convenient to work with than real numbers, because all complex (and hence all real) polynomials of degree n have n complex roots. Many trigonometric identities can be derived more simply using complex numbers.","Be able to add, multiply, and divide complex numbers, and raise them to powers. Know the definitions of\ncomplex conjugate\nnorm (or absolute value) of a complex number. Why can't i and -i be distinguished?. Be able to work with the polar representation of complex numbers",
complex vectors and matrices,Linear Algebra,https://metacademy.org/graphs/concepts/complex_vectors_and_matrices,We can define complex vectors and matrices with properties closely analogous to their real-valued analogues. Complex matrices come up when dealing with eigendecompositions of non-symmetric matrices. They are also used in computing the fast Fourier transform.,Know the definitions of the Hermitian operator and the complex dot product. Why is the Hermitian operator used for complex matrices rather than the matrix transpose?,"complex numbers,dot product,matrix transpose"
computational complexity of graphical model inference,"Probabilistic Graphical Models,Theory of Computation",https://metacademy.org/graphs/concepts/complexity_of_inference,"The major inference problems for graphical models (marginalization, MAP assignment, and partition function) are all intractable in the worst case. In particular, marginalization and partition function computation are both #P-complete, and MAP inference is NP-complete.",,"inference in MRFs,NP-completeness"
computations on multivariate Gaussians,"Machine Learning,Probabilistic Graphical Models,Probability Theory",https://metacademy.org/graphs/concepts/computations_on_multivariate_gaussians,Multivariate Gaussians are widely used in computational sciences because many useful operations can be performed efficiently. Marginalization is easy: we simply pull the relevant rows and columns of the mean and covariance. Conditioning can be done with a matrix inversion.,,"multivariate Gaussian distribution,conditional distributions,covariance matrices,multivariate distributions,matrix inverse"
computing matrix inverses,Linear Algebra,https://metacademy.org/graphs/concepts/computing_matrix_inverses,Matrix inverses can be computed using Gaussian elimination.,Be able to use Gaussian elimination to compute the inverse of a matrix.,"matrix inverse,Gaussian elimination"
computing probabilities by counting,Probability Theory,https://metacademy.org/graphs/concepts/computing_probabilities_by_counting,"Sometimes probability calculations involve large numbers of outcomes with equal probabilities. In this case, we can compute probabilities by counting the number of outcomes, using formulas for permutations and combinations.",Be able to compute probabilities of events using the formulas for the numbers of permutations and combinations.,probability
computing the nullspace,Linear Algebra,https://metacademy.org/graphs/concepts/computing_the_nullspace,The nullspace of a matrix can be computed using Gaussian elimination.,Be able to compute the basis for the nullspace using Gaussian elimination,"column space and nullspace,Gaussian elimination"
conditional distributions,Probability Theory,https://metacademy.org/graphs/concepts/conditional_distributions,"The conditional distribution of a random variable X given another random variable Y is the distribution of X when Y is observed to take some value. While the precise mathematical definition is involved, for discrete and continuous variables, it amounts to dividing the joint PDF or PMF of X and Y by the PDF or PMF of Y.","Know the definitions of the conditional distribution for both discrete and continuous random variables. For continuous random variables, why isn't it mathematically rigorous to condition on an event of probability zero?. Know how the joint distribution of a set of random variables decomposes as a product of conditional distributions","random variables,multivariate distributions,conditional probability"
conditional expectation,Probability Theory,https://metacademy.org/graphs/concepts/conditional_expectation,The conditional expectation E[X | Y] is the expectation of X in the conditional distribution P(X | Y).,Know the definitions of conditional expectation for both discrete and continuous random variables. Know and be able to apply the law of iterated expectations,"expectation and variance,conditional distributions"
conditional independence,Probability Theory,https://metacademy.org/graphs/concepts/conditional_independence,"Two random variables X and Y are conditionally independent given a random variable Z if they are independent in the conditional distribution given Z. Conditional independence is central notion in probabilistic modeling, because a model's conditional independence assumptions often lead to tractable algorithms for inference and learning in that model.","Know the definition of conditional independence. Give examples to show that conditional independence does not imply independence, and vice versa. It's likely that you're seeing this page because you want to learn about graphical models. If so, don't bother memorizing the rules of conditional independence; you'll get more intution and practice with them when you learn about graphical models. Just convince yourself that the basic properties make intuitive sense.","conditional probability,independent events,random variables"
conditional probability,Probability Theory,https://metacademy.org/graphs/concepts/conditional_probability,"The conditional probability of an event A given another event B is the probability that A occurs if it's known that B occurs. Conditional probability is fundamental to probabilistic modeling, since it gives a way of accounting for observed evidence.",Know the definition of conditional probability. Know how the joint probability of a configuration of random variables decomposes as a product of the conditional probabilities,probability
conditional random fields,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/conditional_random_fields,MRFs encode factorization and conditional independence structure in a joint distribution over some set of random variables. Conditional random fields (CRFs) instead directly encode the same kind of structure in the conditional distribution of one set of random variables given another. This allows them to represent more complex dependencies in the conditional distribution. They are commonly used in computer vision and natural language processing.,,Markov random fields
conjugate gradient,Optimization,https://metacademy.org/graphs/concepts/conjugate_gradient,"Conjugate gradient is an algorithm for solving large systems of linear equations of the form Ax = b where A is positive definite. It works by ensuring that the search directions are all conjugate (a property which generalizes the notion of orthogonality). Since it is implemented in terms of matrix-vector products, it is efficient when A is sparse, or when Ax can otherwise be computed efficiently.","Know what quadratic optimization problem conjugate gradient solves. Why does writing Ax = b as such an optimization problem require that A be positive definite? How can CG be applied when A isn't positive definite?. Give some examples where Ax can be computed in less than O(n^2) time (and hence the CG updates are efficient). Know terminology: conjugacy, Krylov subspace. Show that each CG update produces a conjugate search direction. Based on this, show that each CG update minimizes the objective function over the Krylov subspace. Why does this imply that CG finds the exact solution after at most n iterations (where n is the dimension of A)?","gradient,linear systems as matrices,positive definite matrices,optimization problems"
conservative vector fields,Multivariate Calculus,https://metacademy.org/graphs/concepts/conservative_vector_fields,"A vector field is conservative if it can be expressed as the gradient of a function (called the potential function), or equivalently, if all line integrals are path independent. The notion is useful because dynamical systems are easier to analyze if they can be described in terms of a potential function.",Know the Fundamental Theorem of Calculus for line integrals. Know what a conservative vector field is and why it is useful. Show the equivalence of two definitions of a conservative vector field:\ncan be expressed as the gradient of a function\nline integrals are independent of the path. Know the definition of the curl of a vector field. Be able to tell if a vector field is conservative by determining if the curl is 0. Be able to determine the potential function for a conservative vector field,"vector fields,line integrals,gradient"
constraint satisfaction problems,Symbolic AI,https://metacademy.org/graphs/concepts/constraint_satisfaction_problems,"Constraint satisfaction problems (CSPs) are a kind of search problem in AI, where the objective is to assign values to each variable, such that the values obey specified constraints. Many relatively large-scale problems can be solved using off-the-shelf CSP solvers.","Know the definition of constraint satisfaction problem. Know some terminology: variable, constraint, domain, assignment, Boolean CSP. Be able to formulate a CSP to model a problem. Know how a CSP can be formulated as a search problem. Why are unary constraints easy to deal with?. Show that arbitrary higher-order constraints can be converted to ternary constraints (and therefore we need not consider orders higher than 3). Know what the constraint graph represents",search problems
constructing kernels,Machine Learning,https://metacademy.org/graphs/concepts/constructing_kernels,The kernel trick allows us to reformulate linear machine learning models in terms of a kernel function which defines a notion of similarity between data points. A few simple rules allow us to construct kernels which capture a wide variety of similarity functions.,,kernel trick
constructing the integers,Set Theory,https://metacademy.org/graphs/concepts/constructing_the_integers,The integers can be constructed from the natural numbers.,"Construct the integers from the natural numbers. Define addition and multiplication, negation, and the comparison operators. Show that all of these have the properties we would expect","equivalence relations,order relations"
constructing the rationals,Uncategorized,https://metacademy.org/graphs/concepts/constructing_the_rationals,The rational numbers can be constructed from the integers.,"Construct the rational numbers from the integers. Define addition, multiplication, additive and multiplicative inverses, and the comparison operators. Show that these definitions satisfy the field axioms","equivalence relations,order relations,constructing the integers,fields"
constructing the reals,Set Theory,https://metacademy.org/graphs/concepts/constructing_the_reals,The real numbers can be explicitly constructed as sets of rational numbers using the Dedekind cut construction.,Define the real numbers using the Dedekind cut construction.. Show that this construction satisfies the least upper bound property.. Define the basic arithmetic operations and comparison operators. Show that these satisfy the field axioms,"real numbers,set operations,order relations,fields"
context-free grammars,"Programming,Theory of Computation",https://metacademy.org/graphs/concepts/context_free_grammars,"Context-free grammars are a formalism for defining the syntax of formal languages. Strings are generated by applying a sequence of productions, each of which replaces a nonterminal symbol with a sequence of terminals and nonterminals. They are widely used in the implementation of programming languages and in modeling the syntax of natural languages.","Know the basic terminology: context-free grammars, substitution rules (or productions), variables, terminals, start variable, derivation, parse tree.. Define how a context-free grammar specifies a language. Be able to design context-free grammars to represent simple languages. Define what it means for a CFG to be ambiguous, and give an example of an ambiguous grammar",
context-free languages,Theory of Computation,https://metacademy.org/graphs/concepts/context_free_languages,"Context-free languages are languages definable in terms of a context-free grammar, or equivalently, recognizable by a (nondeterministic) pushdown automaton. All regular languages are also context-free. Languages can be shown to be non-context-free using the Pumping Lemma for context-free languages.","From the fact that pushdown automata can simulate finite state automata, conclude that all regular languages are context-free.. Know the statement of the Pumping Lemma for context-free languages.. Prove the Pumping Lemma.. Be able to use the Pumping Lemma to prove that languages are not context-free.","context-free grammars,pushdown automata,regular languages"
convergence of conjugate gradient,Optimization,https://metacademy.org/graphs/concepts/convergence_of_conjugate_gradient,"The conjugate gradient algorithm typically isn't run to completion, but is more commonly stopped after a small number of iterations to obtain an approximate solution. If the eigenspectrum of A satisfies certain properties, one can show that CG can find a good approximate solution quickly.","Using the fact that CG minimizes the objective function over the Krylov subspace, give an explicit expression for the objective function after k iterations in terms of the eigenvalues of A and a polynomial. Understand why convergence can be very fast if the eigenvalues of A form a small number of clumps. Be aware that the number of iterations needed to achieve a given accuracy is at most proportional to the square root of the condition number of A (but that convergence can be much faster in practice). Compare the convergence rate of CG to that of gradient descent","conjugate gradient,roots of polynomials,eigenvalues and eigenvectors,positive definite matrices,convergence of gradient descent"
convergence of gradient descent,Optimization,https://metacademy.org/graphs/concepts/convergence_of_gradient_descent,"Under certain conditions, gradient descent can be shown to obey linear convergence, where the number of significant digits accuracy grows linearly in the number of iterations. Furthermore, the rate of convergence is inversely proportional to the condition number, or the ratio of the largest and smallest eigenvectors of the Hessian near the optimum.","Know some terminology: global convergence, linear convergence, strong convexity, condition number. Be aware that gradient descent obeys linear convergence and that the convergence rate is inversely proportional to the condition number. Be able to analyze the convergence analytically for a quadratic objective. Understand why a careful choice of step size is important","spectral decomposition,gradient descent,convex functions"
converting between graphical models,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/converting_between_graphical_models,"Bayes nets and MRFs are two frameworks for specifying factorization and conditional independence structure in probabilistic models. There are transformations which convert from one graphical model formalism to the other. However, sometimes these transformations must lose precision, because there are sets of independencies which can be represented as Bayes nets but not MRFs, and vice versa.",,"Bayesian networks,d-separation,Markov random fields"
convex functions,Optimization,https://metacademy.org/graphs/concepts/convex_functions,"Intuitively, convex functions are bowl-shaped. They are significant in optimization, because it is often possible to efficiently find the global optimum of a convex function.",Know the definition of a convex function (in multiple dimensions). Know and be able to apply alternative characterizations of convex functions\nfirst-order condition (linear approximation lies below the function)\nsecond-order condition (second derivative matrix is positive semidefinite). Know some examples of convex functions. Why can the value of the function outside its domain be taken to be infinity?,"positive definite matrices,convex sets,vectors,linear approximation,gradient,higher-order partial derivatives"
convex optimization,Optimization,https://metacademy.org/graphs/concepts/convex_optimization,"Convex optimization refers to optimizing a convex function over a convex set. It is a very broad class of problems, encompassing widely used techniques such as linear programming, linear least squares, quadratic programming, and semidefinite programming. Convex optimization is important because for reasonable-sized problems, there are efficient algorithms for finding the global optimum.",Know the definition of a convex optimization problem. Be aware that all local optima to a convex optimization problem are globally optimal. Know and be able to apply the first-order optimality condition for the unconstrained case (i.e. the gradient being zero),"optimization problems,convex sets,convex functions,gradient"
convex sets,Optimization,https://metacademy.org/graphs/concepts/convex_sets,"A set S in R^d is convex if for any two points x and y in S, the line segment connecting x and y is also contained in S. Convex sets are part of the definition of convex optimization problems, a very general class of optimization problems for which the optimal solution can often be found.","Know how convex sets are defined, both\ngeometrically (line segments lie within the set)\nalgebraically (in terms of linear combinations of vectors). Know some common examples of convex sets (e.g. hyperplanes, halfspaces, Euclidean balls)","vectors,dot product"
convolutional neural nets,Machine Learning,https://metacademy.org/graphs/concepts/convolutional_nets,"Convolutional neural networks are a kind of feed-forward neural net architecture geared towards visual processing. In each layer, there are several groups of units whose weights are repeated (or ""shared"") across all spatial locations. The forward pass and backpropagation updates can both be computed efficiently using convolution.","Understand what locality is and why it's advantageous.. Understand what weight sharing is and why it's advantageous.. Know what convolution layers are and how they're parameterized.. Be able to determine the number of units and connections in a convolution layer, as well as the number of arithmetic operations for the forward pass.. Know what pooling layers do and why they're advantageous.. Know the difference between equivariance and invariance, understand how each is achieved by a conv net, and when/why each is advantageous.. Be familiar with an image classification architecture such as LeNet or AlexNet.. Know some terminology: kernel, feature map, stride, filter size, valid/same/full. Be aware that the first convolution layer typically learns oriented filters when applied to images.. Be aware that conv nets can be trained with backprop.","backpropagation,feed-forward neural nets"
cotangent bundle,Differential Geometry,https://metacademy.org/graphs/concepts/cotangent_bundle,"The cotangent bundle of a differentiable manifold M is the bundle of dual spaces to the tangent spaces at each point of M. The elements are known as covectors. An important example of a covector field is the differential of a function -- this is the generalization of the gradient to manifolds. Pullback is an operator which ""pulls back"" a covector field over a map.","Define the cotangent bundle of a manifold. Given a coordinate chart on a manifold, construct a coordinate chart on the cotangent bundle. Define the differential of a function, and understand why it is naturally seen as a covector field rather than a vector field. Define and be able to compute the pullback of a covector field over a map","differentiable manifolds,tangent bundle,dual space,differentiable maps between manifolds,gradient,change of basis"
countable sets,Set Theory,https://metacademy.org/graphs/concepts/countable_sets,"A set is countably infinite if it can be put into one-to-one correspondence with the natural numbers. Countably infinite sets include the natural numbers, the rationals, and the algebraic numbers -- surprisingly, these are all ""the same size"" in the set-theoretic sense. By contrast, the set of real numbers is uncountably infinite. The more general notion of cardinality lets us distinguish different ""sizes"" of infinity.","What does it mean for two sets to have the same cardinality, and why is this an equivalence relation?. Define countability.. Be able to prove that sets are countable.. Prove that the set of real numbers is not countable.",
covariance,Probability Theory,https://metacademy.org/graphs/concepts/covariance,"The covariance of two random variables is a measure of their relatedness. It is closely related to the correlation coefficient, but is more commonly used in probability theory because it has nice mathematical properties.",Know the definitions of covariance and correlation. Write the covariance in terms of the moments of the distribution. Know the Cauchy-Schwartz inequality for covariance (which bounds the covariance in terms of the individual variances). Be able to compute the variance of a linear combination of random variables in terms of their variances and covariances. Know that independent random variables have zero covariance. Show that variance of a sum of independent random variables is a sum of the variances,"expectation and variance,multivariate distributions,independent random variables"
covariance matrices,Probability Theory,https://metacademy.org/graphs/concepts/covariance_matrices,"A covariance matrix generalizes the idea of variance to multiple dimensions, where the i-th j-th element in the covariance matrix is the covariance between the i-th and j-th random variables. Covariance matrices are common throughout both statistics and machine learning and often arise when dealing with multivariate distributions.",Understand how to calculate the entries of a covariance matrix. Understand the difference between positive and negative covariances,"covariance,positive definite matrices"
Cramer-Rao bound,Frequentist Statistics,https://metacademy.org/graphs/concepts/cramer_rao_bound,The Cramer-Rao bound gives the minimum possible variance of an unbiased estimator of the parameters of a probability distribution. It is used to prove the asymptotic efficiency of the maximum likelihood estimator.,"Prove the Cramer-Rao theorem, which bounds the variance of any unbiased estimator of model parameters.. Use the result to compute the asymptotic relative efficiency of an estimator.","Fisher information,covariance,partial derivatives"
Cramer's rule,Linear Algebra,https://metacademy.org/graphs/concepts/cramers_rule,"Cramer's rule is an explicit formula for the inverse of a matrix in terms of determinants of submatrices. While it is inefficient for large matrices, it is useful for analyzing inverses of small matrices algebraically.",Be able to use Cramer's rule to solve linear systems and compute matrix inverses,"linear systems as matrices,matrix inverse,determinant"
cross product,Multivariate Calculus,https://metacademy.org/graphs/concepts/cross_product,"The cross product is an operation which takes to vectors and returns another vector orthogonal to both. It is commonly used to answer geometric questions involving points, lines, and planes, and to compute volumes.","Know the definition and basic properties of the cross product. Know the ""right-hand rule"" for cross products. Know the formula for the cross product in terms of the lengths of two vectors and the angle between them.. Be able to compute the volume of a prism using the dot product and cross product","vectors,dot product"
cross validation,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/cross_validation,"Cross validation is the process of partitioning an observed dataset into a training dataset and a testing dataset and then performing the statistical analysis on the training dataset (e.g. learning the parameters of a distribution used to describe the data) and then validated using the testing dataset (e.g. measuring how well the learned distribution describes the testing dataset). Following the validation step, a new, untested portion of the training dataset becomes the testing dataset and the previous testing dataset is incorporated into the training dataset. This cycle repeats until all data has been tested. This process is used to test how well a statistical analysis generalizes to new data.","Understand the common types of cross validation: holdout method, random subsamples, K-fold, leave-one out",loss function
CRP clustering,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/crp_clustering,"The predictive rule for Chinese Restaurant Process (CRP) can be used to define an ""infinite-capacity"" prior distribution on the clusters in a clustering model. The most common clustering model that uses the CRP is an unbounded analogue to a Gaussian mixture model, where the ""table assignments"" from the CRP determine the mixture component assignments for each data point.",,"mixture of Gaussians models,Bayesian parameter estimation,Chinese restaurant process,collapsed Gibbs sampling"
cumulative distribution function,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/cumulative_distribution_function,"The cumulative distribution function (CDF) of a random variable X is the function F, where F(a) is the probability that X <= a. CDFs are a convenient representation because they apply to both discrete and continuous random variables, and they can simplify many calculations.",Know the definition and basic properties of the CDF. Be able to use the CDF of a distribution to:\ndetermine the probability that a random variable lies in a given range\nrecover the probability mass function or the probability density function,random variables
curse of dimensionality,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/curse_of_dimensionality,The curse of dimensionality refers to a collection of counterintuitive properties of high-dimensional spaces which make it difficult to learn using purely local algorithms such as K nearest neighbors.,"Understand the properties of high-dimensional spaces that make it difficult to learn based on nearest neighbors, e.g.\nif points are sampled uniformly within a high-dimensional box, most distances are similar\nmost of the volume of a ball is concentrated near the boundary\nuniformly sampled points are unlikely to have very close neighbors. If the input variables are sampled uniformly from a D-dimensional box, and one uses KNN as the learning algorithm, why can the number of training examples needed to achieve a given accuracy be exponential in D?","K nearest neighbors,weak law of large numbers"
decision trees,Machine Learning,https://metacademy.org/graphs/concepts/decision_trees,"Decision trees are a kind of tree-structured model used in machine learning and data mining. Each leaf node corresponds to a prediction, and each internal node divides the data points into two or more sets depending on the value of one of the input variables. Decision trees are widely used because of their simplicity and their ability to handle heterogeneous input features.","Know what a decision tree is.. Give examples of functions which can't be represented compactly (e.g. majority, parity). Be able to fit a decision tree using a recursive greedy strategy.. What is the information gain criterion, and why does it produce better splits than classification accuracy?. Be aware that decision trees can be unstable, in that the structure changes dramatically with respect to small changes in the training data.",
deep belief networks,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/deep_belief_networks,"Deep belief networks (DBNs) are a kind of deep, multilayer graphical model which contains both directed and undirected edges. The bottom layer represents the inputs, and the higher layers are meant to represent increasingly abstract features of the data. DBNs can be trained in a layerwise fashion, and are often used to initialize deep discriminative neural networks, a procedure known as generative pre-training.",Know the graphical model structure of a DBN and understand what the combination of directed and undirected edges represents.. Understand why the explaining away effect makes exact inference in a DBN intractable.. Know how to train a DBN in a layerwise fashion.. Optional: understand mathematically why layerwise training is guaranteed to improve the likelihood.,"Markov random fields,Bayesian networks,restricted Boltzmann machines"
defining the cardinals,Set Theory,https://metacademy.org/graphs/concepts/defining_the_cardinals,"Intuitively, one would like to define cardinal numbers as equivalence classes of sets, but unfortunately, these equivalence classes are too large to be sets. Instead, the cardinal numbers can be defined from the ordinal numbers. This construction requires the Axiom of Choice.",Show that the Well-Ordering Theorem is equivalent to the Axiom of Choice. Show that the trichotomy of the dominance relation is equivalent to the Well-Ordering Theorem. Construct the cardinal numbers in terms of the ordinals. Know why one cannot simply define cardinals as equivalence classes of sets under equinumerosity,"cardinality,Russells Paradox,functions and relations as sets,ordinal numbers,Axiom of Choice"
depth-first search,"Data Structures & Algorithms,Programming,Symbolic AI",https://metacademy.org/graphs/concepts/depth_first_search,Depth-first search is a kind of graph search where a branch of the search tree is explored as deeply as possible before backtracking. It can be used to compute a topological sort of a graph.,Be able to implement depth-first search given the adjacency list representation of a graph. Analyze the running time of DFS. Know what the predecessor graph (or search graph) refers to and why it is tree-structured,"stack,graph representations,recursion (programming),tree (data structure)"
determinant,Linear Algebra,https://metacademy.org/graphs/concepts/determinant,"The determinant is a scalar value associated with a square matrix. It is convenient algebraically because it behaves nicely with respect to matrix multiplication, inverses, and transposes, as well as the Gaussian elimination operations. It gives the factor by which volumes are rescaled by the matrix's associated linear transformation. It also equals the product of the eigenvalues.","Define the determinant (in terms of a sum over column permutations). Be able to calculate the determinant recursively in terms of cofactors. Know what the Gaussian elimination operations do to the determinant of a matrix. Be able to manipulate the determinant algebraically, together with matrix multiplication, inverse, and transpose. Derive the identities relating the determinant to matrix multiplication, inverse, and transpose. Show that a matrix is invertible if and only if its determinant is nonzero. Show that the determinant of a triangular matrix is the product of the diagonal entries","vectors,matrix transpose,Gaussian elimination,matrix multiplication,matrix inverse"
determinant and volume,Linear Algebra,https://metacademy.org/graphs/concepts/determinant_and_volume,"The volume of a box in n dimensions is given by the determinant of the matrix whose columns are the sides of the box. When a matrix is used as a linear transformation, it scales the volume of a set by its determinant.",Show that the determinant of a square matrix gives the volume of a box whose sides are given by the columns,determinant
diagonalization,Linear Algebra,https://metacademy.org/graphs/concepts/diagonalization,"Diagonalization refers to factorizing a matrix as A = SDS^-1, where D is a diagonal matrix. The entries of D correspond to the eigenvalues of A, and the columns of S correspond to the eigenvectors. The diagonal representation is useful for computing powers of matrices. Unfortunately, not all matrices are diagonalizable.",Know what it means to diagonalize a matrix. Understand why any matrix with no repeated eigenvalues can be diagonalized. Know how diagonalization can be used to compute powers of a matrix,"matrix multiplication,eigenvalues and eigenvectors,bases,matrix inverse"
differentiable manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/differentiable_manifolds,"A manifold is a space that locally resembles Euclidean space. It generalizes the notion of a surface: is is more general in the sense that a manifold need not be embedded in some higher dimensional Euclidean space. A differentiable (or smooth) manifold is a manifold equipped with a differential (or smooth) structure, which allows us to define notions such as differentiable functions, derivatives, and integrals. It is the basic object of study in differential geometry.","Know the definition of topological manifold. (Feel free to skim this part, since it used less often in differential geometry.). Know the definition of differential structure, and why it is necessary to perform calculus on manifolds.. Be familiar with some examples of differentiable manifolds.","linear approximation,topology of R^n"
differentiable maps between manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/differentiable_maps_between_manifolds,A map from one differentiable manifold to another is differentiable if the corresponding map between local coordinate charts is differentiable. This is independent of the particular coordinate charts. A bijective map which is differentiable in both directions is a diffeomorphism. Differential geometry primarily studies properties which are invariant under diffeomorphism.,Define what it means for a function on a (differentiable) manifold to be differentiable.. Define what it means for a map between differentiable manifolds to be differentiable.. Understand why both definitions are independent of the coordinate chart(s).. Be able to show that functions and maps are differentiable.. Know the definition of a diffeomorphism and understand why it is an equivalence relation.,"differentiable manifolds,linear approximation,equivalence relations,topology of R^n"
differential entropy,Probability Theory,https://metacademy.org/graphs/concepts/differential_entropy,Differential entropy is a generalization of entropy to continuous random variables. It is closely related to the asymptotic entropy of increasingly fine discretizations of the continuous distribution. KL divergence and mutual information can similarly be extended to the continuous case.,"Know the definition of differential entropy. Compute it for some simple examples, such as the uniform distribution and Gaussian distribution. Be aware that differential entropy can be negative. How does differential entropy relate to the asymptotic entropy of discretizations of the distribution?. Extend the definitions of mutual information and KL divergence to the continuous case.\nNote: unlike differential entropy, the continuous versions of mutual information and KL divergence behave like their discrete counterparts. Therefore, in a sense, they are more fundamental.","entropy,mutual information,KL divergence,expectation and variance,Gaussian distribution"
differential forms,"Differential Geometry,Multivariate Calculus",https://metacademy.org/graphs/concepts/differential_forms,"Differential forms are tensor fields on manifolds where the tensors are alternating, i.e. they satisfy properties analogous to those of the determinant. Differential forms can be combined using the wedge product. They are needed to define integration on manifolds, and are fundamental to de Rham cohomology.","Define the set of multilinear functions on a vector space. Determine a basis for the vector space of multilinear functions, and hence determine its dimension. Define differential forms. Define the wedge product of differential forms. Be able to manipulate the wedge product algebraically. Show that the wedge product is natural, i.e. commutes with pullback","tensor fields on manifolds,cotangent bundle,determinant,determinant and volume,vector fields,differentiable maps between manifolds"
Dijkstra's algorithm,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/dijkstras_algorithm,Dijkstra's Algorithm is an algorithm for computing the shortest path between two vertices of a graph where all edges have nonnegative weight. It is based on repeatedly expanding the closest vertex which has not yet been reached.,"Know how Dijkstra's algorithm works, conceptually. Know how Dijkstra's algorithm can be implemented efficiently using a heap. Understand why the assumption of nonnegative edge weights is necessary. Prove the correctness of Dijkstra's algorithm. Analyze the running time of Dijkstra's algorithm","asymptotic complexity,graph representations,breadth-first search,heap (data structure)"
Dirichlet distribution,Probability Theory,https://metacademy.org/graphs/concepts/dirichlet_distribution,"The Dirichlet distribution specifies a distribution on a n-dimensional vector and can be viewed as a probability distribution on a n-1 dimensional simplex (a simplex is an n-dimensional generalization of a triangle). Its parameters determine the distribution of mass on this simplex. The Dirichlet distribution is a conjugate prior to the categorigal and multinomial distributions, and for this reason, it is common in Bayesian statistics. Also, the Dirichlet distribution is a generalization of the beta distribution to higher dimensions (for n=2 it is the beta distribution).",,"beta distribution,gamma function,multinomial distribution"
Dirichlet process,"Bayesian Statistics,Probability Theory",https://metacademy.org/graphs/concepts/dirichlet_process,"The Dirichlet process is a stochastic process that defines a probability distribution over infinite-dimensional discrete distributions, meaning that a draw form a DP is itself a distribution (with a countably infinite number of parameters). Its name stems from the fact that the marginal of a DP for any finite partition is Dirichlet distributed. While the DP is often discussed alongside the Chinese Restaurant Process (CRP), the two are not the same entity. The DP is the de Finetti mixing measure for the CRP, meaning that sampling i.i.d. from a draw of a DP is equivalent to sequentially drawing samples from the CRP.",,"Dirichlet distribution,Chinese restaurant process"
Divergence Theorem,Multivariate Calculus,https://metacademy.org/graphs/concepts/divergence_theorem,The Divergence Theorem is a theorem relating the flux across a surface to the integral of the divergence over the interior. It can be seen as a three-dimensional generalization of Green's Theorem.,Know the definition of divergence. Know the statement of the Divergence Theorem in three dimensions. Prove the Divergence Theorem in three dimensions. Be able to apply the Divergence Theorem to calculate flux across a surface,"surface integrals,partial derivatives,multiple integrals"
dot product,"Linear Algebra,Multivariate Calculus",https://metacademy.org/graphs/concepts/dot_product,"The dot product is an operation on vectors. It is used to define notions such as vector length, perpendicular vectors, and angles between vectors.",Define the dot product. Define the length of a vector in terms of the dot product. Know what unit vectors are and why they are useful. Be able to rescale a vector to unit length. Define orthogonality in terms of the dot product. Know the cosine formula for the dot product. Know the Cauchy-Schwartz inequality. Prove the Cauchy-Schwartz inequality. Prove the triangle inequality,vectors
DPLL procedure,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/dpll_procedure,"The DPLL procedure is a backtracking-based algorithm for solving SAT instances. It is complete, in the sense that it will eventually return a satisfying assignment or prove that none exists.",Know what the DPLL algorithm is. Be able to simulate the algorithm on simple examples. Understand the motivations for the unit clause and pure symbol heuristics. Understand why the algorithm is complete.,propositional satisfiability
dropout,Uncategorized,https://metacademy.org/graphs/concepts/dropout,"Dropout is a widely used technique for improving the generalization of neural networks. In each iteration, a random subset of the units are ""dropped out"", or deactivated. This prevents different units from co-adapting to each other, thereby preventing overfitting. Dropout can also be seen as training an exponentially large ensemble of network architectures with tied weights.",Know how the forward pass is defined for a dropout network.. Be able to derive the backprop update for dropout units.. Know why it is advantageous to set units' activations to their expectations at test time.. Know how dropout can be interpreted as an ensemble method.,"generalization,feed-forward neural nets,expectation and variance,backpropagation,stochastic gradient descent,bagging"
d-separation,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/d_separation,"D-separation gives a way of determining conditional independence properties in Bayes nets in terms of the graph structure. It captures an intuitive notion of the ""flow"" of probabilistic influence through the graph.",,"Bayesian networks,conditional independence"
dynamic memory allocation in C,Programming,https://metacademy.org/graphs/concepts/dynamic_memory_allocation_in_c,"Dynamic memory allocation refers to allocating regions of memory while a program is running. In C, this is done manually using the functions malloc and realloc. Most modern high-level programming languages manage memory automatically, but it is still helpful to understand what's going on beneath the hood.","Understand in what situations memory needs to be allocated dynamically. Know how to use the functions malloc, realloc, and free in C. Understand why failing to free memory results in a ""memory leak,"" and know some best practices for avoiding this",C pointers
early stopping,Machine Learning,https://metacademy.org/graphs/concepts/early_stopping,"Early stopping is a technique for controlling overfitting in machine learning models, especially neural networks, by stopping training before the weights have converged. Often we stop when the performance has stopped improving on a held-out validation set.",,"backpropagation,generalization"
eigenvalues and eigenvectors,Linear Algebra,https://metacademy.org/graphs/concepts/eigenvalues_and_eigenvectors,"If A is a square matrix, the eigenvalues are the scalar values u satisfying Ax = ux, and the eigenvectors are the values of x. Eigenvectors and eigenvalues give a convenient representation of matrices for computing powers of matrices and for solving differential equations. An important special case is the spectral decomposition of symmetric matrices.",Know the definitions of eigenvalues and eigenvectors. Understand why the eigenvectors for a given eigenvalue form a subspace. Be able to calculate eigenvalues and eigenvectors in terms of the roots of the characteristic polynomial. Show that the sum of the eigenvalues equals the trace. Show that the product of the eigenvalues equals the determinant. Know why eigenvalues and eigenvectors can sometimes be complex valued. Show that the complex eigenvalues of a real matrix come in conjugate pairs. Show that eigenvectors corresponding to distinct eigenvalues are linearly independent,"matrix multiplication,roots of polynomials,linear systems as matrices,complex numbers,determinant,column space and nullspace"
EM algorithm for PCA,Machine Learning,https://metacademy.org/graphs/concepts/em_for_pca,"While probabilistic PCA has a closed-form solution, it is infeasible to compute for large and high-dimensional datasets. The expectation-maximization (EM) algorithm provides an alternative. Despite its iterative nature, it can be far more computationally efficient.",Derive the update rules for using EM to compute the principal components. Understand why EM can be advantageous over the closed-form solution (in terms of computational complexity),"probabilistic PCA,Expectation-Maximization algorithm,maximum likelihood: multivariate Gaussians"
entropy,Probability Theory,https://metacademy.org/graphs/concepts/entropy,"Entropy is a measure of the information content of a random variable, and one of the fundamental quantities of information theory. It determines the minimum expected code length necessary to encode samples of the random variable.",Understand the notion of entropy of a discrete random variable.. What is the largest possible entropy of a discrete random variable which takes on r possible values?. Know the definitions of joint entropy and conditional entropy.. Derive the chain rule for writing joint entropy as a sum of conditional entropies.. Show that the entropy of a set of independent random variables is the sum of the individual entropies.,"expectation and variance,conditional distributions,independent random variables,optimization problems"
equivalence relations,Set Theory,https://metacademy.org/graphs/concepts/equivalence_relations,"A relation is an equivalence relation if it is reflexive, symmetric, and transitive. Equivalence relations can be used to partition a set into equivalence classes. Examples include equality, isomorphism, and graph connectivity.",Define an equivalence relation. Be able to show that something is an equivalence relation. Understand how equivalence relations partition a set into equivalence classes,
Euler's formula,Uncategorized,https://metacademy.org/graphs/concepts/eulers_formula,Euler's formula gives a way of raising numbers to complex powers. It gives a more compact way of representing and deriving trigonometric identities.,Know and be able to use Euler's formula. Derive the formula using Taylor series. Be able to use the formula to derive trig identities,complex numbers
evaluating multiple integrals: change of variables,Multivariate Calculus,https://metacademy.org/graphs/concepts/eval_multi_int_change_of_variables,A useful trick for computing multiple integrals is to find a simpler parameterization of the region and apply the change of variables formula.,Know the change of variables formula for integration. Be able to use it to compute integrals,"evaluating multiple integrals: polar coordinates,linear approximation,determinant,determinant and volume"
evaluating multiple integrals: polar coordinates,Multivariate Calculus,https://metacademy.org/graphs/concepts/eval_multi_int_polar_coordinates,A common trick for computing double integrals is to transform them into a polar coordinate representation. Canonical examples include integrating a Gaussian and computing moments of inertia.,Be able to evaluate double integrals using polar coordinates,multiple integrals
evidence approximation,Bayesian Statistics,https://metacademy.org/graphs/concepts/evidence_approximation,"The evidence approximation is an approximation to Bayesian parameter estimation and model comparison. Rather than integrating out model hyperparameters, the hyperparameters chosen to maximize the marginal likelihood of the data.",Know what the evidence approximation is. Be aware of the heuristic justification: we're unlikely to overfit by maximizing over only a small number of hyperparameters,"Bayesian model comparison,optimization problems"
exceptions (programming),Programming,https://metacademy.org/graphs/concepts/exceptions_programming,"In many modern programming languages, a function can raise an exception to indicate that it's received an exceptional input and cannot continue processing. Execution then passes up the call stack until it reaches a function that ""catches"" it, i.e. provides code for handling it.","In a programming language such as Java or Python, understand what happens to the program execution when an exception is raised (and possibly caught). Be able to handle exceptions using a try/catch (or try/except) block. Know some situations where it is appropriate to raise an exception (including cases where it need not indicate a bug in the program). If the language supports it, know the difference between checked and unchecked exceptions and when it is appropriate to use which one. Know what the ""finally"" block does and what it's useful for. Be familiar with the language's exception hierarchy, and understand what it means for one exception to be a subtype of another",classes (programming)
expectation and variance,Probability Theory,https://metacademy.org/graphs/concepts/expectation_and_variance,"The expectation of a random variable is the value that it takes ""on average,"" and the variance is a measure of how much the random variable deviates from that value ""on average."" Expectation and variance have several convenient properties that often allow one to abstract away the underlying PDFs or PMFs.","Know the definitions of expectation and variance for both discrete and continuous random variables. Be able to compute expectations and variances. Be able to compute the expectation of a function of a random variable. Understand why the expectation of a random variable lies between its minimum and maximum possible values. Know that the expectation of a sum is the sum of the expectations, and be able to use this fact to compute expectations (this is a surprisingly useful trick!). Know that the expectation of a product of independent random variables is the product of the expectations. Prove both of these properties",random variables
expectation propagation,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/expectation_propagation,"In some graphical models, it is intractable even to compute the messages in loopy belief propagation. Expectation propagation is a way of approximating these messages in terms of expectations of sufficient statistics. It can be viewed as a variational inference algorithm, and often gives much more accurate results than mean field based approximations.",,"loopy belief propagation,variational inference,exponential families"
Expectation-Maximization algorithm,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/expectation_maximization,"Expectation-Maximization (EM) is an algorithm for maximum likelihood estimation in models with hidden variables (usually missing data or latent variables). It involves iteratively computing expectations of terms in the log-likelihood function under the current posterior, and then solving for the maximum likelihood parameters. Common applications include fitting mixture models, learning Bayes net parameters with latent data, and learning hidden Markov models.","Understand why the optimization landscape for maximum likelihood in a mixture model is difficult (i.e. it is nonconvex, and it has symmetries and saddle points). Derive the EM updates for a simple example such as a Gaussian mixture model. Know the general framework for EM. Show that each EM update improves the likelihood","maximum likelihood,mixture of Gaussians models,maximum likelihood: multivariate Gaussians"
expectimax search,"Reinforcement Learning,Symbolic AI",https://metacademy.org/graphs/concepts/expectimax_search,"Expectimax search is a search/decision-making algorithm that maximizes the average (expected) reward. It is typically applied to trees that have stochastic nodes, where the outcome of an action is uncertain.",,"probability,expectation and variance"
exploding and vanishing gradients,Uncategorized,https://metacademy.org/graphs/concepts/exploding_and_vanishing_gradients,"One of the problems of training recurrent or very deep feed-forward neural networks with backprop is that gradients can explode or vanish. This results from lots of Jacobian matrices being multiplied together. Alternatively, one can think about it as resulting from the complicated input/output behavior arising from the composition of lots of simpler functions.","Understand why gradients explode or vanish, both. in terms of the mechanics of backprop. in terms of the complexity of iterated functions. Understand why it's advantageous for each layer of a network to (be able to) compute something close to the identity function.. Know some techniques people use to get around the exploding/vanishing gradients problem.","backpropagation,recurrent neural networks"
exponential distribution,Probability Theory,https://metacademy.org/graphs/concepts/exponential_distribution,"The exponential distribution is a continuous distribution whose PDF decays exponentially. It is most commonly used to model the waiting time until an event occurs, where that event is equally likely to happen at any point in time.","Know the PDF of the exponential distribution. Know what it means for a distribution to be memoryless, and show that the exponential distribution has this property. Be aware of the motivation for this distribution in terms of waiting times","random variables,cumulative distribution function"
exponential families,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/exponential_families,"Exponential families are a broad class of probability distributions which includes many basic distributions such as Bernoullis and Gaussians, as well as Markov random fields. What they have in common is that the distributions can be represented in terms of log-linear functions of sufficient statistics.","Know the basic definitions: exponential family, natural parameter, sufficient statistic. Derive the exponential family representations of some simple distributions, e.g.\nGaussian\nBernoulli. Give an example of a family of distributions which is not an exponential family.","expectation and variance,binomial distribution,Gaussian distribution"
exterior derivative,"Differential Geometry,Multivariate Calculus",https://metacademy.org/graphs/concepts/exterior_derivative,The exterior function is a generalization of the differential of a function to differential forms. It appears in the statement of Stokes's Theorem for manifolds.,"Define the exterior derivative of a function and of a differential form. Show that the exterior derivative is independent of the coordinate system. Show that d(dw) = 0 for any differential form w. Show that the exterior derivative is natural, i.e. commutes with pullback. Be able to manipulate exterior derivatives algebraically","cotangent bundle,differential forms,partial derivatives,tensor fields on manifolds"
F measure,Machine Learning,https://metacademy.org/graphs/concepts/f_measure,The F measure (F1 score or F score) is a measure of a test's accuracy and is defined as the weighted harmonic mean of the precision and recall of the test.,"Understand why the harmonic mean of the precision and recall is used instead of the arithmetic mean. Should a web search engine, such as Google, favor high precision or high recall for its top 10 search results? What weights should they then use for the F measure?",precision and recall
factor analysis,Machine Learning,https://metacademy.org/graphs/concepts/factor_analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. In other words, it is possible, for example, that variations in three or four observed variables mainly reflect the variations in fewer unobserved variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ""error"" terms. [from Wikipedia]",Understand the probabilistic interpretation of factor analysis and how the model is closely related to a number of other common probabilistic models used in machine learning.. How can latent variables capture higher-order correlations? How does this apply to factor analysis?,"multivariate distributions,conditional distributions,computations on multivariate Gaussians,Expectation-Maximization algorithm,maximum likelihood: multivariate Gaussians"
factor graphs,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/factor_graphs,"Markov random fields often can't reflect the full conditional independence structure of a probabilistic model. For instance, they can't encode whether the variables in a clique have a fully general interaction, or merely pairwise interactions. Factor graphs are a more fine-grained representation of Boltzmann distributions where the factors are shown explicitly in the graph.",,Markov random fields
feed-forward neural nets,Machine Learning,https://metacademy.org/graphs/concepts/feed_forward_neural_nets,"Feed-forward neural networks are a supervised learning architecture consisting of a set of neuron-like ""units,"" each one of which computes a simple function of its inputs. Because layers of such neurons can be stacked, neural nets are capable of learning complex nonlinear functions of the inputs.","Know the basic structure of a neural net unit (linear function followed by nonlinear activation function).. Understand how units can be combined into layers and networks.. Know basic neural net terminology (unit, connection, layer, network, input layer, output layer, weight, width depth).. Be familiar with important activation functions, such as linear, ReLU, and logistic.. Be able to set weights by hand to compute simple functions like XOR.. Understand why deep linear networks are no more powerful than linear models.. Understand how nonlinear hidden layers allow neural nets to compute functions that linear models can't.. Be able to write the network computations in matrix form.. Know what it means for multilayer perceptrons to be universal function approximators, and what are the limitations of this.. Understand how neural nets can be seen as adaptive basis function models.","binary linear classifiers,basis function expansions,matrix multiplication"
finite automata,Theory of Computation,https://metacademy.org/graphs/concepts/finite_automata,"Finite automata are a formal model of computation with finite memory. The machine can be in any of a finite number of states, and it has rules for transitioning between states depending on its input. Finite automata can only perform fairly limited computations, such as matching regular expressions.",formally define finite automata. be able to interpret state diagrams. be able to design finite automata to solve simple tasks,
finite-difference approximations to derivatives,Optimization,https://metacademy.org/graphs/concepts/finite_difference_approximations_to_derivatives,We can estimate the derivative of a function at a point by evaluating the function at nearby points. This gives a useful method for checking gradient computations.,Know how first derivatives can be approximated with finite differences. Why is the two-sided approximation more accurate than the one-sided one?. What are the pitfalls of choosing too large or too small a step size?. Know how finite differences can be used to check gradient calculations,"gradient,partial derivatives"
first-order logic,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/first_order_logic,"First-order logic refers to a class of formal languages which include the propositional connectives, quantifiers, functions, and predicates. It underlies many automated reasoning systems and can be used to define various formalizations of mathematics, such as Peano arithmetic, and Zermelo-Frankl set theory.",know the meanings of existential and universal quantifiers. be able to manipulate expressions containing quantifiers. be able to express simple statements of arithmetic and set theory in FOL. be able to determine if a variable occurs free in an expression. understand the semantics of FOL at an informal level,propositional logic
first-order resolution,Logic,https://metacademy.org/graphs/concepts/first_order_resolution,Resolution is an inference rule for first-order logic which is a key part of many automated theorem provers.,"Be able to eliminate existential quantifiers using Skolemization.. Be able to reduce a set of first-order sentences to conjunctive normal form.. State the resolution rule and be able to apply it to simple examples.. Know how resolution can be used to give constructive proofs, where one desires a set of substitutions to variables which make the consequent true.","first-order logic,propositional resolution,first-order unification"
first-order unification,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/first_order_unification,"Unification is a procedure which takes two symbolic expressions containing variables, and returns a substitution to the variables which makes the expressions identical. It is a key component of first-order theorem provers and type inference systems.","Know what the unification procedure is and what the output is.. Be able to simulate it on simple examples.. What is the occurs check, why is it needed, and why does it increase the complexity of the algorithm?. Why do variables sometimes need to be renamed before performing unification?. What does it mean for one unifier to be more general than another, and why does one desire the most general unifier?",first-order logic
Fisher information,Frequentist Statistics,https://metacademy.org/graphs/concepts/fisher_information,"Fisher information is a quantity from statistics which measures the amount of information a sample from a distribution provides about the parameter value. It has several equivalent definitions, one of which is the variance of the derivative of the log-likelihood. Since it determines the asymptotic behavior of maximum likelihood estimation, it can be used to compute approximate confidence intervals for parameters.",Define Fisher information in terms of the expected second derivative of the log-likelihood. Define Fisher information in terms of the variance of the derivative of the log-likelihood. Show that these definitions are equivalent. Be able to compute Fisher information for simple distributions. Be aware of how Fisher information relates to maximum likelihood estimation,"maximum likelihood,expectation and variance,Taylor approximations"
Fisher information matrix,Frequentist Statistics,https://metacademy.org/graphs/concepts/fisher_information_matrix,"The Fisher information matrix is a generalization of Fisher information to distributions with multiple parameters. Roughly speaking, it determines the amount that observing a sample from the distribution constrains the parameters in any given direction.",Define the Fisher information matrix in terms of the expected outer product of Fisher scores. Define the Fisher information matrix in terms of the covariance of Fisher scores. Define the Fisher information matrix as the expected second derivative of the log-likelihood. Show that these definitions are equivalent. Be able to compute the Fisher information matrix for simple distributions (e.g. Gaussian). Be aware that the Fisher information matrix determines the asymptotic behavior of maximum likelihood estimators,"covariance matrices,Fisher information,positive definite matrices"
Fisher metric,Differential Geometry,https://metacademy.org/graphs/concepts/fisher_metric,The Fisher metric is a Riemannian metric on statistical manifolds. The coordinate representation is the Fisher information matrix. One motivation is that it gives the second-order approximation to some widely used divergences such as KL divergence.,Define the Fisher metric of a statistical manifold. Understand why the Fisher metric is independent of the parameterization. Be able to derive the Fisher metric for a parametric family of distributions. Show that the Fisher metric is additive for the joint distribution over independent random variables. Show that the Fisher metric is the second-order Taylor approximation to the KL divergence between two distributions (which motivates its use as a metric),"Fisher information matrix,covariance matrices,Riemannian metrics,statistical manifolds"
Fisher's linear discriminant,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/fishers_linear_discriminant,Fisher's linear discriminant is a technique for visualizing high-dimensional data belonging to multiple classes by projecting it onto a low-dimensional subspace. The subspace is chosen to maximize the ratio of between-class to within-class variance.,Derive the subspace which maximizes the ratio of between-class and within-class variance.. Why might this projection give better classification results than GDA in the original space?,"projection onto a subspace,Gaussian discriminant analysis,eigenvalues and eigenvectors,optimization problems,covariance matrices"
fitting logistic regression with iterative reweighted least squares,Machine Learning,https://metacademy.org/graphs/concepts/logistic_regression_irls,"One way of fitting logistic regression is using Newton's method. This winds up having an intuitive form, where each update takes the form of a linear regression problem and the data points are all assigned weights depending how far they are from the decision boundary.",,"logistic regression,linear regression: closed-form solution"
floating point representation,Programming,https://metacademy.org/graphs/concepts/floating_point_representation,"Floating points are the main representation most computers use for performing approximate computations on real-valued quantities. They are represented in terms of a sign bit, a mantissa, and an exponent.",Know the basic parts of the floating point representation. How many significant digits can be represented with single- and double-precision floats?. What are the largest and smallest positive values that can be represented as single- and double-precision floats?. Know about the special values inf and NaN. Be aware that 0 and -0 have different representations. Be aware of the pitfalls of testing if two floats are equal,machine representations of integers and characters
flows on manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/flows_on_manifolds,"Given a vector field X on a manifold, an integral curve for X through p is a curve passing through p whose tangent vector matches X at every point. The set of all integral curves can be analyzed in terms of a flow, where each point is transported along the corresponding integral curve. Flows are used to define the Lie derivative.","Know the definition of an integral curve. Show that finding an integral curve (locally) reduces to solving an ordinary differential equation on a coordinate chart. Be aware that the existence of an integral curve is guaranteed locally. Give an example where the integral curve is not defined for all values of the curve parameter t. Know the definition of the flow. Be aware that, under mild conditions, the flow is a group of diffeomorphisms","tangent bundle,groups,differentiable maps between manifolds,linear approximation"
forward-backward algorithm,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/forward_backward_algorithm,"The forward-backward algorithm is an algorithm for computing posterior marginals in a hidden Markov model (HMM). It is based on dynamic programming, and has linear complexity in the length of the sequence. It is used as a component of several other algorithms, such as the Baum_Welch algorithm and block Gibbs sampling in factorial HMMs.",Know what is meant by filtering and smoothing. Know the steps of the forward-backward algorithm. Show that the algorithm gives the correct marginals. Be able to implement the algorithm. Be familiar with the trellis representation of the algorithm. Analyze the computational complexity of the algorithm,"multivariate distributions,hidden Markov models,conditional independence"
four fundamental subspaces,Linear Algebra,https://metacademy.org/graphs/concepts/four_fundamental_subspaces,"The four fundamental subspaces of a matrix A are the column space, nullspace, row space, and left nullspace. The bases of all four spaces can be obtained using Gaussian elimination, and certain of them are orthogonal to one another. There are close relationships between the dimensions of all four spaces, and the dimensions of the row and column spaces both equal the rank of A.","Know the definitions of the four fundamental subspaces: column space, nullspace, row space, and left nullspace. Be able to compute bases for all four subspaces. Know how the four subspaces are related to each other. Know the definition of matrix rank","matrix transpose,Gaussian elimination,subspaces,bases,column space and nullspace"
function pointers in C,Programming,https://metacademy.org/graphs/concepts/function_pointers_in_c,"In C, a function pointer is a pointer to the segment of code implementing a function. It is important for implementing generic data structures in C.","Understand conceptually what a function pointer points to, and what happens when the function pointed to is called. Know the (hideous) syntax for declaring and using function pointers. Understand how function pointers can be used to implement generic functions in C (e.g. a sorting routine with a generic comparator function)","C pointers,call stack"
functions and relations as sets,Set Theory,https://metacademy.org/graphs/concepts/functions_and_relations_as_sets,"Ordered pairs, relations, and functions can all be defined in terms of sets. This is part of why set theory is such a powerful framework in the foundations of mathematics.","Define ordered pairs, functions, and relations in terms of sets. Show that the definitions correspond with our intuitions for how these objects should behave",set operations
functions of several variables,Multivariate Calculus,https://metacademy.org/graphs/concepts/functions_of_several_variables,"Multivariable calculus deals with functions of multiple variables. For two dimensions, we can visualize these with graphs or with level sets.",Know the distinction between dependent and independent variables. Be familiar with the visualization of a function as a graph. Be familiar with the visualization of a function as level sets,
gamma distribution,"Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/gamma_distribution,"The gamma distribution is a continuous distribution which gives the waiting time for n events to occur, when each event is equally likely to happen at any point in time. It is also commonly used in Bayesian statistics as a prior for scale variables.","Know the PDF of the gamma distribution. Show that the distribution is closed under sums. Derive the normalizing constant, expectation, and variance of a gamma distribution. Be aware of the motivation in terms of the waiting time for n events to occur","random variables,gamma function,expectation and variance"
gamma function,Uncategorized,https://metacademy.org/graphs/concepts/gamma_function,"The gamma function is a generalization of factorials to real and complex numbers, e.g. 5.25! isn't well defined, but Gamma(5.25) is well defined. The Gamma function is formally defined as an improper integral that converges. It appears in a number of common distributions, e.g. the beta, gamma, and Dirichlet distribution.",Know the definition of the gamma function. Know how it is related to the factorial function,
Gaussian BP on trees,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/gaussian_bp_on_trees,"Marginalization in Gaussian MRFs can be performed in cubic time by inverting a matrix, but this is too slow for some applications. If the model is tree-structured, belief propagation can compute the means and single-node variances in linear time. Unlike in general MRFs, it turns out that the loopy version yields the correct means.",,"Gaussian MRFs,computations on multivariate Gaussians,sum-product on trees,max-product on trees"
Gaussian discriminant analysis,Machine Learning,https://metacademy.org/graphs/concepts/gaussian_discriminant_analysis,Gaussian discriminant analysis (GDA) is a generative model for classification where the distribution of each class is modeled as a multivariate Gaussian.,"Derive the form of the decision boundary in the two-class case when the within-class covariance matrices are shared. In particular, show that it is always a hyperplane.. What do the decision regions look like when there are more than two classes?. Show that the decision boundary is quadratic when the covariance matrices are not shared between classes.. Optional: show that the decision boundary in the two-class case is equivalent to performing classification with linear least squares .","binary linear classifiers,mixture of Gaussians models,maximum likelihood,covariance matrices,dot product"
Gaussian distribution,Probability Theory,https://metacademy.org/graphs/concepts/gaussian_distribution,"The Gaussian (or normal) distribution has a bell shape, and is one of the most common in all of statistics. The Central Limit Theorem shows that sums of large numbers of independent, identically distributed random variables are well approximated by a Gaussian distribution. The parameter estimates in a statistical model are also asymptotically Gaussian. Gaussians are widely used in probabilistic modeling for these reasons, together with the fact that Gaussian distributions can be efficiently manipulated using the techniques of linear algebra.",Know the PDF of the Gaussian distribution and be aware that it is bell-shaped. Show that linear functions of Gaussian random variables are also Gaussian. Show that the parameters mu and sigma represent the mean and standard deviation. Derive the normalizing constant. Be aware that Gaussian distributions result from the sum of a large number of i.i.d. random variables. Show that the sum of independent Gaussian random variables is Gaussian,"random variables,expectation and variance"
Gaussian elimination,Linear Algebra,https://metacademy.org/graphs/concepts/gaussian_elimination,"Gaussian elimination is an algorithm for solving systems of linear equations, computing matrix inverses, and computing the LU factorization of a matrix.","Know the row operations which are allowed in Gaussian elimination. Know the Gaussian elimination algorithm, and be able to simulate it on simple examples. Know what happens in the algorithm when there is one solution, no solutions, or many solutions",linear systems as matrices
Gaussian MRFs,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/gaussian_mrfs,Gaussian Markov random fields (MRFs) are MRFs where the variables are all jointly Gaussian. The graph structure is reflected in the sparsity pattern of the precision matrix.,,"information form for multivariate Gaussians,Markov random fields"
Gaussian process classification,Machine Learning,https://metacademy.org/graphs/concepts/gaussian_process_classification,"Gaussian process classification is a Bayesian model for nonparametric classification. The data points have associated latent variables which are drawn from a GP prior, and the labels are modeled as stochastic functions of the latent variables.",,"Gaussian process regression,Bayesian logistic regression"
Gaussian process regression,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/gaussian_process_regression,"Gaussian process regression is a Bayesian model for nonparametric regression. (That is, nonparametric in the sense that the complexity of the regression function grows with the amount of data.) The model places a prior directly on the output values without reference to an underlying parametric model.",,"Gaussian processes,Bayesian linear regression"
Gaussian processes,"Bayesian Statistics,Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/gaussian_processes,"Gaussian processes are distributions over functions such that the joint distribution at any finte set of points is a multivariate Gaussian. They are commonly used in probabilistic modeling when we want to put a prior over functions without reference to an underlying parametric representation. Usually they express fairly weak beliefs about the function, such as smoothness, but more structured versions are also possible. The most common use case is nonparametric regression and classification.",,"kernel trick,multivariate Gaussian distribution"
Gaussian variable elimination,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/gaussian_variable_elimination,"Marginalization in Gaussian MRFs can be performed in cubic time by inverting a matrix, but this is too slow for some applications. If the model has the right structure, variable elimination can result in a big speedup.",,"Gaussian MRFs,variable elimination,computations on multivariate Gaussians"
Gaussian variable elimination as Gaussian elimination,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/gaussian_variable_elimination_as_gaussian_elimination,The computations involved in Gaussian variable elimination are the same as those used to compute the Cholesky decomposition of the covariance matrix.,,"Gaussian variable elimination,Gaussian elimination,Cholesky decomposition"
Gauss-Newton algorithm,Optimization,https://metacademy.org/graphs/concepts/gauss_newton_algorithm,"The Gauss-Newton algorithm is a method for solving nonlinear least squares problems, based on iteratively solving linearized versions of the problem. It can also be seen as an approximation to Newton's method where terms corresponding to the curvature of the mapping are dropped.","Know what the Gauss-Newton algorithm is. What approximation is it making, when viewed as an approximation to Newton's method?. Know why the Gauss-Newton matrix, unlike the Hessian matrix, is guaranteed to be positive semidefinite","linear least squares,linear approximation,Newtons method (optimization),positive definite matrices"
generalization,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/generalization,"When we fit a statistical model, we are interested in generalizing, i.e. making good predictions on data we haven't seen yet. We can fail at this in two ways: by underfitting (missing important structure in the data), or by overfitting (where the model is too sensitive to idiosyncrasies in the data). We can measure the generalization error of a model by training it on a ``training set'' and then evaluating it on a separate ``test set.'' Understanding the tradeoffs of model fit vs. complexity and how to measure generalization is key to getting any machine learning algorithm to work in practice.",,linear regression
generalized linear models,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/generalized_linear_models,"Generalized linear models are a class of linear models which unify several widely used models, including linear regression and logistic regression. The distribution over each output is assumed to be an exponential family distribution whose natural parameters are a linear function of the inputs.","Understand the basic assumptions behind a generalized linear model.\nIn particular, the natural parameters are assumed to be a linear function of the inputs. What role does this play in the maximum likelihood update rule?. Show that some other model (e.g. linear regression or logistic regression) is a special case of generalized linear models.. Derive the gradient ascent update rule for GLMs.","exponential families,maximum likelihood,optimization problems,linear regression as maximum likelihood,gradient descent"
generic collections in Java,Programming,https://metacademy.org/graphs/concepts/generic_collections_in_java,"In programming, code that defines or uses container classes should be generic in several senses. Typically, the container should be able to hold objects of differing types. Also, code which uses containers shouldn't rely on a particular implementation, but should call methods which are common to different implementations. Both aspects are tricky in a statically typed language like Java, but Java provides some features, such as parameterized classes, to help deal with this. (Note: you should use Java 5 or later, since earlier versions did not support generics.)","Know the syntax and semantics of parameterized classes in Java. Understand a key motivation for using generic containers: you can then switch to a different underlying implementation if it turns out to be more efficient. Be familiar with Java's collections hierarchy, including Collection, Map, and List, and know what sorts of methods are defined at what level of the hierarchy. Be aware of some useful functions in the Collections class, such as sorting. Know how to define a Comparator class to pass to a generic sorting routine. Be able to define a parameterized class in Java. Understand a common gotcha in Java generics: why can't you pass a List to a function which requires a List? How can you get around this?","iterators,interfaces and abstract classes in Java,inheritance (programming),abstract data types,classes (programming)"
Gibbs sampling,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/gibbs_sampling,Gibbs sampling is a Markov Chain Monte Carlo (MCMC) algorithm where each random variable is iteratively resampled from its conditional distribution given the remaining variables. It's a simple and often highly effective approach for performing posterior inference in probabilistic models.,Know the Gibbs sampling update rules. Understand why Gibbs sampling's stationary distribution is the model distribution. Understand why Gibbs sampling can be inefficient when variables are tightly coupled,"conditional distributions,Markov chain Monte Carlo,Markov random fields"
Gibbs sampling as a special case of Metropolis-Hastings,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/gibbs_as_mh,Gibbs sampling can be seen as a special case of the Metropolis-Hastings algorithm where the transition operators are chosen such that the acceptance probability is 1.,Show that Gibbs sampling can be viewed as a special case of the M-H algorithm,"Metropolis-Hastings algorithm,Gibbs sampling"
Godel numbering,Logic,https://metacademy.org/graphs/concepts/godel_numbering,"One can define a system (""Godel numbering"") for encoding expressions and derivations (in some logical system) as numbers. All the syntactic operations needed to verify derivations are recursive and can be represented in arithmetic. This shows that any logical theory which includes arithmetic is powerful enough to talk about itself. This idea is used to construct self-referential sentences in Godel's Incompleteness Theorem and related results.","Define a system for assigning numbers (""Godel numbers"") to expressions and derivations in a first-order language. Be aware that all the syntactic manipulations needed to verify a proof are representable in arithmetic. Work through the details of the above","first-order logic,proofs in first-order logic,representability in arithmetic"
Godel's Incompleteness Theorems,Logic,https://metacademy.org/graphs/concepts/godels_incompleteness_theorems,"Godel's Incompleteness Theorems are fundamental results showing the limitations of formal mathematics. The First Incompleteness Theorem shows that in any consistent logical system which includes arithmetic, some statements cannot be proved or disproved. Equivalently, the set of true statements about the natural numbers is undecidable. The Second Incompleteness Theorem states that no formal system which includes arithmetic can prove its own consistency, or that of a more powerful theory.",State Godel's First Incompleteness Theorem (in terms of some statements not being provable one way or the other). State the theorem in terms of the theory of natural numbers being undecidable. State Godel's Second Incompleteness Theorem. Be aware of Hilbert's program and why the second Incompleteness Theorem showed it to be impossible. Prove the theorems,"semantics of first-order logic,proofs in first-order logic,Godel numbering,decidability,Church-Turing thesis"
GP classification with the Laplace approximation,Machine Learning,https://metacademy.org/graphs/concepts/gp_classification_laplace,"Unlike with GP regression, there is no closed-form solution to GP classification. The most basic method for approximating it is to use the Laplace approximation, thereby formulating it as an optimization problem.",,"Gaussian process classification,Laplace approximation,fitting logistic regression with iterative reweighted least squares,learning GP hyperparameters"
gradient,"Multivariate Calculus,Optimization",https://metacademy.org/graphs/concepts/gradient,The gradient of a function gives the direction of maximum increase. Its entries are given by the partial derivatives of the function. It is widely used in optimization.,Know the definition of the gradient. Be able to compute the gradient by taking partial derivatives. Be able to use the gradient to compute directional derivatives. Show that the gradient gives the direction of maximum increase. Understand why the gradient is perpendicular to the level surfaces,"dot product,functions of several variables,partial derivatives,linear approximation"
gradient descent,Optimization,https://metacademy.org/graphs/concepts/gradient_descent,"Gradient descent, also known as steepest descent, is an iterative optimization algorithm for finding a local minimum of differentiable functions. At each iteration, gradient descent operates by moving the current solution in the direction of the negative gradient of the function (the direction of ""steepest descent"").",Be able to apply gradient descent to functions of several variables. Why is gradient descent not guaranteed to find the global optimum?,"gradient,functions of several variables"
graph representations,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/graph_representations,"Graphs, both directed and undirected, are one of the fundamental data structures in algorithms. Depending if the graph is sparse or dense, we may prefer to represent it as adjacency lists or an adjacency matrix. Or, we may directly carry our an algorithm on an implicit graph representation without ever constructing a representation of the full graph.","Know what it means for a graph to be sparse or dense. Know what is meant by adjacency matrices and adjacency lists, and when you would want to use one vs. the other. Give an example where it is preferable to operate on an implicit representation of a graph than to represent the graph explicitly",linked lists
Green's Theorem,Multivariate Calculus,https://metacademy.org/graphs/concepts/greens_theorem,Green's Theorem is a theorem relating the integrals of the curl and the divergence of a vector field over a closed region to a line integral along its boundary.,Know the definitions of curl and divergence. Prove two versions of Green's Theorem:\nrelating line integrals to curl\nrelating flux to divergence. Be able to apply Green's Theorem to compute:\nthe area of a closed region\nline integrals\nflux across a curve. Use Green's Theorem to show that conservative vector fields have zero curl,"vector fields,line integrals,multiple integrals,conservative vector fields"
Hamiltonian flows,Differential Geometry,https://metacademy.org/graphs/concepts/hamiltonian_flows,"Suppose one defines a function (called the Hamiltonian) on a symplectic manifold. This defines a Hamiltonian vector field, which is somewhat analogous to the gradient. The flow for this vector field corresponds to simulating Hamiltonian dynamics.","Define the Hamiltonian vector field of a function. Show that the Hamiltonian flow preserves the Hamiltonian. Show that the Hamiltonian flow is a symplectomorphism, i.e. it preserves the symplectic structure. Define the Poisson bracket and be able to manipulate it algebraically. Be able to check if a quantity is conserved using the Poisson bracket. Be aware of the physical interpretation of Hamiltonian flows","cotangent bundle,symplectic manifolds,flows on manifolds,Lie derivatives"
Hamiltonian Monte Carlo,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/hamiltonian_monte_carlo,"Hamiltonian Monte Carlo (HMC) is an MCMC algorithm which makes use of gradient information in order to avoid random walks and move more quickly toward regions of high probability. It is based on a discretization of Hamiltonian dynamics, with a Metropolis-Hastings accept/reject step to ensure that it has the right stationary distribution.",Know the definition of the HMC algorithm. Be aware of an example in which it avoids the random walk behavior of generic M-H. Know that it preserves phase space volume and why that's important. Know what the leapfrog discretization refers to and why it is preferable to the naive discretization. Understand why the acceptance probability increases as the step size decreases and why it approaches 1 for arbitrarily small steps,"Metropolis-Hastings algorithm,gradient descent"
hash tables,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/hash_tables,"A hash table is a data structure which implements a symbol table with constant time lookup, insertion, and deletion operations. It is based on a hash function, which should map a given key to a seemingly random location in an array. Hash tables are one of the most commonly used data structures in programming.","Know what operations a hash table supports as well as their running time. Know what a hash function refers to and what properties are desirable for a hash function. Give examples of pathological hash functions which lead to lots of collisions. Know of a technique for resolving collisions (e.g. chaining). Know a ""quick-and-dirty"" hash function which can be easily implemented. Be able to implement a hash table","C pointers,asymptotic complexity,linked lists,modular arithmetic"
heap (data structure),"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/heap_data_structure,"A heap is a data structure which efficiently implements a priority queue, where one can insert values and retrieve the smallest value, each in logarithmic time. Conceptually, it is implemented as a binary tree where each node is smaller than either of its children. Canonical use cases include heapsort, an efficient sorting algorithm, and the priority queues used in graph search algorithms. (Note that this data structure has nothing to do with the heap where memory is allocated.)","Know which operations a priority queue is expected to support, and the running times of those operations. Know the heapsort algorithm and be able to analyze its running time. Be familiar with the heap data structure: know what invariants it maintains, and how it can be implemented as an array. Understand and be able to implement the INSERT and FIND-MIN heap operations","tree (data structure),asymptotic complexity,sorting"
hidden Markov models,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/hidden_markov_models,"Hidden Markov models (HMMs) are a kind of probabilistic model widely used in speech and language processing. There is a discrete latent state which evolves over time as a Markov chain, and the current observations depend stochastically on the current latent state. HMMs are popular because they support efficient exact inference algorithms.",Know the definition of a hidden Markov model. Know what the homogeneity assumption refers to. Understand why HMMs are not Markov chains over the observed variables. Know how to represent higher order HMMs in terms of standard HMMs. Be familiar with the state transition diagram representation of HMMs. Know what graphical model HMMs correspond to,"conditional distributions,matrix multiplication,random variables,Bayesian networks,conditional independence,Markov chains"
hierarchical Dirichlet process,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/hierarchical_dirichlet_process,"The Hierarchical Dirichlet Process (HDPs) is a stochastic process that can be used to define a nonparametric distribution on a mixture of mixtures (or admixture) model. That is, each grouping of data is a draw from a mixture model, and the mixture components are shared among the different groups. Using a hierarchy of Dirichlet processes allows the number of mixture components to be inferred from the data. HDPs are most commonly used in topic modeling, where the top mixture corresponds to the global set of topics shared among the entire corpus (all documents) and the secondary mixture corresponds to the topic mixture for a given document.",,"Chinese restaurant franchise,Dirichlet process"
higher-order partial derivatives,Multivariate Calculus,https://metacademy.org/graphs/concepts/higher_order_partial_derivatives,"When we take partial derivatives of partial derivatives, we get what are known as higher-order partial derivatives. This can describe local properties of a function which aren't captured by the first-order approximation.",Be able to compute higher-order partial derivatives. Show that higher order partial derivatives are symmetric with respect to the ordering of the variables,"partial derivatives,linear approximation"
HMM inference as belief propagation,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/hmm_inference_as_bp,"The forward-backward algorithm for computing posterior marginals in an HMM can be viewed as a special case of sum-product belief propagation. Similarly, the Viterbi algorithm for computing the most likely state sequence can be viewed as a special case of max-product belief propagation.",Know how an HMM can be represented as a factor graph. Show the equivalence between the forward-backward and sum-product algorithms. Show the equivalence between the Viterbi and max-product algorithms,"forward-backward algorithm,sum-product on trees,max-product on trees,Viterbi algorithm"
Hopfield networks,Machine Learning,https://metacademy.org/graphs/concepts/hopfield_networks,Hopfield networks are a kind of recurrent neural network which implements an associative memory. The behavior of the network can be modeled in terms of minimizing an energy function.,"Know the rules for\nlearning the weights of a Hopfield net\nfinding a low energy configuration. Understand why the dynamics of the network can be described in terms of an energy function.. What happens if you try to store too many memories in a Hopfield net?. Know how Hopfield nets can be applied to optimization problems (e.g. image interpretation, Traveling Salesman)",
IBP linear-Gaussian model,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/ibp_linear_gaussian_model,"The linear-Gaussian IBP model is a simple matrix factorization model, where the model assumes the observed data results from linearly combining a subset of K independent real-valued latent factors: X = Z x A + E, where X is the N x D observed data matrix, Z is the N x K binary latent feature matrix, A is the K x D latent real-valued factor matrix, and E is N x D matrix of iid noise. Using an IBP prior allows the number of latent features, K, to be learned from the data. This model is commonly used for developing new IBP inference techniques.",,"factor analysis,Indian buffet process,CRP clustering,collapsed Gibbs sampling"
importance sampling,"Bayesian Statistics,Probabilistic Graphical Models,Probability Theory",https://metacademy.org/graphs/concepts/importance_sampling,"Importance sampling is a way of estimating expectations under an intractable distribution p by sampling from a tractable distribution q and reweighting the samples according to the ratio of the probabilities. While importance sampling has unreasonably large variance when applied naively, it forms the basis for some very effective Monte Carlo estimators.","Know the definition of importance sampling, both the normalized and unnormalized versions. Understand why the unnormalized version is unbiased but the normalized version isn't. Why do we still normally prefer the normalized version?. Know what the effective sample size refers to","conditional distributions,Monte Carlo estimation"
incompleteness of set theory,"Logic,Set Theory",https://metacademy.org/graphs/concepts/incompleteness_of_set_theory,"Godel's Incompleteness Theorems apply to any formal deductive system which includes arithmetic. Since the natural numbers can be defined using set theory, the theorems apply to set theory as well.",Extend Godel's Incompleteness Theorems (stated in terms of arithmetic) to axiomatic set theory,"Zermelo-Frankl axioms,Godels Incompleteness Theorems,natural numbers as sets,interpretations between theories"
independent component analysis,Machine Learning,https://metacademy.org/graphs/concepts/independent_component_analysis,Independent component analysis (ICA) is a latent variable model where the observations are modeled as linear combinations of latent variables which are usually drawn from a heavy-tailed distribution. Common uses include source separation and sparse dictionary learning.,Know what modeling assumptions are made by independent component analysis. Know of one algorithm for fitting ICA (e.g. gradient ascent on the likelihood). Why do the latent variables need to be non-Gaussian?. Be aware that the data is first preprocessed with PCA. Know how ICA can be used for source separation,"maximum likelihood,orthonormal bases,principal component analysis,optimization problems,heavy-tailed distributions,determinant,multivariate Gaussian distribution"
independent events,Probability Theory,https://metacademy.org/graphs/concepts/independent_events,"Intuitively, two events are independent if the first event happening doesn't influence whether the second is likely to occur. Mathematically, some set of events are independent if the joint probability of some subset of the events decomposes into a product of the probabilities of the individual events. In statistics and AI, a probabilistic model often must make independence assumptions in order for things to be efficiently computable.","Know what it means for two events to be independent. Know that if A is independent of B, then A is also independent of B's complement. What does it mean for multiple events to be independent, and why is this stronger than pairwise independence?. Be able to do probability calculations involving independent events","conditional probability,probability"
independent random variables,Probability Theory,https://metacademy.org/graphs/concepts/independent_random_variables,"Intuitively, two random variables are independent if they don't influence each other. Mathematically, two random variables are independent if the events associated with each random variable lying in some set are independent. In statistics and probabilistic modeling, different random variables are often assumed to be independent in order to allow for efficient estimation and inference.","Define independent random variables in terms of independent events. Know several equivalent definitions of independence, in terms of the PDF, PMF, and CDF of the joint distribution.","multivariate distributions,random variables,independent events"
Indian buffet process,"Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/indian_buffet_process,"The Indian Buffet Process (IBP) is a generative model for random ""feature allocations"" (a ""feature allocation"" is analogous to a clustering except a given datum can belong to more than one cluster). So while the Chinese Restaurant Process describes a generative model for dividing N integers (customers) into K partitions (table assignments), the IBP describes a generative model for dividing N integers into K subsets, where each integer can occur in an arbitrary number of subsets. These subsets are known as ""features"" and the entire set is known as a ""feature allocation"". Note: the IBP has a more formal definition in probability theory where it is known as the marginalized distribution of a Beta process.",,"Chinese restaurant process,beta distribution,Poisson distribution,gamma function"
inference in MRFs,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/inference_in_mrfs,"One reason we build graphical models is so we can perform inference, i.e. ask questions about the distribution. The most common queries include: (1) finding the marginal distribution of one or several nodes, (2) finding the most likely joint assignment, or (3) computing the partition function. Items (1) and (3) are closely related. While exact inference is intractable in the general case, there are powerful approximate inference algorithms, as well as interesting classes of tractable models.",,Markov random fields
information form for multivariate Gaussians,"Machine Learning,Probabilistic Graphical Models,Probability Theory",https://metacademy.org/graphs/concepts/multivariate_gaussians_information_form,"While we normally represent multivariate Gaussians in terms of their mean and covariance, information form is often a useful alternative. The distribution is represented in terms of a quadratic ""energy function."" This representation is convenient for conditioning, and is the basis for Gaussian Markov random fields.",,multivariate Gaussian distribution
inheritance (programming),Programming,https://metacademy.org/graphs/concepts/inheritance_programming,"In object oriented programming, inheritance refers to a situation where a more specific class (the ""subclass"") inherits properties from a more general class (the ""superclass""). Inheritance is commonly used to share implementations between classes with similar functionality.","Know some basic terminology: inheritance, subclass, superclass, class hierarchy, overriding. In a programming language such as Java, C++, or Python, know the syntax for defining subclasses and the rules for overriding methods. Understand how inheritance can be useful for code reuse. Be able to interpret an inheritance diagram. Be able to design an appropriate class hierarchy for a problem. If the language is statically typed, understand why a variable of type S can be assigned a value which is a subclass of S but not a superclass",classes (programming)
inner product,Linear Algebra,https://metacademy.org/graphs/concepts/inner_product,"An inner product is a kind of mathematical operator defined on a vector space which generalizes the dot product. It can be used to generalize notions like length, orthogonality, and angles to vector spaces other than the Euclidean one.",Know the definition of inner product. Know how the inner product defines a norm. Define orthogonality in terms of the inner product. Prove the Cauchy-Schwartz inequality. Prove the Triangle Inequality,"vector spaces,dot product"
integration on manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/integration_on_manifolds,"One can define integration of differential forms on manifolds in a way that is independent of the coordinate charts. This generalizes the notions of line integrals, volume, and surface integrals.",Show the existence of a partition of unity on a manifold. Define the integral of a differential form on an oriented manifold. Show that this is independent of the choice of coordinate charts and partition of unity. Define the integral of a volume form (or density) over a (possibly non-orientable) manifold. Be able to compute the integrals of differential forms and volume forms in simple cases. Understand how a Riemannian metric gives a natural volume form,"oriented manifolds,compact sets,Riemannian metrics,differential forms,multiple integrals,line integrals"
interfaces and abstract classes in Java,Programming,https://metacademy.org/graphs/concepts/interfaces_and_abstract_classes_in_java,"In Java, one often wants to define a class hierarchy where it doesn't make sense to specify a particular implementation of the superclass. A good example is the Collection, a type which includes container implementations as diverse as arrays and hash tables. Interfaces and abstract classes are two ways to specify abstract types which can't themselves be instantiated, but which can be used to write polymorphic code.","Know the syntax and semantics of Java interfaces and abstract classes. Know how interfaces and abstract classes can be used to write polymorphic code. Know how interfaces can be used to get around Java's restriction against multiple inheritance. When would you use an interface, and when would you use an abstract class?","abstract data types,classes (programming),inheritance (programming)"
interpretations between theories,Uncategorized,https://metacademy.org/graphs/concepts/interpretations_between_theories,Somtimes one mathematical theory T1 (e.g. set theory) is powerful enough to define the objects in and derive all the theorems of another theory T0 (e.g. Peano arithmetic). An interpretation from T0 into T1 is a systematic way of translating statements in T0 into the corresponding statements in T1.,Define the notion of an interpretation function from one first-order theory to another. Be able to show that an interpretation function preserves the semantics of a theory,"semantics of first-order logic,recursion theorem"
iterators,Programming,https://metacademy.org/graphs/concepts/iterators,"In programming, an iterator (sometimes called a generator) is an object which allows the user to iterate over the elements of a container in order. In many high-level programming languages, common iteration idioms (such as foreach loops) implicitly use iterators.","Know what the iterator ADT is and what operations it supports. Be able to use an iterator by explicitly calling the methods. Know the language's implicit iteration construct (e.g. a foreach loop) and how it is actually using the iterator. Understand the advantage of iterators over for loops with explicit indexing. Be able to implement an iterator (if you are using Python, this can be done implicitly with a generator, but try to understand how this actually works). Be aware of the pitfalls of modifying a collection while it is being iterated over","abstract data types,classes (programming)"
Jeffreys prior,Bayesian Statistics,https://metacademy.org/graphs/concepts/jeffreys_prior,"The Jeffreys prior is a kind of uninformative prior defined in terms of Fisher information, and motivated in terms of transformation invariance.",Know the definition of the Jeffreys prior. Be aware of its motivation in terms of invariance to transformation,"Bayesian parameter estimation,uninformative priors,Fisher information"
Jensen's inequality,"Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/jensens_inequality,"Jensen's Inequality states that the expectation of a convex function is larger than the function of the expectation. It is used to prove the Rao-Blackwell theorem in statistics, and is the basis behind many algorithms for probabilistic inference, including Expectation-Maximization (EM) and variational inference.",Know the statement of Jensen's inequality. Prove Jensen's inequality,"expectation and variance,convex functions"
junction trees,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/junction_trees,"The sum-product algorithm is a way of computing marginals in a tree-structured graphical model. The junction tree algorithm generalizes this to arbitrary graphs by grouping together variables into cliques, such that the cliques form a tree.",,"variable elimination,sum-product on trees"
K nearest neighbors,Machine Learning,https://metacademy.org/graphs/concepts/k_nearest_neighbors,"K nearest neighbors is a very simple machine learning algorithm which simply averages the labels of the K nearest neighbors in the training set. It is a canonical example of a nonparametric learning algorithm. It has the advantage that it can learn arbitrarily complex functions, but it is especially sensitive to the curse of dimensionality.","Know what the K nearest neighbors algorithm is.. Be aware of the tradeoffs of KNN vs. parameteric models, including:\nKNN requires no work at training time, but lots of work at test time\nKNN can learn arbitrarily complex functions\nKNN requires storing the entire training set\nKNN is especially prone to the curse of dimensionality. Derive the asymptotic classification error of KNN, both with K = 1 and with K increasing.",independent events
Kalman filter,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/kalman_filter,"The Kalman filter is an algorithm for inference in linear dynamical systems. Specifically, the task is to infer the posterior over the current latent state given past observations. It forms the basis for approximate inference algorithms in more general state space models.",Know what is meant by filtering and smoothing. Know the steps of the Kalman filter algorithm. Analyze the running time of the algorithm,"conditional distributions,computations on multivariate Gaussians,multivariate distributions,linear dynamical systems"
Kalman filter derivation,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/kalman_filter_derivation,Mathematical derivation of the Kalman filter.,Prove the correctness of the Kalman filter,"computations on multivariate Gaussians,Kalman filter"
Kalman smoother,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/kalman_smoother,Kalman smoothing is a posterior inference algorithm for linear dynamical systems (LDSs). It computes posterior marginals for all time steps conditioned on all of the observations. It is used in parameter learning for LDSs.,Know the steps of the Kalman smoother. Analyze the running time,Kalman filter
Kalman smoothing as forward-backward,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/kalman_as_forward_backward,Kalman smoothing can be seen as a special case of the forward-backward algorithm for inference in HMMs. This leads to a simpler derivation than the classical one.,Show that the Kalman filter can be seen as a special case of the forward-backward algorithm,"forward-backward algorithm,Kalman smoother"
kernel SVM,Machine Learning,https://metacademy.org/graphs/concepts/kernel_svm,"The main advantage of the SVM as a linear classifier is that it can be kernelized in order to represent complex nonlinear decision boundaries. Conveniently, since only a (hopefully) sparse subset of the training examples are used, kernels only need to be computed with a small fraction of the training examples. Kernel SVMs are one of the most widely used classifiers in machine learning, because off-the-shelf tools often perform very well.",,"SVM optimality conditions,kernel trick"
kernel trick,Machine Learning,https://metacademy.org/graphs/concepts/kernel_trick,"We can use linear models to model complex nonlinear functions by mapping the original data to a basis function representation. Such a representation can get unweildy, however. The kernel trick allows us to implicitly map the data to a very high (possibly infinite) dimensional space by replacing the dot product with a more general inner product, or kernel.",,"basis function expansions,positive definite matrices,ridge regression"
KKT conditions,Optimization,https://metacademy.org/graphs/concepts/kkt_conditions,The Karush-Kuhn-Tucker (KKT) conditions are a set of optimality conditions for optimization problems in terms of the optimization variables and Lagrange multipliers.,Understand why primal and dual solutions correspond to upper and lower bounds on the optimal value. Know the statements of the KKT conditions for nonconvex and convex optimization problems. Be able to derive the KKT conditions for a convex optimization problem,"gradient,Lagrange duality,convex optimization"
KL divergence,Probability Theory,https://metacademy.org/graphs/concepts/kl_divergence,"KL divergence, roughly speaking, is a measure of the distance between two probability distributions P and Q, and corresponds to the number of extra bits required to encode samples from P using an optimal code for Q. It is not truly a distance function, because it's not symmetric and it doesn't satisfy the triangle inequality. Despite this, it's widely used in information theory and probabilistic inference.",Know the definition of KL divergence.. Derive some basic properties:\nthat it is nonnegative\nthat the KL divergence between a distribution and itself is 0. Show that it is not a true distance metric because\nit is not symmetric\nit doesn't satisfy the triangle inequality,"entropy,expectation and variance"
k-means,Machine Learning,https://metacademy.org/graphs/concepts/k_means,"K-means is a clustering algorithm, i.e. a way of partitioning a set of data points into ""clusters,"" or sets of data points which are similar to one another. It works by iteratively reassigning data points to clusters and computing cluster centers based on the average of the point locations. It is commonly used for vector quantization and as an initialization for Gaussian mixture models.",What objective function does the k-means algorithm minimize?. What happens during the two stages of each k-means iteration?. Is it possible that the k-means algorithm increases the objective function in a full iteration?. What are some scenarios in which k-means gets stuck in poor local optima?\nWhat are some general techniques to overcome these scenarios?. What are some techniques for choosing the initial points of the clusters?. What are some techniques for choosing the number of clusters?,
k-means++,Machine Learning,https://metacademy.org/graphs/concepts/k_means_pp,"The k-means++ algorithm is a simple stochastic procedure for choosing the initial cluster centers for the classic k-means algorithm. This initialization guarantees that the expected objective of the final clustering solution will be within a constant factor of the optimal objective. Briefly stated, k-means++ selects the initial cluster centers as follows: the first center is chosen randomly from the input data points. Next, each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center.","Understand the k-means++ initialization procedure, how it's a stochastic algorithm, and the interpretation of the optimality guarantee","k-means,probability"
Lagrange duality,Optimization,https://metacademy.org/graphs/concepts/lagrange_duality,"The Lagrange dual of a convex optimization problem is another convex optimization problem where the optimization variables are the Lagrange multipliers of the original problem. It leads to surprising relationships between seemingly different optimization problems. Duality is commonly used in approximation algorithms, since constraining the dual corresponds to relaxing the original problem.",Know the definition of the Lagrangian function. Know how the Lagrangian can be interpreted as a lower bound on the function (where the function is defined to be infinity outside the feasible set). Know the definition of the Lagrange dual problem. Be able to construct the dual problem for simple convex optimization problems,"Lagrange multipliers,convex optimization"
Lagrange multipliers,Optimization,https://metacademy.org/graphs/concepts/lagrange_multipliers,"Lagrange multipliers are a tool for solving optimization problems with equality or inequality constraints. In particular, some linear combination of the gradients of the constraints must match the gradient of the function. Lagrange multipliers are a key idea behind Lagrange duality, a central concept in convex optimization.",Know what equation must be satisfied by the optimal solution to an optimization problem with one or more equality or inequality constraints. Be able to solve simple optimization problems using Lagrange multipliers. Why doesn't solving this equation imply global optimality?,"gradient,partial derivatives,optimization problems"
Laplace approximation,Machine Learning,https://metacademy.org/graphs/concepts/laplace_approximation,"The Laplace approximation is a way of approximating Bayesian parameter estimation and Bayesian model comparison. It is based on a second-order Taylor approximation of the log posterior around the MAP estimate, which results in a Gaussian approximation to the posterior.",Know the formula for the Laplace approximation. Derive the formula from the Taylor approximation to the log-posterior,"Bayesian parameter estimation,Bayesian model comparison,Taylor approximations,MAP parameter estimation,multivariate Gaussian distribution"
LASSO,Machine Learning,https://metacademy.org/graphs/concepts/lasso,"The Lasso is a form of regularized linear regression. Unlike ridge regression, it puts an L1 penalty on the weights, which encourages sparsity, i.e. it encourages most of the weights to be exactly zero. The general trick of using L1 norms to encourage sparsity is widely used in machine learning.",,"linear regression,optimization problems,ridge regression"
latent Dirichlet allocation,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/latent_dirichlet_allocation,"Latent Dirichlet Allocation (LDA) is a probabilistic mixture of mixtures (or admixture) model for grouped data. It is most commonly used as a topic model, where the observed data is the words and the groups are the individual documents. In the LDA topic model, the observed data (words) within the groups (documents) are the result of probabilistically choosing words from a specific topic (multinomial over the vocabulary), where the topic is itself drawn from a document-specific multinomial that has a global Dirichlet prior.",,"probabilistic Latent Semantic Analysis,Bayesian parameter estimation: multinomial distribution"
latent semantic analysis,Machine Learning,https://metacademy.org/graphs/concepts/latent_semantic_analysis,"Latent Semantic Analysis (LSA), or Latent Semantic Indexing (LSI), is a statistical technique typically used for analyzing relationships between a set of documents and the terms they contain. At its core, LSA performs singular value decomposition (SVD) on a term-by-document count matrix of a corpus and interprets the SVD factors as the ""topics"" of the documents. We can then use these resulting factors (topics) to determine the document-document, document-term, and term-term similarities in the given corpus.",,singular value decomposition
learning Bayes net parameters with missing data,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/learning_bayes_nets_missing_data,"There is no closed-form solution for the maximum likelihood parameters of a Bayes net when some of the variables are unobserved. However, it is possible to apply the EM algorithm, where the E step involves computing marginals and the M step involves computing the maximum likelihood parameters with fully observed data.","Be able to use the EM algorithm to learn Bayes net parameters when some of the variables are unobserved.\nKnow how to derive the update rules.\nKnow how you would implement it if you're given toolboxes for inference and for parameter learning with fully observed data. What outputs are needed from the inference algorithm?. What is the missing at random assumption, and why is it needed to apply EM?. In the fully observed case, maximum likelihood decomposed into separate estimation problems for each clique. Why doesn't that happen when there is missing data?\nAnd why does the decomposition hold in the M step?. Give an example where the likelihood function is multimodal (and therefore you shouldn't always expect to the global optimum).. Give an example where the model is unidentifiable, i.e. multiple parameter settings are equally good.","Bayes net parameter learning,Expectation-Maximization algorithm,maximum likelihood,inference in MRFs"
learning GP hyperparameters,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/learning_gp_hyperparameters,"In order to apply Gaussian processes in practice, it is necessary to fit the hyperparameters of the model, such as the lengthscale and variance of a squared-exp kernel. Marginal likelihood is one commonly used criterion for doing so.",,"Gaussian process regression,Bayesian model comparison"
learning invariances in neural nets,Machine Learning,https://metacademy.org/graphs/concepts/learning_invariances_in_neural_nets,"The human visual system is capable of recognizing objects despite changes in factors such as location, orientation, and lighting. We'd like the representations learned by neural networks to be invariant to at least some of these things as well. There are several different strategies for achieving this, including enforcing invariance in the network architecture, using an appropriate regularization term, or generating randomly perturbed training data.",,"backpropagation,generalization"
learning linear dynamical systems,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/learning_linear_dynamical_systems,"We can perform maximum likelihood estimation for the parameters of a linear dynamical system using the EM algorithm. The E step involves running a Kalman smoother, and the M step involves maximum likelihood inference in multivariate Gaussians.","Derive the EM update rules for linear dynamical systems. Be aware of some pitfalls of maximum likelihood estimation: non-identifiability of the system, and instability of the system","Kalman smoother,Expectation-Maximization algorithm,maximum likelihood: multivariate Gaussians,linear dynamical systems"
Lie derivatives,Differential Geometry,https://metacademy.org/graphs/concepts/lie_derivatives,The Lie derivative determines how an object (such as a vector field or a differential form) changes as it is acted on by a flow. The Lie derivative of a vector field is equivalent to the Lie bracket operator.,"Define the Lie derivatives of functions, vector fields, and tensor fields in terms of flows. Be able to compute Lie derivatives explicitly in terms of coordinate charts. Define the Lie bracket operator. Show that the Lie derivative of a vector field is equivalent to the Lie bracket. Be able to manipulate Lie brackets and Lie derivatives algebraically","tangent bundle,flows on manifolds,tensor fields on manifolds"
limited memory BFGS,Optimization,https://metacademy.org/graphs/concepts/limited_memory_bfgs,"Limited-memory BFGS (L-BFGS) is a quasi-Newton optimization method, and one of the most popular optimization methods for problems with large numbers of variables. It is based on BFGS, but rather than store the approximate inverse Hessian explicitly, it maintains an implicit representation as a product of rank-one-plus-identity factors. All but the most recent K factors are dropped, which keeps the storage requirements bounded.",Understand how the approximate inverse Hessian computed in BFGS can be stored implicitly as a product of low-rank-plus-identity factors. Compare the memory requirements of BFGS versus L-BFGS,BFGS
limits and continuity in R^n,Multivariate Calculus,https://metacademy.org/graphs/concepts/limits_and_continuity_in_rn,"The concept of a limit extends naturally to functions of multiple variables, and higher-dimensional limits share many of the same properties as the single-variable ones.","Know the definition of a limit in R^n. Show that the limit of a sum/product is the sum/product of the limits. Know the definition of a continuous function. Give an example where the limit of a function does not exist at some point p, even though there is a limit coming from any given direction",functions of several variables
line integrals,Multivariate Calculus,https://metacademy.org/graphs/concepts/line_integrals,The line integral gives a notion of integrating a vector field along a curve. Common uses include computing work done by a force and finding potential functions corresponding to a gradient field.,"Be able to compute line integrals of vector fields with respect to curves. Be able to express the line integral in terms of the arc length parameterization, or in terms of an arbitrary parameterization. Why does the line integral of a vector field depend on the orientation of a curve?. Be aware that the line integrals are independent of the parameterization of a path, for a given orientation","functions of several variables,vector fields"
line search,Optimization,https://metacademy.org/graphs/concepts/line_search,"In optimization algorithms such as gradient descent, a careful choice of step size can be important for good performance. Line search is a class of methods for choosing a step size large enough to make progress, but not so large as to cause instability.","Understand the pitfalls of choosing a step size which is too small or too large. Understand why it can be computationally expensive to exactly minimize the objective along the search direction. Know of at least one condition which prevents the step size from being too large (e.g. Wolfe or Goldstein). Know of at least one condition or method which prevents the step size from being too small (e.g. Wolfe, Goldstein, or backtracking). Know at least one practical line search algorithm",gradient descent
linear approximation,"Linear Algebra,Multivariate Calculus",https://metacademy.org/graphs/concepts/linear_approximation,A function is differentiable at a point x if it can be approximated by a linear function around x. The linear approximation can be computed in terms of the partial derivatives at x.,"Know the definition of a differentiable function (in terms of existence of a linear approximation). Be able to compute the linear approximation by taking partial derivatives. Know that differentiability implies continuity. Prove this. Know that if the partial derivatives are continuous, then the function is differentiable. Prove this. Know how the linear approximation can be interpreted as a tangent plane","dot product,partial derivatives,limits and continuity in R^n"
linear dynamical systems,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/linear_dynamical_systems,"Linear dynamical systems (LDSs) are a kind of probabilistic model where a latent state evolves over time, all the variables are jointly Gaussian, and all the dependencies are linear. They are commonly used in robotics, computer vision (for tracking), and time series modeling. They are useful because we can perform exact posterior inference using Kalman filtering and smoothing. Algorithms for LDSs form the basis for analogous techniques in more general state space models, where some of the assumptions are not satisfied.",Know the definition of a linear dynamical system. Know of some representative examples,"conditional distributions,conditional independence,multivariate Gaussian distribution"
linear least squares,Linear Algebra,https://metacademy.org/graphs/concepts/linear_least_squares,"Linear least squares gives a value of x which minimizes the norm of Ax - b. It is well defined even in cases where Ax = b has no solution. It is the basis of linear regression, one of the most widely used methods in statistics.","Understand the motivation of least squares, in terms of situations where Ax=b has no solution. Know the normal equations which characterize a solution. Know how linear least squares is used in line fitting. Know how the least squares solution can be interpreted as a projection","matrix transpose,linear systems as matrices,partial derivatives,projection onto a subspace,four fundamental subspaces"
linear regression,"Linear Algebra,Machine Learning",https://metacademy.org/graphs/concepts/linear_regression,"Linear regression is an algorithm for learning to predict a real-valued ``target'' variable as a linear function of one or more real-valued ``input'' variables. It is one of the most widely used statistical learning algorithms, and with care it can be made to work very well in practice. Because it has a closed-form solution, we can exactly analyze many properties of linear regression which have no exact form for other models. This makes it a useful starting point for understanding many other statistical learning algorithms.","Know the cost function for linear regression. Know of at least one method for fitting the model (e.g. gradient descent, closed form solution)",matrix multiplication
linear regression as maximum likelihood,Machine Learning,https://metacademy.org/graphs/concepts/linear_regression_as_maximum_likelihood,"One way to solve a standard linear regression problem, y=w*x, is to assume the likelihood of the observed y, p(y; w*x, sigma^2) is Gaussian. This assumption means that we believe the observed values of y are a deterministic function of w*x plus some random Gaussian noise: y = w*x + e, where e is random Gaussian noise. If we assume a known sigma, the maximum likelihood estimator for w is obtained by minimizing the sum-of-squares error, Sum[(y-w*x)^2] for all y and x pairs, which has a closed form solution.",,"linear regression,maximum likelihood"
linear regression: closed-form solution,"Linear Algebra,Machine Learning",https://metacademy.org/graphs/concepts/linear_regression_closed_form,Linear regression has a closed-form solution in terms of basic linear algebra operations. This makes it a useful starting point for understanding many other statistical learning algorithms.,Derive the optimality conditions by setting the gradient equal to zero. Know why the matrix in the closed-form solution might not be invertible and what can be done in this case,"linear least squares,linear regression"
linear systems as matrices,Linear Algebra,https://metacademy.org/graphs/concepts/linear_systems_as_matrices,We can represent systems of linear equations in terms of matrices and vectors. This allows us to analyze and solve them using the tools of linear algebra.,"Know how systems of linear equations can be compactly represented in terms of matrix-vector multiplication. Give examples of overdetermined and underdetermined systems, and systems with a unique solution",vectors
linear transformations as matrices,Linear Algebra,https://metacademy.org/graphs/concepts/linear_transformations_as_matrices,"Linear transformations from one vector space to another can be represented as matrices if a basis is given for each space. Common examples include projections, reflections, rotations, and scaling. Linear combinations, compositions and inverses of transformations can be analyzed in terms of matrix operations.","Know what is meant by a linear transformation, and what properties it must satisfy. Show that matrix-vector products are linear. Be able to recover the matrix for a linear transformation in the standard basis for R^n. Why do linear combinations of linear transformations correspond to linear combinations of matrices?. Be familiar with several examples of transformations represented as matrices: reflection, rotation, and scaling",dot product
linear-Gaussian models,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/linear_gaussian_models,"A linear-Gaussian model is a Bayes net where all the variables are Gaussian, and each variable's mean is linear in the values of its parents. They are widely used because they support efficient inference. Linear dynamical systems are an important special case.",,"multivariate Gaussian distribution,Bayesian networks"
linked lists,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/linked_lists,The linked list is one of the fundamental data structures in programming. Each node may have a link to a successor node. Linked lists can be used to implement stacks and first-in-first-out queues.,Be able to implement a linked list. Be able to analyze the running times of common operations on linked lists. Understand the tradeoffs of linked lists vs. arrays,"C pointers,recursion (programming)"
Lob's Theorem,Logic,https://metacademy.org/graphs/concepts/lobs_theorem,"Lob's Theorem asserts that no formal system T can prove statements of the form ""If A is provable in T, then A is true"" unless it is also capable of proving A directly. It can be seen as a generalization of Godel's Second Incompleteness Theorem.",Know the statement of Lob's Theorem and why it is significant. Show how to derive Godel's Second Incompleteness Theorem as a special case of Lob's Theorem. Prove Lob's Theorem,"semantics of first-order logic,proofs in first-order logic,Godel numbering,Godels Incompleteness Theorems"
local search,Symbolic AI,https://metacademy.org/graphs/concepts/local_search,"Local search algorithms are search algorithms which only maintain the current state, and not the way in which it was constructed. Local search algorithms often take the form of ""hill climbing,"" where the state is iteratively replaced with a nearby one which improves the objective function.","What distinguishes local search algorithms from other search algorithms?. Know some terminology: current state, objective function, hill climbing, global maximum, local maximum, plateau. Why do local optima and plateaux make the search difficult?. Why are random restarts often a good idea?",search problems
logistic regression,Machine Learning,https://metacademy.org/graphs/concepts/logistic_regression,"Logistic regression is a machine learning model for binary classification, i.e. learning to classify data points into one of two categories. It's a linear model, in that the decision depends only on the dot product of a weight vector with a feature vector. This means the classification boundary can be represented as a hyperplane. It's a widely used model in its own right, and the general structure of linear-followed-by-sigmoid is a common motif in neural networks.",Know what assumption logistic regression makes about p(y|x). Know of one method for fitting the maximum likelihood solution (e.g. gradient descent). Understand why logistic regression uses a linear decision boundary to classify data. Understand why it's common to use a regularization term (such as the squared norm of the weights),"binary linear classifiers,linear regression as maximum likelihood,ridge regression,optimization problems"
log-linear MRFs,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/log_linear_mrfs,"Often, when we use MRFs, we want to assign a particular functional form to the cliques. A common choice is a log-linear representation, where the potentials are log-linear functions of the model parameters. Boltzmann machines and Gaussian MRFs are probably the most common examples.",,Markov random fields
long short-term memory (LSTM),Uncategorized,https://metacademy.org/graphs/concepts/long_short_term_memory,"Long Short-Term Memory (LSTM) is one of the most widely used RNN architectures today. It's built out of LSTM blocks, which have read and write gates which control how it talks to other units in the network, as well as forget gates which determine whether it erases its previous value. They are good at modeling long sequences, partly because they make the exploding/vanishing gradient problem less severe.","Be familiar with the LSTM block architecture. In particular, know what happens when the various gates are set to different values.. Understand why this helps attenuate the problem of exploding/vanishing gradients.","exploding and vanishing gradients,recurrent neural networks"
loopy belief propagation,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/loopy_belief_propagation,"The sum-product and max-product algorithms give exact answers for tree graphical models, but if we apply the same update rules on a general graph, it often gives pretty reasonable results. This is known as loopy belief propagation, and it is a widely used approximate inference algorithm in coding theory and low level vision.",,"sum-product on trees,max-product on trees"
loopy BP as variational inference,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/loopy_bp_as_variational,"Loopy belief propagation sounds like a hack, but it can be interpreted as a variational inference algorithm. In particular, it is a fixed point update for an approximation to variational inference, where both the energy functional and the marginal polytope are approximated. While this analysis doesn't lead to any strong guarantees, it is the basis for generalizations of loopy BP which have stronger guarantees.",,"loopy belief propagation,variational inference,Lagrange multipliers"
loss function,Uncategorized,https://metacademy.org/graphs/concepts/loss_function,"A loss function or cost function is a function that maps the outcome of a decision to a real-valued cost associated with that outcome. Loss functions are common in machine learning, information theory, statistics, and mathematical optimization, and help guide decision making under uncertainty.",,
Lowenheim-Skolem theorems,"Logic,Set Theory",https://metacademy.org/graphs/concepts/lowenheim_skolem_theorems,"The Lowenheim-Skolem theorems state that if a first-order theory has an infinite model, then it has models of every infinite cardinality. It implies that certain foundational theories in mathematics, such as Peano arithmetic and Zermelo-Frankl set theory, must have nonstandard models.",Know the statements of the upward and downward L-S theorems.. Prove the upward and downward L-S theorems.. Know what Skolem's Paradox refers to and why it is paradoxical.. Know why the L-S theorems imply nonstandard models of the Peano Axioms.,"semantics of first-order logic,cardinality,completeness of first-order logic,compactness of first-order logic,Zermelo-Frankl axioms,countable sets,Peano axioms"
lower bound on sorting,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/lower_bound_on_sorting,"It can be shown that any general purpose sorting algorithm, i.e. one which uses only comparisons, requires at least O(n log n) operations in the worst case. This motivates the need for special purpose sorting algorithms which exploit additional problem structure.","Know what a comparison-based sorting algorithm is, and give examples which do and do not fall in this category. Prove that any comparison-based sorting algorithm requires at least O(n log n) operations","Stirlings approximation,sorting"
LU factorization,Linear Algebra,https://metacademy.org/graphs/concepts/lu_factorization,The LU factorization is a factorization of a matrix into a lower triangular and an upper triangular matrix. It can be computed by recording the row operations used in Gaussian elimination. It can be a more efficient and numerically stable method of solving linear systems compared to matrix inverses.,Know how the row operations of Gaussian elimination can be represented as matrix multiplications. Why does Gaussian elimination without row exchanges give an LU factorization?. Know how Gaussian elimination with row exchanges can be represented with an additional permutation matrix. Show that the LU factorization is unique,"matrix transpose,Gaussian elimination,matrix multiplication,matrix inverse"
machine representations of integers and characters,Programming,https://metacademy.org/graphs/concepts/machine_representations_of_integers_and_characters,"In most programming languages, integers are stored in memory using a particular, standardized binary representation. (At least, standard for any given hardware architecture.) Characters are represented similarly. Understanding this representation is important when performing bitwise operations, and it explains why overflow bugs are possible.","Understand how to represent positive integers in binary. Understand how (fixed-size) integers are represented in memory. How many different values can be represented in a given number of bytes?. Know the difference between signed and unsigned ints, and the meaning of the sign bit in the two's complement representation; understand why the sign bit was designed this way (as opposed to simply negating the number). What is the difference between shorts, ints, and longs in C?. Know the distinction between big endian and little endian representations. Be familiar with the ASCII representation system, and know why capital and lowercase letters start at the places they do",
MAP parameter estimation,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/map_parameter_estimation,"In Bayesian parameter estimation, unless the prior is specially chosen, often there's no analytical way to integrate out the model parameters. In these cases, maximum a posteriori (MAP) estimation is a common approximation, where we choose the parameters which maximize the posterior. Although this is computationally convenient, it has the drawbacks that it's not invariant to reparameterization, and that the MAP estimate may not be typical of the posterior.",Know the definition of the MAP parameter estimate. Why might the mode of the posterior be an atypical point?. Why isn't the MAP estimate invariant to reparameterization?,"Bayesian parameter estimation,optimization problems"
Markov and Chebyshev inequalities,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/markov_and_chebyshev_inequalities,"Markov's inequality and Chebyshev's inequality are tools for bounding the probability of a random variable taking on extreme values. While the bounds are weak, they apply under very general conditions. One use of Chebyshev's inequality is to prove the weak law of large numbers.",Know the statements of the Markov and Chebyshev inequalities. Prove these inequalities,"random variables,expectation and variance"
Markov chain Monte Carlo,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/markov_chain_monte_carlo,Markov Chain Monte Carlo (MCMC) is a set of techniques for approximately sampling from a probability distribution p by running a Markov chain which has p as its stationary distribution. Gibbs sampling and Metropolis-Hastings are the most common examples.,"Know the basic problem setup: that you want to compute approximate samples from an intractable distribution. Be able to check that a Markov chain has a given stationary distribution, either using the detailed balance conditions or directly from the definition. Understand why you can average several Monte Carlo operators with the same stationarity distribution p and obtain one whose stationary distribution is also p","conditional distributions,Monte Carlo estimation,Markov chains"
Markov chains,"Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/markov_chains,"In a Markov chain, a system transitions stochastically from one state to another. It is a memoryless process, in the sense that the distribution over the next state depends only on the current state, and not on the state at any past time. Markov chains are useful models of many natural processes and the basis of powerful techniques in probabilistic inference and randomized algorithms.",Know the definition of a Markov chain. Know the definition of the stationary distribution. Know of a simple condition for showing convergence to a unique stationary distribution (e.g. ergodicity),"conditional distributions,matrix multiplication,conditional independence"
Markov decision process (MDP),"Probabilistic Graphical Models,Reinforcement Learning",https://metacademy.org/graphs/concepts/markov_decision_process,A Markov Decision Process (MDP) is a mathematical framework for handling search/planning problems where the outcome of actions are uncertain (non-deterministic). MDPs aim to maximize the expected utility (minimize the expected loss) throughout the search/planning.,"Understand the four components that comprise an MDP. Understand the following terminology: states, actions, rewards, transition functions/probabilities, policy, utility, discount parameter, q-states, value. Understand the recursive definition of value. Understand the justification for discount parameters (especially with infinite time MDPs)","conditional probability,expectation and variance"
Markov models,"Bayesian Statistics,Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/markov_models,"Markov models are a kind of probabilistic model often used in language modeling. The observations are assumed to follow a Markov chain, where each observation is independent of all past observations given the previous one.",,"Markov chains,matrix multiplication,conditional distributions,conditional independence"
Markov random fields,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/markov_random_fields,"Markov random fields (MRFs) are a kind of probabilistic model which encodes the model structure as an undirected graph. Two variables are connected by an edge if they directly influence each other. MRFs are useful for domains which can be described in terms of ""soft constraints"" between variables. MRFs can be equivalently characterized in terms of factorization of the joint distribution or conditional independence properties.",,"random variables,conditional probability,conditional independence"
matrix inverse,Linear Algebra,https://metacademy.org/graphs/concepts/matrix_inverse,"The inverse of a matrix A is a matrix which, when multiplied by A, gives the identity matrix. When it exists, it can be used to solve systems of linear equations.",Know the definition of the matrix inverse. Express the inverse in terms of solutions to systems of linear equations. Show that left and right inverses are equivalent. Why might the inverse not exist?. Know the formula for the inverse of a product. Show that the inverse is unique (assuming it exists),"matrix multiplication,linear systems as matrices"
matrix multiplication,Linear Algebra,https://metacademy.org/graphs/concepts/matrix_multiplication,"Matrix multiplication is an operator on matrices which satisfies many of the properties of multiplication, although not commutativity.","Be familiar with three equivalent interpretations of matrix multiplication: dot products, outer products, and matrix-vector multiplication. Know what constraint the sizes of the matrices must satisfy for the product to be defined. Show that matrix multiplication is associative and distributive but not commutative. Know what the identity matrix refers to",dot product
matrix transpose,Linear Algebra,https://metacademy.org/graphs/concepts/matrix_transpose,"The matrix transpose is an operator which flips a matrix over its diagonal, i.e. it switches the row and column indices of the matrix.",Know the definition of the matrix transpose. Know the formula for the transpose of a product. Know the formula for the transpose of an inverse. Know how to represent dot products and outer products in terms of transposes. Know the definition of a symmetric matrix,matrix multiplication
maximum likelihood,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/maximum_likelihood,"Maximum likelihood is a general and powerful technique for learning statistical models, i.e. fitting the parameters to data. The maximum likelihood parameters are the ones under which the observed data has the highest probability. It is widely used in practice, and techniques such as Bayesian parameter estimation are closely related to maximum likelihood.",,"random variables,independent random variables,optimization problems,Gaussian distribution"
maximum likelihood in exponential families,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/maximum_likelihood_in_exponential_families,"For any exponential family model, the maximum likelihood parameters are such that the model moments match the data moments.","Derive the general formula for the maximum likelihood parameter estimate for exponential families: namely, that the model moments match the empirical moments.","maximum likelihood,exponential families,optimization problems"
max-product on trees,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/max_product_on_trees,"Max-product is an algorithm for MAP estimation in graphical models, based on dynamic programming. It is a generalization of the Viterbi algorithm for hidden Markov models.",,"inference in MRFs,factor graphs"
MCMC convergence,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/mcmc_convergence,"Markov chain Monte Carlo (MCMC) samplers eventually converge to their stationary distribution, but they may take a long time to do so. The ""mixing time"" of a chain refers to how long a chain must be run in order for one sample to be independent of another. Diagnosing mixing time is important for judging the reliability of estimates obtained from an MCMC algorithm.","Know what is meant by mixing time. Why is burn-in needed when computing expectations from an MCMC chain?. Why are we allowed to use all of the samples when estimating expectations, even though they are not independent?. Be aware that the accuracy of the estimates depends on the autocorrelation, which may be difficult to estimate. Know some practical convergence diagnostics for MCMC algorithms","Markov chain Monte Carlo,covariance"
mean field approximation,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/mean_field,"In variational inference algorithms, we try to approximate an intractable distribution with a tractable one. Mean field is probably the most common example. The approximating distribution is factorized into independent terms corresponding to different variables or groups of variables. Variational Bayes and variational Bayes EM are important applications of mean field to Bayesian parameter estimation.",,"variational inference,independent random variables,Lagrange multipliers"
merge sort,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/merge_sort,"Merge sort is one of the most efficient sorting algorithms, and exemplifies the idea of divide-and-conquer algorithms. It works by splitting the array into two sub-arrays, recursively sorting those sub-arrays, and then merging them into a larger sorted array.",Be able to implement merge sort. Analyze the running time of merge sort. Analyze the memory usage of merge sort,"recursion (programming),sorting,asymptotic complexity"
method of moments,Frequentist Statistics,https://metacademy.org/graphs/concepts/method_of_moments,The method of moments is a simple method for estimating the parameters of a probability distribution from data. The parameters are chosen so that the model moments match the empirical moments.,Understand what the method of moments estimator is and how to compute it for simple parametric models.,expectation and variance
Metropolis-Hastings algorithm,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/metropolis_hastings,"Markov Chain Monte Carlo (MCMC) is a method for approximately sampling from a distribution p by defining a Markov chain which has p as a stationary distribution. Metropolis-Hastings is a very general recipe for finding such a Markov chain: choose a proposal distribution and correct for the bias by stochastically accepting or rejecting the proposal. While the mathematical formalism is very general, there is an art to choosing good proposal distributions.","Know the statement of the detailed balance conditions. Know the definition of the Metropolis-Hastings algorithm. Show that M-H satisfies the detailed balance conditions. Be aware of the possible failure modes if the proposal distribution isn't chosen carefully: slow mixing, and low acceptance probabilities","Markov chain Monte Carlo,multivariate Gaussian distribution"
minimax search,Symbolic AI,https://metacademy.org/graphs/concepts/minimax_search,"Minimax search is a method for solving two-person adversarial games with sequential moves. It is a form of dynamic programming which solves for the optimal moves at the leaves of the game tree and works its way up. Typically, it is infeasible to run minimax search exactly, so it is instead run to a finite depth, and the value of the state is estimated using an evaluation function.","Know what the nodes and edges of the game tree represent. Know what minimax search is. Understand why it computes the optimal moves at each node. Be able to implement minimax search. Know why the value of a board state often needs to be estimated with an evaluation function, and give some examples of evaluation functions",recursion (programming)
minimum spanning trees,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/minimum_spanning_trees,"A minimum spanning tree of a graph is a subgraph which is a tree, and which minimizes, out of all possible subtrees, the total weight of the edges. A minimum spanning tree can be computed efficiently using Kruskal's Algorithm or Prim's Algorithm.","Know what a minimum spanning tree of a graph refers to. Know (conceptually) how Kruskal's Algorithm and Prim's Algorithm work. Know what data structures need to be maintained in order to efficiently guarantee acyclicity. Analyze the running times of both algorithms, assuming either a binary heap or a Fibonacci heap. Prove the correctness of both algorithms","graph representations,heap (data structure)"
mixture of Gaussians models,Machine Learning,https://metacademy.org/graphs/concepts/mixture_of_gaussians,"Mixture of Gaussians is a probabilistic model commonly used for clustering: partitioning a set of data points into a set of clusters, where data points within a cluster are similar to one another.",Know what a mixture of Gaussians model is and what assumptions it makes about the data. Know how a mixture of Gaussians model can be used for clustering,multivariate Gaussian distribution
moment generating functions,Probability Theory,https://metacademy.org/graphs/concepts/moment_generating_functions,"The moment generating function (MGF) is a function which characterizes the distribution of a random variable. MGFs are useful for analyzing sums of independent random variables. In particular, they are used in the proof of the Central Limit Theorem and in deriving the Chernoff bounds, which bound the probability that a sum of independent random variables takes on extreme values.","Know the definition of the moment generating function. Be able to derive the MGFs of simple distributions (e.g. Gaussian, multinomial). Show that the moments of the distribution can be obtained from the derivatives of the MGF at zero. Show that the MGF of a sum of independent random variables is the product of the MGFs. Know that an MGF uniquely determines a distribution","expectation and variance,independent random variables"
Monte Carlo estimation,"Bayesian Statistics,Probability Theory",https://metacademy.org/graphs/concepts/monte_carlo_estimation,"One way to answer queries about a probability distribution is to simulate from the distribution, a procedure known as Monte Carlo estimation. In particular, we estimate the expected value of some function f with respect to a distribution p by generating samples from p and averaging the values of f over those samples.",Know what Monte Carlo estimation refers to. Know how Monte Carlo estimation can be used to estimate quantities such as expectation and variance. Know why the Monte Carlo estimate converges to the expectation in the large sample limit,expectation and variance
MRF parameter learning,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/mrf_parameter_learning,The parameters of a Markov random field (MRF) can be fit to data using maximum likelihood. The optimal parameters have an interesting interpretation: they are the parameters such that certain sufficient statistics of the model must match the corresponding statistics of the empirical distribution.,"Consider the maximum likelihood objective function for learning the parameters of an MRF given fully observed data.\nWhy doesn't the optimization problem decompose into separate optimization problems for each variable?\nWhy is it hard even to compute the objective function?\nDerive the gradient of the objective function.\nBy setting the gradient to zero, show that for the maximum likelihood parameters, the model statistics must match the data statistics.. Performing gradient descent requires performing inference in the MRF. Which quantities need to be computed?. Optional: show that the maximum likelihood optimization problem is convex (which implies there are no local optima). You may first want to read about covariance matrices and [convex optimization](convex_optimization) .","Markov random fields,maximum likelihood,inference in MRFs,optimization problems"
multi-dimensional arrays in C,Programming,https://metacademy.org/graphs/concepts/multi_dimensional_arrays_in_c,"Multi-dimensional arrays refer to arrays which take multiple indices. In C, these can be represented either by allocating a fixed-size array on the stack or in terms of an array of arrays. The latter is more flexible, but it leads to some tricky pointer arithmetic.","Be able to manipulate pointers to pointers: in particular, be able to determine the types of, and evaluate, expressions involving pointers to pointers. Understand how multi-dimensional arrays can be represented using pointers to pointers. Know how to allocate a multi-dimensional array (represented in terms of pointers to pointers). Understand what happens when you declare a multidimensional array, and why this is different from the pointers-to-pointers situation. Understand how an array of strings is represented","dynamic memory allocation in C,C pointers,call stack,C strings"
multidimensional scaling,Machine Learning,https://metacademy.org/graphs/concepts/multidimensional_scaling,Multidimensional scaling is a method for visualizing similarity between data points by embedding the data into a low-dimensional subspace. The locations are chosen so that the distances in the embedding space match the dissimilarities as closely as possible.,Know what stress function multidimensional scaling is minimizing. Understand how it can be used to visualize similarity data. Be aware that the exact solution can be computed efficiently,optimization problems
multinomial coefficients,Uncategorized,https://metacademy.org/graphs/concepts/multinomial_coefficients,"Multinomial coefficients are a generalization of binomial coefficients. They give the total number of ways that n objects can be partitioned into some number of categories, when the counts for each category are given.",Know the formula for the number of ways to partition n different things into groups of specified sizes. Understand why this formula works,
multinomial distribution,Probability Theory,https://metacademy.org/graphs/concepts/multinomial_distribution,The multinomial distribution is a generalization of the binomial distribution to the case where each of the events may take on more than two possible values.,Know the PMF of the distribution. Understand why the marginal over one of the counts is a binomial distribution. Be aware of how the distribution can model counts of independent events,"binomial distribution,multinomial coefficients"
multiple integrals,Multivariate Calculus,https://metacademy.org/graphs/concepts/multiple_integrals,"A multiple integral generalizes integration to functions of n variables and produces a general (n-1)-dimensional volume. For instance, n=2 corresponds to an area. Multiple integrals occur frequently in probability theory and machine learning when examining marginal densities.",Be able to compute multiple integrals by iteratively computing single integrals. Know that the order of the integrals doesn't matter. Be able to compute areas and volumes using multiple integrals,functions of several variables
multiplicity of eigenvalues,Linear Algebra,https://metacademy.org/graphs/concepts/multiplicity_of_eigenvalues,"The characteristic polynomial of a matrix may have repeated roots. If this is the case, the geometric multiplicity of a given eigenvalue (the dimension of the corresponding eigenspace) may be less than the algebraic multiplicity. This property determines whether a matrix is diagonalizable, and it is relevant to the solutions of differential equations.","Know the distinction between the algebraic and geometric multiplicity. Show that the sum of the algebraic multiplicities is the dimension of the space. Show that the geometric multiplicity of an eigenvalue can be less than the algebraic multiplicity, but not greater","roots of polynomials,eigenvalues and eigenvectors"
multivariate CDF,Probability Theory,https://metacademy.org/graphs/concepts/multivariate_cdf,Multivariate cumulative distribution functions (CDFs) are a way of characterizing multivariate distributions which generalize the univariate CDF.,,"random variables,multiple integrals,cumulative distribution function,higher-order partial derivatives"
multivariate distributions,Probability Theory,https://metacademy.org/graphs/concepts/multivariate_distributions,Multivariate distributions are a way of representing the dependencies between multiple random variables.,"Be able to represent a multivariate distribution in terms of the joint PMF or PDF. Know the definition of the joint CDF of a multivariate distribution. Be able to convert between the joint PDF and CDF. Be able to compute the probability that the random variables lie within a set. Know the definition of the marginal distribution over one or more variables, and be able to compute it from the joint distribution","random variables,multiple integrals"
multivariate Gaussian distribution,Probability Theory,https://metacademy.org/graphs/concepts/multivariate_gaussian_distribution,The multivariate Gaussian distribution is a generalization of the Gaussian distribution to higher dimensions. The parameters of an n-dimension multivariate Gaussian distribution are an n-dimensional mean vector and an n-by-n dimensional covariance matrix.,,"Gaussian distribution,covariance,covariance matrices,determinant,matrix inverse"
mutual information,Probability Theory,https://metacademy.org/graphs/concepts/mutual_information,"Mutual information is a measure of the amount of information one random variable conveys about another. It is one of the fundamental quantities of information theory, and determines the rate at which information can be conveyed over a noisy channel.",Know the definition of mutual information (in terms of the difference between joint entropy and conditional entropy). Derive some basic properties of mutual information:\nthat it is symmetric\nthat the mutual information of a random variable with itself is the entropy\nthat it is nonnegative\nthat it is zero for independent random variables. Know various ways joint entropy decomposes into sums of conditional entropies and mutual information,"entropy,independent random variables,conditional distributions"
naive Bayes,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/naive_bayes,"Naive Bayes is a modeling assumption used in classification, where we assume the observed data are conditionally independent given their class assignments. Despite its name, the standard naive Bayes model does not use Bayesian inference, but rather, a maximum likelihood estimation.",,"binary linear classifiers,maximum likelihood,optimization problems"
natural gradient,"Differential Geometry,Optimization",https://metacademy.org/graphs/concepts/natural_gradient,"The natural gradient is an operator analogous to the gradient, but which depends only on the intrinsic geometry of a manifold, not on the parameterization. Most commonly, it is used in maximum likelihood estimation, where the Riemannian metric is taken to be the Fisher metric.",Know the natural gradient update rule. Understand why it is invariant to reparameterization. Be able to derive the update rule for simple examples. What part of the natural gradient update rule makes it expensive to compute in models with many parameters?,"stochastic gradient descent,Riemannian metrics,statistical manifolds,maximum likelihood,Fisher metric"
natural numbers as sets,Set Theory,https://metacademy.org/graphs/concepts/natural_numbers_as_sets,"The natural numbers can be explicitly constructed as sets. Zero is defined to be the empty set, and each n > 0 is defined to be the set of natural numbers less than n. This is an example of how set theory serves as a powerful foundation for much of mathematics.","Construct the natural numbers in terms of sets.. Show that the constructed number system satisfies the Peano postulates.. Define equality and comparison operators, and show that these are an equivalence relation and order relations, respectively.. Define the addition and multiplication operators.. Show that these operators are commutative and associative.","set operations,equivalence relations,order relations,Peano axioms"
neural probabilistic language models,Uncategorized,https://metacademy.org/graphs/concepts/neural_probabilistic_language_models,"Neural probabilistic language models are a type of language model where the conditional probability distributions are approximated with a neural network. They are able to learn and exploit distributed representations of language, so that information can be shared between related words.",Know the basic architecture of a neural probabilistic language model.. Understand how it's able to share information between related words and why this is advantageous.,"softmax regression,n-gram language models,feed-forward neural nets"
Newton's method (optimization),Optimization,https://metacademy.org/graphs/concepts/newtons_method_optimization,"Newton's method is an optimization algorithm which, in the convex setting, iteratively minimizes quadratic approximations to the objective function. It can be impractical for high-dimensional problems because it requires inverting the Hessian matrix, but many highly effective optimization algorithms can be viewed as approximations to Newton's method.","Know the update rule for Newton's method. Understand its justification in terms of (a) minimizing quadratic approximations to the objective function, and (b) solving the linearized optimality conditions. Be aware of a pitfall of using Newton's method for nonconvex problems: it looks for critical points, not (local) minima. Understand why Newton's method is guaranteed to give a descent direction in the convex setting. Which part of the Newton update makes it impractical for high-dimensional problems?. Be aware that a step size of 1 suffices late in optimization, but a line search is often required early in optimization. Be aware that there is often a linear convergence regime followed by a quadratic convergence regime. Show that Newton's method is invariant to affine transformations of the parameters","higher-order partial derivatives,line search,convergence of gradient descent,matrix inverse,optimization problems,second derivative test,gradient descent,convex functions,computing matrix inverses,positive definite matrices,linear transformations as matrices,change of basis"
n-gram language models,Uncategorized,https://metacademy.org/graphs/concepts/n_gram_language_models,"Language models attempt to model the distribution of natural language sentences, for instance to serve as the prior distribution for a speech recognition system. N-gram models are perhaps the simplest type of language model. They exploit the chain rule for conditional probability, decomposing the probability of a sequence into a product of conditional probabilities. Each of these conditional probabilities is represented using lookup tables over word sequences.","Know how to decompose the probability of a sentence as a product of conditional probabilities using the Chain Rule for Conditional Probability.. Know what the Markov assumption is and why it's significant.. Know the basic terminology (unigram, bigram, etc.). Be able to estimate the parameters of the conditional probability tables using maximum likelihood.. Understand the tradeoffs involving small and large values of n.. Know the definition of perplexity.. Be aware of the problems of data sparsity and unknown words.",conditional probability
nondeterministic finite automata,Theory of Computation,https://metacademy.org/graphs/concepts/nondeterministic_finite_automata,"Nondeterministic finite automata are a model of computation that generalizes (deterministic) finite automata. Each state is allowed multiple transitions for a given input character, and the automaton accepts the input if there is any legal sequence of transitions which ends in an accepting state. While NFAs appear more powerful than DFAs, the two are provably equivalent in terms of the languages they can represent.","Formally define a nondeterministic finite automaton. Using the subset construction, show that deterministic and nondeterministic finite automata are equivalent",finite automata
nondeterministic Turing machines,Theory of Computation,https://metacademy.org/graphs/concepts/nondeterministic_turing_machines,"Nondeterministic Turing machines are a variant of Turing machines which can have multiple possible actions in a given state. They accept an input if there is any possible execution path which accepts it. They are no more powerful than ordinary Turing machines in terms of what they can compute, but are believed to be much more powerful in terms of what they can compute efficiently.",Define a nondeterministic Turing machine. Show that a single-track Turing machine can simulate a multi-track Turing machine. Show that a multi-track Turing machine (and therefore also a single-track one) can simulate a nondeterministic Turing machine,Turing machines
nonlinear conjugate gradient,Optimization,https://metacademy.org/graphs/concepts/nonlinear_conjugate_gradient,"While conjugate gradient is a method for solving linear systems (and important properties of the algorithm depend on this), it can also be applied heuristically to optimization problems more generally. The Fletcher-Reeves and Polak-Ribiere algorithms are two particular methods for doing so.","Understand why the gradient is a generalization of the residual to the nonlinear setting. Why don't the linear CG updates maintain conjugacy in the nonlinear setting, with the gradient in place of the residual?. Why doesn't the step size formula from linear CG choose the optimal step size? Know of at least one heuristic for choosing a step size.. Know the update rules for the Fletcher-Reeves and Polak-Ribiere methods. Why are the Fletcher-Reeves and Polak-Ribiere methods equivalent for quadratic objectives?","gradient,conjugate gradient"
NP complexity class,Theory of Computation,https://metacademy.org/graphs/concepts/np_complexity_class,"NP, or ""nondeterministic polynomial,"" is the complexity class of problems with ""polynomial time verifiers."" I.e., if the problem has a solution, it must be possible to verify the solution in polynomial time, even if finding the solution is much harder. Equivalently, it is the class of problems which can be solved efficiently by nondeterministic Turing machines. The ""P vs. NP"" question, whether all NP problems can be solved in worst-case polynomial time, is the central question in computational complexity theory.","Define the complexity classes P and NP. Be able to show that problems are in NP. Be aware of the ""P vs. NP"" problem and why it's important","asymptotic complexity,nondeterministic Turing machines"
optimization problems,"Multivariate Calculus,Optimization",https://metacademy.org/graphs/concepts/optimization_problems,"In an optimization problem, one is interested in minimizing or maximizing a function, possibly subject to equality or inequality constraints. The extrema must occur on the boundary of the set, at points which are not differentiable, or at points where the partial derivatives are zero.","Know what is meant by local and global extrema. Know that the extreme must satisfy at least one of the following conditions: on the boundary, not differentiable, or all partial derivatives are zero. Give an example where a critical point is a saddle point rather than an extremum",partial derivatives
order relations,Set Theory,https://metacademy.org/graphs/concepts/order_relations,"A relation is an order relation if it is irreflexive, asymmetric, and transitive. An order relation < is a total order if for any A and B, either A < B, B < A, or A = B; otherwise it is a partial order.",Define order relation. Know the distinction between total orders and partial orders. Be able to show that something is total or partial order. Know the distinction between maximal element and largest element,equivalence relations
ordinal numbers,Set Theory,https://metacademy.org/graphs/concepts/ordinal_numbers,"The ordinal numbers are a way of measuring the size of well-ordered sets. They give a way of talking about different sizes of infinity. They are needed to construct the cardinal numbers, which measure the sizes of sets more generally.","Define what it means for a set to be an ordinal. Know that the ordinals can be divided into limit and successor ordinals. Show that the subset relation is a well ordering on the ordinals. Define what it means for two well-ordered sets to be isomorphic. Show that every well-ordered set is isomorphic to a unique ordinal. Show that for every ordinal, there is a larger ordinal","natural numbers as sets,well orderings,set operations"
oriented manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/oriented_manifolds,"For operations such as integration, one often needs to assign a manifold an orientation, or ""handedness"" of the coordinate charts. For some manifolds, this can be done; other manifolds, such as the Mobius strip, are not orientable.",Define an orientation in terms of an equivalence class of coordinate charts. Define an orientation in terms of having a nonzero volume form. Show that these definitions are equivalent. Give an example of a non-orientable manifold,"differentiable manifolds,tangent bundle,bases,differential forms"
orthogonal subspaces,Linear Algebra,https://metacademy.org/graphs/concepts/orthogonal_subspaces,"Two subspaces X and Y are orthogonal if any vector in X is orthogonal to any vector in Y. Canonical examples include the row space and nullspace, and the column space and left nullspace. Orthogonality is used to analyze projections and least squares approximation.",Know what it means for two subspaces to be orthogonal. Know the definition of the orthogonal complement to a subspace. Know that the row space is the orthogonal complement of the nullspace and the column space is the orthogonal complement of the left nullspace. Prove this. Know that the orthogonal complement of the orthogonal complement is the original subspace. Prove this,"dot product,subspaces,four fundamental subspaces"
orthonormal bases,Linear Algebra,https://metacademy.org/graphs/concepts/orthonormal_bases,An orthonormal basis is a basis such that each vector has unit length and each pair of vectors is orthogonal. An orthonormal basis for the full space can be represented as an orthogonal matrix. Such matrices have nice algebraic properties and are useful for representing projections and least squares solutions.,Know the definitions of orthogonal matrix and orthonormal basis. Show that the inverse of an orthogonal matrix is its transpose. Show that the transpose (or inverse) of an orthogonal matrix is also orthogonal. Be able to use orthonormal bases to compute projections. Show that orthogonal matrices preserve dot products (and therefore angles and Euclidean norms). Be able to compute orthogonal bases using Gram-Schmidt,"matrix transpose,matrix multiplication,dot product,projection onto a subspace,bases,matrix inverse"
PAC learning,Theory of Computation,https://metacademy.org/graphs/concepts/pac_learning,Probably approximately correct (PAC) learning is a theoretical framework for analyzing the generalization error of a learning algorithm in terms of its error on a training set and some measure of complexity. The goal is typically to show that an algorithm achieves low generalization error with high probability.,"Know what it means for a hypothesis class to be PAC-learnable.. Derive the bound on the probability of having generalization error greater than epsilon, in terms of the number of training examples and the size of the hypothesis class.\nTry rearranging the terms of the formula to view each variable as a function of the others. E.g., how many training examples do you need in order to guarantee a certain generalization accuracy?. Carry out the analysis for a particular hypothesis class, such as boolean functions.. Try to get a sense for how large hypothesis spaces are. E.g., what is the size of the hypothesis space for:\nclassifying based on a single Boolean variable (from a set of D)\nall possible functions of D Boolean variables","generalization,unions of events,independent events,Chernoff bounds"
parameterizing lines and planes,"Linear Algebra,Multivariate Calculus",https://metacademy.org/graphs/concepts/parameterizing_lines_and_planes,"Lines and planes can be defined either as spans of vectors or as solutions to systems of linear equations. Vector operations (in particular, dot products and cross products) allow us to convert between these representations. These tools also let us compute distances between points, lines, and planes.","Know how lines and planes can be represented as spans of vectors or as solutions to linear systems. Be able to convert between the two representations. Be able to compute distances between points, lines, and planes","dot product,linear systems as matrices,cross product"
partial derivatives,Multivariate Calculus,https://metacademy.org/graphs/concepts/partial_derivatives,"""A partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary)"" [wikipedia entry]. Intuitively, a partial derivative measures the instantaneous rate of change for a single variate in a multivariate function.","Know the definition of partial derivatives. Be able to compute partial derivatives. Know how they can be interpreted as rates of change, or as tangent lines",functions of several variables
particle filter,"Bayesian Statistics,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/particle_filter,"The particle filter is a Monte Carlo algorithm for posterior inference in temporal models. The posterior is approximated with a weighted set of discrete particles. In each step, each particle's state is extended according to a proposal distribution, and its weight is updated based on the likelihood of the evidence. The algorithm is useful in robotics and in visual tracking because it doesn't require storing the entire history.","Know the steps of the particle filter algorithm.. Be aware that an important advantage of the algorithm is that it does not require storing the entire history.. Be aware that the following are major sources of variance in the estimator, and that effective performance can depend on the details of the implementation:\nthe proposal distribution\nthe resampling method","hidden Markov models,Monte Carlo estimation,importance sampling,conditional distributions"
PDFs of functions of random variables,Probability Theory,https://metacademy.org/graphs/concepts/pdfs_of_functions_of_random_variables,The PDF of a continuous function of continuous random variables can be computed in terms of the joint PDF and the Jacobian of the function.,Be able to compute the PDF of a function of a random variable. Derive the formula in the univariate case (in terms of the CDF),"multivariate distributions,random variables,linear approximation,determinant"
Peano axioms,Logic,https://metacademy.org/graphs/concepts/peano_axioms,"The Peano axioms are a set of axioms, either in first- or second-order logic, for the system of natural numbers. The axioms define a successor operation and the principle of induction, and in some versions, addition, multiplication, and ordering as well. The Peano axioms serve as a formal model for number theory.",,first-order logic
perceptron algorithm,Machine Learning,https://metacademy.org/graphs/concepts/perceptron,The perceptron is a simple algorithm for binary classification where the weights are adjusted in the direction of each misclassified example.,Know the perceptron update rule. Optional: show that the algorithm terminates if the data are separated by some margin. Why can't the algorithm terminate if the data are not linearly separable?,binary linear classifiers
Poisson distribution,Probability Theory,https://metacademy.org/graphs/concepts/poisson_distribution,"The Poisson distribution is a discrete probability distribution for the counts of independent random events in a given time interval, e.g. babies born in a hospital in 1 month or lightening strikes in Mexico in 1 week. It is one of the most common discrete distributions used in virtually every scientific and financial field.",Know the PMF of the Poisson distribution. Know some examples of phenomena it can be used to model. Derive the distribution as the limit of the binomial distribution,"binomial distribution,random variables"
policy iteration,Reinforcement Learning,https://metacademy.org/graphs/concepts/policy_iteration,Policy iteration is a two step iterative algorithm for computing an optimal policy for a Markov decision process. Policy iteration alternates between (i) computing the value function for a fixed policy (which could be initialized randomly) and (ii) improving the policy by selecting the actions that maximize the values computed in the previous step. Policy iteration generally converges to an optimal policy much quicker than value iteration.,,"value iteration,Bellman equations"
positive definite matrices,Linear Algebra,https://metacademy.org/graphs/concepts/positive_definite_matrices,"A symmetric matrix A is positive definite if x^T A x > 0 for any nonzero vector x, or positive semidefinite if the inequality is not necessarily strict. They can be equivalently characterized in terms of all of the eigenvalues being positive, or all of the pivots in Gaussian elimination being positive. Examples of PSD matrices include covariance matrices and Hessian matrices of convex functions. The singular value decomposition (SVD) is closely related to the eigendecomposition of a positive semidefinite matrix.","Know the definition of a positive (semi-)definite matrix (in terms of the quadratic form). Show that a symmetric matrix is PD if and only if the diagonal entries of the spectral decomposition are all positive. Equivalently, show that a symmetric matrix is PD if and only if the eigenvalues are all positive","spectral decomposition,matrix transpose,matrix multiplication,eigenvalues and eigenvectors,matrix inverse"
precision and recall,Machine Learning,https://metacademy.org/graphs/concepts/precision_recall,"In pattern recognition and information retrieval, precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also called sensitivity) is the fraction of relevant instances that are retrieved (Wikipedia). For instance, if there were 50 relevant documents in a corpus where 20 of the 50 documents were relevant to a user, and an information retrieval (IR) system returned 20 documents, where 6 of the documents were relevant, the recall would be 6/50 = 0.12, and the precision would be 6/20 = 0.3.",Understand the definition of precision and recall and why optimizing an information retrieval system solely for precision or recall is a bad idea.. What is the difference between precision and recall?. Should you use precision or recall for ranked retrieval sets?,
preconditioned conjugate gradient,Optimization,https://metacademy.org/graphs/concepts/preconditioned_conjugate_gradient,Conjugate gradient can converge slowly if the matrix A defining the linear system is ill-conditioned. Preconditioning is a technique for transforming the linear system into a coordinate system where A is (hopefully) better conditioned.,"Understand conceptually what transformation preconditioning applies to the variables in CG. Know how preconditioned CG can be implemented without explicit construction of the preconditioned version of A. Be aware of the motivation in terms of reducing the condition number of A (and hence understand why the preconditioning matrix M should approximate A). Be aware that the diagonal entries of A are a common choice of preconditioner. Know what the incomplete Cholesky factorization is, and why it might be a better choice of preconditioner than the diagonal entries","conjugate gradient,Cholesky decomposition,convergence of conjugate gradient"
principal component analysis,Machine Learning,https://metacademy.org/graphs/concepts/principal_component_analysis,"Principal component analysis is a method for projecting data into a lower dimensional space. It works by finding the space which maximizes the variance of the projections, or equivalently, minimizes the reconstruction error. Mathematically, it corresponds to computing the SVD of the transformed data, or the spectral decomposition of the covariance matrix.","Know two formulations of PCA: subspace of maximum variance, and subspace of minimum reconstruction error. Know how the optimal solution can be computed, either using the SVD or the spectral decomposition of the covariance. Know how PCA can be used for compression and visualization","spectral decomposition,covariance matrices,singular value decomposition"
principal component analysis (proof),Machine Learning,https://metacademy.org/graphs/concepts/pca_proof,The proof that principal component analysis (PCA) finds the subspace maximizing the variance and minimizing the reconstruction error.,Prove that the PCA subspace maximizes the variance and minimizes the reconstruction error,"variational characterization of eigenvalues,principal component analysis"
probabilistic Latent Semantic Analysis,Machine Learning,https://metacademy.org/graphs/concepts/probabilistic_lsa,"Probabilistic Latent Semantic Analysis (pLSA), also known as probabilistic Latent Semantic Indexing (pLSI), is a matrix decomposition technique for binary and count data, where one component of the data is conditionally independent of the other component given some unobserved factor. pLSA is most commonly used for document modeling, where the count data is the number of times a term appears in each document (forming an observed term by document count matrix), and the factors are interpreted as the latent/unobserved topics.",Understand the difference between pLSA and LSA\nWhy is pLSA considered a statistical model while LSA is not?\nWhat objective function does pLSA maximize in order to determine the decomposition?. How would a trained pLSA model handle new documents? (see Blei et al.'s LDA paper),"latent semantic analysis,maximum likelihood,optimization problems"
probabilistic PCA,Machine Learning,https://metacademy.org/graphs/concepts/probabilistic_pca,"Probabilistic principal component analysis (PCA) is a formulation of PCA as a latent variable model. Each data point is assumed to be generated as a linear function of Gaussian latent variables, plus noise. Like PCA, it has a closed form solution in terms of the truncated SVD of the covariance matrix.",Know how PCA can be formulated as a probabilistic model. Derive the formula for posterior inference of the latent variables. Derive the maximum likelihood solution from the eigendecomposition. Know how the optimal noise parameter can be determined,"computations on multivariate Gaussians,maximum likelihood,principal component analysis,optimization problems,principal component analysis (proof)"
probability,Probability Theory,https://metacademy.org/graphs/concepts/probability,"Probability theory is a set of mathematical techniques for reasoning about uncertainty. Intuitively, probabilities can be thought of as describing long-run frequencies or subjective beliefs. Mathematically, a probability measure is a function on subsets of a sample space which satisfy certain axioms.","Know what is meant by the sample space and events. Know the axioms of probability. Be able to work with finite probability spaces. Be aware of some common interpretations of probability: subjective beliefs, and long-run averages",
probit regression,Machine Learning,https://metacademy.org/graphs/concepts/probit_regression,"Probit regression is a discriminative model for classification. In this model, the binary targets are generated by sampling latent Gaussian variables whose means are linear in the inputs, and passing them through a threshold.","Know what the probit regression model is. Be able to derive the gradient descent update rules. Understand the relationship with logistic regression:\nthe two have similar activation functions\nhowever, probit regression is more sensitive to outliers. Interpret the model in terms of a latent Gaussian variable and a threshold","binary linear classifiers,logistic regression,probit function,gradient descent"
projection onto a subspace,Linear Algebra,https://metacademy.org/graphs/concepts/projection_onto_a_subspace,"The projection of a vector b onto a subspace X is the closest point to b contained in X. Projection is a linear operation, and can be computed using a projection matrix. It is used in linear least squares approximation.",Know the definition of projection onto a subspace. Show that projection is a linear transformation. Give an explicit formula for the projection matrix. Show that the error in the projection is orthogonal to the subspace. Why is the projection the closest point within the subspace?,"matrix transpose,subspaces,bases,matrix inverse,orthogonal subspaces"
proofs in first-order logic,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/proofs_in_first_order_logic,"One can prove statements in first-order logic using a formal deductive calculus. Such a formal proof system is a central tool in the foundations of mathematics, since it serves as a formal model of mathematical reasoning more generally.",Be able to prove statements in a formal first-order proof system.,"first-order logic,propositional proofs"
propositional logic,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/propositional_logic,"Propositional logic is a logical formalism where the variables correspond to atomic sentences which are either true or false. Formulas are constructed using the connectives AND, OR, NOT, and IMPLIES. The semantics can be defined in terms of ""truth tables,"" or equivalently boolean functions.","understand the meanings of the propositional connectives AND, OR, NOT, and IMPLIES. be able to evaluate the truth value of a propositional formula given the truth values of the variables. be aware of ways in which IMPLIES does not correspond to our intuitive notions of implication. be able to manipulate propositional formulas (e.g. with distributive laws, de Morgan’s laws). be able to define mathematically: a truth assignment, whether a formula is satisfied. define what it means for one set of formulas to tautologically imply another",
propositional proofs,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/propositional_proofs,"A formal proof is an argument in a formal system where each step is justified by one of a precisely defined set of inference rules. Formal proofs are used as a model for studying mathematics itself, and are used in formal verification. One hopes a proof system is sound, in that each inference rule yields only true statements when its premises are true. A system is complete if all true statements expressible in some logical language can be proved within the system. Various proof systems have been defined for propositional logic which are both sound and complete.",understand the difference between formal and informal proofs. be able to prove tautologies in a formal propositional proof system. know what it means for a proof system to be sound and complete. prove the soundness of a propositional proof system. prove the completeness of a propositional proof system,propositional logic
propositional resolution,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/propositional_resolution,"Resolution is an inference rule for logical systems -- in this case, for propositional logic. It is complete, in that if a set of propositional sentences is unsatisfiable, one can prove that using resolution. It is the basis for many powerful automated theorem provers.",Be able to convert SAT instances to conjunctive normal form.. Know the resolution inference rule.. Show that resolution is sound.. Show that resolution is complete.,"propositional logic,propositional satisfiability,propositional proofs"
propositional satisfiability,"Logic,Symbolic AI,Theory of Computation",https://metacademy.org/graphs/concepts/propositional_satisfiability,"Propositional satisfiability (SAT) is a computational problem where one is given a setence in propositional logic, and one wants to determine if there is a satisfying assignment (an assignment of truth values to all the variables which makes the assignment true). SAT is central to computer science because it is the prototypical NP-complete problem and because SAT solvers underly a lot of automated reasoning systems.",define the propositional satisfiability (SAT) problem. give a brute force algorithm in terms of truth tables. show that it suffices to consider inputs in conjunctive normal form. show how one can use satisfiability to prove a conclusion from a set of premises,propositional logic
pullback,"Differential Geometry,Multivariate Calculus",https://metacademy.org/graphs/concepts/pullback,Pullback is a mathematical operator which represents functions or differential forms on one space in terms of the corresponding object on another space. They are used to define surface integrals of differential forms.,"Define the pullback of a function and of a differential form. Show that the pullback commutes with the exterior derivative. Be able to manipulate pullback, wedge products, and the exterior derivative algebraically. Define the surface integral of a differential form in terms of pullback","differential forms,exterior derivative,Chain Rule,evaluating multiple integrals: change of variables"
pushdown automata,Theory of Computation,https://metacademy.org/graphs/concepts/pushdown_automata,"Pushdown automata are a simple model of computation where the memory consists of an (unbounded) stack of symbols. Nondeterministic pushdown automata are equivalent in expressive power to context-free grammars. Deterministic pushdown automata can recognize only a smaller set of languages, but most modern programming languages are recognizable by a deterministic pushdown automaton.","Define the pushdown automaton, both the deterministic and nondeterministic versions.. Be able to program pushdown automata to recognize simple languages.. Give an example of a language recognizable by a nondeterministic, but not by a deterministic, pushdown automaton.. Be aware that nondeterministic pushdown automata are equivalently expressive to context-free grammars.. Prove that nondeterministic pushdown automata are equivalently expressive to context-free grammars.","context-free grammars,finite automata"
QR decomposition,Linear Algebra,https://metacademy.org/graphs/concepts/qr_decomposition,The operations of the Gram-Schmidt procedure can be represented as a factorization of a matrix into an orthogonal matrix Q and an upper triangular matrix R. This is a useful representation for solving linear least squares problems.,Show that the Gram-Schmidt operations can be represented with the QR decomposition,orthonormal bases
queue,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/queue,"A queue is an abstract data type which supports two operations: enqueue (put at item at the back of the queue) and dequeue (remove the element from the front). It is a first-in-first-out data structure, since elements are removed in the order they are added.",Know what a queue is and what operations it supports. Know how a queue can be implemented as a linked list. Know how a queue can be implemented using an array (i.e. as a circular buffer),linked lists
quicksort,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/quicksort,"Quicksort is one of the most efficient general-purpose sorting algorithms. It works by choosing a ""pivot"" element from the array, dividing the array into those elements less than or larger than the pivot, and recursively sorting the subarrays. It exemplifies the divide-and-conquer approach to algorithm design and the benefits of randomness.",Be able to implement quicksort. Analyze the worst-case running time of quicksort. Analyze the average case running time of quicksort. Know a technique for fixing the worst-case behavior (such as randomly choosing the pivot),"recursion (programming),sorting,asymptotic complexity,analyzing recursive algorithms,expectation and variance"
random forests,Machine Learning,https://metacademy.org/graphs/concepts/random_forests,Random forests are a machine learning algorithm which averages the predictions over decision trees restricted to random subsets of the input features. They are widely used because they often perform very well with almost no parameter tuning.,Know the basic random forest algorithm. What effect does varying the number of features have? What are the advantages of larger or smaller values?. How do you determine the relevance of each of the input features to the classification?. How do you estimate out-of-sample error as the training is progressing?,"decision trees,bagging,generalization,expectation and variance"
random variables,Probability Theory,https://metacademy.org/graphs/concepts/random_variables,"Random variables are the central object of probability theory. As their name implies, can be thought of as variables whose values are randomly determined. Mathematically, they are represented as functions on a sample space.",Know the definition of a random variable (as a function on the sample space). Know the definition of the distribution of a random variable. Know the distinction between discrete and continuous random variables. Know how distributions can be represented in terms of probability mass functions and probability density functions,probability
reasoning with Horn clauses,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/reasoning_with_horn_clauses,"Horn clauses are a restricted kind of formula in propositional or first-order logic which are computationally tractable, yet highly expressive. They form the core of programs in Prolog, a popular logic programming language. One can reason efficiently with Horn clauses using a combination of forward and backward chaining.","Define what a Horn clause is. Know the forward chaining algorithm, and be able to simulate it on simple examples. Know the backward chaining algorithm, and be able to simulate it on simple examples. Why can naive backward chaining get stuck in infinite loops, and how can that be dealt with?","propositional satisfiability,first-order logic,first-order unification"
recurrent neural networks,Machine Learning,https://metacademy.org/graphs/concepts/recurrent_neural_networks,"Recurrent neural networks (RNNs) are a kind of neural net often used to model sequence data. They maintain a hidden state which can ""remember"" certain aspects of the sequence it has seen. RNNs can be trained using backpropagation through time, although efficient training remains an open problem.","Know what is meant by recurrent connections, and in particular how an RNN can be unrolled into a feed-forward computation graph.. Know how to train an RNN using backpropagation through time.. Understand the advantage of recurrent memory compared with autoregressive neural nets with limited contexts.. Understand how RNNs can be applied to sequence-to-sequence modeling tasks, including the role of the encoder and decoder.","backpropagation,feed-forward neural nets,neural probabilistic language models"
recursion (programming),Programming,https://metacademy.org/graphs/concepts/recursion_programming,"In programming, a recursive function is a function which calls itself. Many important algorithms and operations on data types are most easily expressed recursively. Recursive backtracking is an elegant and often effective search strategy.","Know some terminology: recursion, recursive call, base case. Be able to write recursive functions to solve simple problems. Know what arm's length recursion refers to and why it should be avoided. Know of examples where the naive recursive approach is inefficient (e.g. calculating Fibonacci numbers) and how to get around this",
recursive backtracking,"Data Structures & Algorithms,Programming,Symbolic AI",https://metacademy.org/graphs/concepts/recursive_backtracking,"Suppose you wish to find an object that satisfies certain constraints, such as a solution to a Sudoku puzzle. Recursive backtracking is an elegant way to implement a brute-force search. It uses recursive calls to build up a candidate solution, backtracking whenever it violates a constraint so that it can move on to the next possibility.",Understand what recursive backtracking does at a conceptual level. Be able to implement recursive backtracking to solve search problems,"stack,recursion (programming)"
recursive functions,Logic,https://metacademy.org/graphs/concepts/recursive_functions,"Recursive functions are a kind of mathematical function which, if the Church-Turing hypothesis is correct, correspond to functions which can be computed algorithmically. A subclass knows as primitive recursive functions consists of functions computable by programs where a bound on the number of steps can be calculated in advance. Recursive functions are useful for formalizing notions of computability, because the notion of a recursive function is more precise than that of an algorithm.","Define the set of primitive recursive functions. Be able to show that simple functions are primitive recursive. Know what the minimization operator is and what additional power it adds. Know why definitions in terms of minimization can give only partial, not total, functions",
red-black trees,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/red_black_trees,Red-black trees are a kind of binary search tree which stays approximately balanced as insertions and deletions are performed. It is based on maintaining a set of tree invariants which together guarantee that the tree is approximately balanced.,Know what invariants are maintained by a red-black tree. Understand why these invariants imply that the tree is approximately balanced. Know what the rotation operation is and why it preserves the ordering property. Be aware of how insertion and deletion can be performed while maintaining the invariants. (Don't worry about memorizing the details -- it's pretty involved.),"binary search trees,asymptotic complexity"
register machines,Theory of Computation,https://metacademy.org/graphs/concepts/register_machines,"Register machines are a model of computation involving a set of registers which can hold arbitrarily large positive integers. Despite the model's simplicity, it is Turing complete. The simplicity of the definition is helpful in computability theory, because reductions from counter machines can be easier than reductions from Turing machines.",Define a register machine (there are many variants which are Turing complete). Be able to write programs to compute simple functions. Show that register machines are Turing-complete,Turing machines
regular expressions,"Programming,Theory of Computation",https://metacademy.org/graphs/concepts/regular_expressions,"A regular expression is a sequence of characters describing a pattern against which strings can be matched. They are commonly used to search text files for patterns, e.g. as in tools like grep.",Be familiar with regular expression syntax. Be able to write regular expressions to solve simple problems,
regular languages,Theory of Computation,https://metacademy.org/graphs/concepts/regular_languages,"A regular language is a formal language which can be defined using a regular expression. Equivalently, it is a language definable as the set of strings accepted by some finite automaton, either deterministic or nondeterministic.",Show that regular expressions and finite automata define the same sets of languages. Be able to show that languages are not regular (e.g. using the Pumping Lemma). Prove the Pumping Lemma,"nondeterministic finite automata,regular expressions"
rejection sampling,"Probabilistic Graphical Models,Probability Theory",https://metacademy.org/graphs/concepts/rejection_sampling,"Rejection sampling (RS) is a monte carlo method for sampling from a potentially complex distribution p(x) given a simpler distribution q(x). RS is applicable when we can evaluate p(x) easily and sample from q(x) easily. The basic idea is to scale q(x) by some constant K such that K*q(x) >= p*(x) for all x, where p(x) = 1/Z p*(x) for some constant Z (that is, we only need to know p(x) up to a normalization factor, which is common in practice). RS operates by sampling a value x0 from q(x) and then generating a uniform random number, u, between [0, K*q(x0)]. If u <= p*(x0) we keep x0 as a valid sample from p(x), else we discard x0.",Know the definition of rejection sampling. Understand why rejection sampling gives correct samples. Why is rejection sampling impractical for high-dimensional distributions?,"conditional distributions,Monte Carlo estimation"
representability in arithmetic,Logic,https://metacademy.org/graphs/concepts/representability_in_arithmetic,"A relation is definable in a first-order theory T if there is a formula f expressible in the language of T, such that f(x) is provable in T whenever x is in R; otherwise the negation must be provable. It can be shown that the functions representable in arithmetic are exactly those functions which are recursive, or equivalently, computable. The notion of representability is used to construct self-referential sentences in the proof of Godel's Incompleteness Theorem.",Define a system of axioms for basic arithmetic in first-order logic. Define what it means for functions and relations to be representable in a logical theory. Be able to show that simple arithmetic functions are representable in arithmetic (using a suitable set of axioms). Show that the set of functions representable in arithmetic is closed under composition. Show that there is a representable encoding of tuples of numbers into single numbers,"first-order logic,proofs in first-order logic,functions and relations as sets"
representation invariants,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/representation_invariants,"Representation invariants are a mathematical tool for checking the correctness of implementations of abstract data types (ADTs). A set of invariants are defined which together guarantee the internal consistency of the data structure. Each method is required to preserve these invariants. To show correctness, one also needs an abstraction function, which maps the underlying representation back to the ADT's abstract types.",Know what representation invariants and abstraction functions refer to. Be able to use representation invariants to (informally) demonstrate the correctness of the implementation of an abstract data type,"specifications (programming),abstract data types"
restricted Boltzmann machines,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/restricted_boltzmann_machines,"Restricted Boltzmann machines (RBMs) are a type of undirected graphical model typically used for learning binary feature representations. The structure consists of a bipartite graph with a layer of visible units to represent the inputs and a layer of hidden units to represent more abstract features. Training is intractable, but approximations such as contrastive divergence work well in practice. RBMs are a building block of many models in deep learning.","Know what an RBM is and what distributions it can represent.. Understand why training an RBM is intractable. In particular,\nwhy is it intractable to compute the gradient?\nwhy does the likelihood function have local optima?. Know about the contrastive divergence training criterion and understand what approximation is being made.. Why does the structure of the model simplify the Gibbs sampling update?. Be able to implement an RBM training algorithm such as contrastive divergence.","Markov random fields,MRF parameter learning,stochastic gradient descent,Gibbs sampling"
reversible generative models,Uncategorized,https://metacademy.org/graphs/concepts/reversible_generative_models,"Reversible models are a kind of generative model based on a generator network which implements an easily invertible function. Because the density defined by such a network can be efficiently computed, reversible models can be trained using maximum likelihood.","Understand how a generative model can be defined in terms of a simple noise distribution and a nonlinear mapping.. Be able to determine the density using the change-of-variables formula, and know what assumptions are required to be able to do this.. Be able to define a reversible network architecture.. Know what volume preservation means and how this can be achieved.. Be able to train a reversible generative model.","PDFs of functions of random variables,maximum likelihood,feed-forward neural nets,linear approximation,determinant"
reversible jump MCMC,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/reversible_jump_mcmc,Reversible jump MCMC is a special case of Metropolis-Hastings where proposals are made between continuous spaces of differing dimensionality. The most common use is in Bayesian model averaging.,Understand why generic MCMC operators aren't applicable when sampling over spaces of differing dimensionality.. Know the basic idea behind reversible jump: augmenting the parameter spaces so that they are equal dimension.. Derive the acceptance probability for the proposal. The tricky part is dealing with the fact that part of the proposal distribution is deterministic.,"Bayesian model averaging,Metropolis-Hastings algorithm,PDFs of functions of random variables"
ridge regression,Machine Learning,https://metacademy.org/graphs/concepts/ridge_regression,"A problem with vanilla linear regression is that it can overfit, by forcing the learned parameters to match all the idiosyncrasies of the training data. Ridge regression, or regularized linear regression, is a way of extending the cost function with a regularizer which penalizes large weights. This leads to simpler solutions and often improves generalization performance. This idea of regularization can be used to improve the generalization performance of many other statistical models as well.",,"linear regression,generalization"
ridge regression as SVD,Machine Learning,https://metacademy.org/graphs/concepts/ridge_regression_as_svd,"It's possible to write the ridge regression solution in terms of the SVD of the dataset. This gives insight into how it makes predictions. It also gives a way of defining the ""degrees of freedom"" or ""effective number of parameters"" of the model, which lets us analyze the degree of overfitting.",,"ridge regression,singular value decomposition"
Riemannian metrics,Differential Geometry,https://metacademy.org/graphs/concepts/riemannian_metrics,"A Riemannian metric assigns an inner product to the tangent space at each point of a differentiable manifold. It gives a local notion of distance, and allows one to define notions such as orthogonal vectors, the norm of a vector, the length of a path, and the distance between two points.","Define Riemannian metric. Given a Riemannian metric, define the norm of a vector, orthogonality, and the length of a path.. Show that the length of a curve is independent of the parameterization. Given a Riemannian metric, define the distance between two points on a manifold. Understand how a Riemannian metric provides a natural isomorphism between the tangent and cotangent bundles, and how this corresponds to ""raising"" and ""lowering"" indices. Be aware that every differentiable manifold has a Riemannian metric","tensor fields on manifolds,inner product,tangent bundle,cotangent bundle"
Russell's Paradox,"Logic,Set Theory",https://metacademy.org/graphs/concepts/russells_paradox,"Consider the set S of all sets which are not members of themselves: is S a member of itself? Either answer leads to a contradiction. Russell's Paradox, as this is called, showed that Cantor's original formulation of set theory was inconsistent and led to more precise axiomatizations of set theory, such as the Zermelo-Frankl axioms.",Know what Russell's Paradox refers to and why it is a contradiction in naive set theory. Know what is meant by the (unrestricted) Axiom of Comprehension and why it is responsible for the paradox,"set operations,first-order logic"
sampling from a Gaussian,Probability Theory,https://metacademy.org/graphs/concepts/sampling_gaussian,"The transformation method can't be applied directly to sample from a Gaussian, since there's no closed form for the CDF. However, we can apply it using a simple trick.","Know the Box-Muller trick for sampling from a Gaussian distribution, and understand why it works","Gaussian distribution,transformation method,multivariate Gaussian distribution"
search problems,Symbolic AI,https://metacademy.org/graphs/concepts/search_problems,"Many AI tasks can be formulated as search problems, which are defined in terms of an idealized ""state"" which encapsulates all the relevant facts about the world. The agent can apply various actions which modify the state, and the objective is to find a sequence of actions which reach a ""goal state"" which satisfies certain conditions. Sometimes each operator is assigned a cost, and the objective is to find the sequence with lowest cost.","Know some terminology: initial state, action, successor function, state space, path, goal test, step cost, solution, optimal solution. Be able to formulate simple puzzles as search problems",
second derivative test,"Multivariate Calculus,Optimization",https://metacademy.org/graphs/concepts/second_derivative_test,"In an optimization problem, a critical point (where the partial derivatives are zero) may be a local minimum or maximum, or a saddle point. The second derivative test is a way of testing optimality: a point is a (local) minimum if the Hessian matrix is positive definite.","Know the definition of the Hessian matrix. Be able to compute it for simple examples. Know that if the Hessian matrix exists and is positive definite, then a critical point is a local minimum. Why isn't it sufficient for the matrix to be positive semidefinite?","higher-order partial derivatives,optimization problems,positive definite matrices"
semantics of first-order logic,"Logic,Symbolic AI",https://metacademy.org/graphs/concepts/semantics_of_first_order_logic,"The semantics of a first-order language is defined in terms of mathematical structures which give the meanings of all the constants, functions, and predicates in the language. In particular, one can recursively define a function which evaluates, given a structure and a first-order sentence, whether the structure satisfies the sentence. If all structures satisfying a set A of sentences also satisfy another set B of sentences, then A logically implies B.",define what it means for a mathematical structure to satisfy (or be a model of) a set of first-order sentences. define what it means for one set of first-order sentences to logically entail another,"first-order logic,propositional logic,recursion theorem"
sequential Monte Carlo,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/sequential_monte_carlo,"Sequential Monte Carlo is a general framework for Monte Carlo algorithms which involve sampling from a sequence of distributions. It encompasses sequential importance sampling, particle filters, and annealed importance sampling as special cases.",Learn the general formulation of sequential Monte Carlo samplers. Understand why each of the following operators preserves weighted samples:\nimportance weighting\nsampling from the sampling (proposal) distribution\nresampling\nrejuvenation. Know how to use SMC to estimate the normalizing constant of a distribution. What is the optimal sampling (proposal) distribution?\nKnow how this can be approximated using look-ahead,"particle filter,importance sampling,expectation and variance,Markov chain Monte Carlo"
set operations,Set Theory,https://metacademy.org/graphs/concepts/set_operations,"Basic operations on sets include intersection, union, and difference. The power set of a set A is the set of all subsets of A.","Know what it means for two sets to be equal. Know how set union, intersection, and difference are defined mathematically. Know what the power set is. Be able to prove simple statements involving all of these concepts",
simulated annealing,Symbolic AI,https://metacademy.org/graphs/concepts/simulated_annealing,"Simulated annealing is a kind of hill-climbing search, often applied to constraint satisfaction problems (CSPs) or optimization problems. In order to escape local optima, it starts with a ""hotter"" version of the problem, where downhill moves are frequently taken. It gradually cools down the temperature, such that the behavior approaches standard hill-climbing. If the cooling down is done slowly enough, simulated annealing is guaranteed to find the global optimum.","Understand intuitively why simulated annealing can help better explore the search space. Know of at least one specific instance of simulated annealing. Be aware that if the temperature is cooled down slowly enough, simulated annealing finds a global minimum","search problems,local search"
singular value decomposition,Linear Algebra,https://metacademy.org/graphs/concepts/singular_value_decomposition,"The singular value decomposition is a factorization of a matrix A into three matrices UDV^T, where D is diagonal and U and V have orthonormal columns. It's closely related to the eigenvalues and eigenvectors of A^T A and A A^T. It gives a way of analyzing general matrices (not necessarily square) in terms of things somewhat analogous to eigenvalues. Common applications include latent semantic analysis (LSA) and principal component analysis (PCA), a dimensionality reduction algorithm.",Know the definition of the SVD. Know how the SVD of A is related to the eigendecomposition of A^T A and A A^T. Know how the SVD can be used to obtain orthogonal bases for the four fundamental subspaces,"spectral decomposition,matrix transpose,matrix multiplication,orthonormal bases,four fundamental subspaces,positive definite matrices"
slice sampling,Machine Learning,https://metacademy.org/graphs/concepts/slice_sampling,"Slice sampling is a method for sampling from a one-dimensional probability distribution by doing Gibbs sampling in an auxiliary variable model. A major virtue is that it doesn't require specifying a step size. For this reason, it's a useful tool for constructing MCMC samplers which don't require tuning step size parameters.","Know the definition of slice sampling. Know how it can be interpreted as Gibbs sampling in an augmented state space. Know of an efficient method for sampling uniformly over an interval when you can test membership of points, but the endpoints are unknown. Be aware of the motivation of the algorithm in terms of not having to choose step sizes","Metropolis-Hastings algorithm,Gibbs sampling"
soft margin SVM,Machine Learning,https://metacademy.org/graphs/concepts/soft_margin_svm,"The standard SVM objective function, which maximizes the margin, only makes sense when the training set is linearly separable. The soft margin SVM gives more flexibility by allowing some of the training points to be misclassified. In addition to handling non-separable training sets, it also can be more robust to outliers or mislabeled data.",,support vector machine
soft weight sharing in neural nets,Machine Learning,https://metacademy.org/graphs/concepts/soft_weight_sharing_neural_nets,Soft weight sharing is a form of regularization for neural networks where groups of weights are encouraged to have similar values.,,"weight decay in neural networks,mixture of Gaussians models"
softmax regression,Machine Learning,https://metacademy.org/graphs/concepts/multinomial_logistic_regression,"Softmax regression, or multiclass logistic regression, is a generalization of logistic regression to the case where there are more than two categories. The distribution over outputs is given by the softmax function.",Know what the multinomial logistic regression model is. Derive the gradient descent update rule,"logistic regression,maximum likelihood,optimization problems,gradient descent"
solution sets of linear systems,Linear Algebra,https://metacademy.org/graphs/concepts/solution_sets_of_linear_systems,"The set of solutions to Ax = b can be characterized in terms of the column space and nullspace of A. If b is in the column space of A, then a solution exists, otherwise not. The full set of solutions is given by any particular solution x0, plus the nullspace of A.","Show that if there is at least one solution, then the solution space is given in terms of any particular solution and the nullspace","linear systems as matrices,column space and nullspace"
solving difference equations with matrices,Linear Algebra,https://metacademy.org/graphs/concepts/solving_difference_equations_with_matrices,"Difference equations, such as the recurrence formula for the Fibonacci sequence, can be represented as powers of a matrix. If that matrix is diagonalizable, the eigenvalues and eigenvectors yield a closed form solution to the difference equation.",Know how solutions to difference equations can be defined in terms of matrix powers. Be able to solve difference equations using the eigendecomposition in cases where the matrix is diagonalizable,diagonalization
sorting,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/sorting,Sorting refers to rearranging the elements of an array to be ordered according to some criterion. It is commonly used as a running example in introductory algorithms courses because the common sorting algorithms exemplify important algorithm design and analysis techniques.,"Know what sorting refers to. Know a simple sorting algorithm (e.g. bubble sort, insertion sort) and be able to analyze its running time. Be able to implement that algorithm",asymptotic complexity
sparse coding,Machine Learning,https://metacademy.org/graphs/concepts/sparse_coding,"Sparse coding is a probabilistic model of natural images where each region of an image is represented as a linaer combination of a small number of components drawn from a dictionary. When the model is fit to natural images, the dictionary elements resemble the receptive fields of cells in the primary visual cortex.",Know how sparse coding is formulated as a probabilistic model. Formulate maximum likelihood in the sparse coding model as an optimization problem. Know what the results of running sparse coding on natural images look like,"matrix multiplication,maximum likelihood,optimization problems,heavy-tailed distributions"
specifications (programming),Programming,https://metacademy.org/graphs/concepts/specifications_programming,"In programming, a specification of a procedure is an abstract description of the behavior of the procedure. Typically, it would describe requirements for the inputs (preconditions), the effects (postconditions), and any possible side effects. Specifications do not describe the underlying implementation; this ""abstraction barrier"" makes them easier to understand and use, and allows the implementation to change as long as it still meets the specification.","Understand the benefits of procedural abstraction:\nthat one need not understand how the procedure is implemented in order to use it\nit gives more flexibility for changing the implementation, so long as it obeys the specification\nthat the user can replace one implementation with another if it proves to be more effective. Know some terminology: precondition, postcondition, side effect. Know what it means for behavior to be undefined, and why one should not rely on undefined behavior in libraries. Be able to design a specification for a procedure based on the user's anticipated needs (in particular, what should be put in the specification and what should be left to the implementation?)",
spectral decomposition,Linear Algebra,https://metacademy.org/graphs/concepts/spectral_decomposition,"The spectral decomposition is the decomposition of a symmetric matrix A into QDQ^T, where Q is an orthogonal matrix and D is a diagonal matrix. The columns of Q correspond to the eigenvectors of A, and the diagonal entries of D correspond to the eigenvalues. This is possible because of a surprising fact about symmetric matrices: they have a full set of orthogonal eigenvectors. This decomposition gives a useful way to think about symmetric matrices: they are like diagonal matrices in a rotated coordinate system.",Know the statement of the Spectral Theorem for symmetric matrices. Prove the Spectral Theorem in the case where all of the eigenvalues are distinct. Prove the Spectral Theorem in the general case. Understand how the spectral decomposition can be interpreted as a change-of-basis transformation,"orthonormal bases,matrix transpose,eigenvalues and eigenvectors,matrix inverse,change of basis"
stack,"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/stack,A stack is an abstract data type which supports two operations in constant time: pushing (inserting an element on top) and popping (removing the top element). It can be implemented as an array or as a linked list.,Know what a stack is and what operations it must support. Know how a stack can be implemented as an array. Know how a stack can be implemented as a linked list,linked lists
statistical hypothesis testing,Frequentist Statistics,https://metacademy.org/graphs/concepts/statistical_hypothesis_testing,"Statistical hypothesis testing is a method for deciding what conclusions can be drawn from data. A central question is determining whether an outcome is statistically significant, or unlikely to have arisen by chance.","Understand what is required of a hypothesis test in the Neyman-Pearson paradigm. Know basic terminology, including:\nnull hypothesis and alternative hypothesis\ntest statistic\ntype 1 and type 2 errros\nthe power function of a test\nthe level of a test\nsimple and composite hypotheses. Know what is meant by a p-value and how to compute it from the test statistic and its distribution\nIn particular, understand why it doesn't give the probability that a hypothesis is true. Understand why statistical significance doesn't imply that a difference is large in magnitude","probability,random variables,Gaussian distribution,cumulative distribution function"
statistical manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/statistical_manifolds,"A statistical manifold is a differentiable manifold where each point represents a probability distribution. They give a way of talking about intrinsic relationships between probability distributions, i.e. without reference to an arbitrary coordinate system (parameterization).",Define statistical manifold and give some simple examples,"random variables,differentiable manifolds,Gaussian distribution"
Stirling's approximation,Uncategorized,https://metacademy.org/graphs/concepts/stirlings_approximation,Stirling's approximation is a formula for approximating N!. It is derived from the trapezoidal rule for integration.,Know Stirling's approximation. Be able to derive the approximation from the trapezoidal rule for integration,
stochastic gradient descent,Optimization,https://metacademy.org/graphs/concepts/stochastic_gradient_descent,"Stochastic gradient descent (SGD) is an iterative optimization algorithm that can be applied to functions that are a linear combination of differentiable functions. These types of functions often arise when the full objective function is a linear combination of objective functions at each data point, e.g. a least squares objective function. While batch gradient descent uses the full gradient of the function, SGD approximates the full gradient by using the gradient at each of the functions in the linear combination, e.g. the gradient of the objective function at each data point. SGD is often used to optimize non-convex functions, e.g. those that arise in neural networks.",Understand the difference between stochastic gradient descent and batch gradient descent. When does stochastic gradient descent provide a reasonable approximation to the full gradient?. What is the interpretation of the learning rate for stochastic gradient descent compared to full gradient descent?,gradient descent
Stokes' Theorem (three dimensions),Multivariate Calculus,https://metacademy.org/graphs/concepts/stokes_theorem_three_dimensions,Stokes' Theorem is a theorem relating a line integral along the boundary of a surface to the integral of curl over the surface. It can be seen as a three-dimensional generalization of Green's Theorem.,Know the definition of curl in three dimensions. Know the statement of Stokes' Theorem (in three dimensions). Prove Stokes' Theorem (in three dimensions). Be able to use Stokes' Theorem to compute line integrals and surface integrals,"surface integrals,line integrals,Greens Theorem,determinant"
strong law of large numbers,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/strong_law_of_large_numbers,"Roughly, the laws of large numbers state that the average of a large number of draws of a random variable approaches the expectation. The strong law states that the probability that the average of the sequence fails to converge to the expectation is zero. This is a strictly stronger statement than the weak law, but requires stronger assumptions.",Know the statement of the Strong Law of Large Numbers. Understand why it is stronger than the Weak Law (almost sure convergence implies convergence in probability). Prove the Strong Law,"expectation and variance,independent random variables,weak law of large numbers"
strongly connected components,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/strongly_connected_components,The strongly connected components of a graph G are maximal subgraphs S of G such that there is a directed path from any node of S to any other node of S. The graph of strongly connected components is a directed acyclic graph (DAG). The strongly connected components can be computed using depth-first search.,Know what strongly connected components refers to. Understand why the graph of strongly connected components is a DAG. Know how the strongly connected components can be found using two depth-first searches. Prove the correctness of this algorithm,"graph representations,depth-first search"
structural induction,"Logic,Set Theory",https://metacademy.org/graphs/concepts/structural_induction,"Many mathematical objects, such as natural numbers, lists, and trees, are defined recursively in terms of basic components and composition rules. Structural induction is a technique for proving statements inductively about recursively defined structures. One proves base cases corresponding to the base objects, as well as inductive steps corresponding to each of the composition rules.","be able to define a mathematical structure (e.g. the natural numbers) in terms of initial elements and operators. know the Induction Principle, which lets us perform mathematical induction on such structures. prove the correctness of the Induction Principle",
structured mean field,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/structured_mean_field,"The naive mean field approximation assumes a fully factorized approximating distribution, which can be inaccurate if variables are tightly coupled. Structured mean field instead assumes the distribution factorizes into a product of tractable distributions, such as trees or chains.",,"mean field approximation,inference in MRFs,sum-product on trees"
Student-t distribution,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/student_t_distribution,The student-t distribution is a continuous probability distribution motivated by estimating the mean of a Gaussian population with unknown variance.,"Understand how the student-t distribution is defined in terms of samples drawn from a normal distribution.. Know how the shape of the distribution depends on the degrees of freedom (few degrees of freedom gives a heavy-tailed distribution, many degrees of freedom means approximately normal)",Gaussian distribution
subspaces,Linear Algebra,https://metacademy.org/graphs/concepts/subspaces,A subspace is a subset of a vector space which is itself a vector space. Examples include spans of sets of vectors and solution sets to linear equations of the form Ax = 0.,Know the definition of a subspace. Give some examples of subspaces and of spaces which aren't subspaces. Know that subspaces can be represented as spans of vectors,vectors
sufficient statistics,Frequentist Statistics,https://metacademy.org/graphs/concepts/sufficient_statistics,Sufficient statistics are statistics which summarize all of the information a dataset contains about the parameters of a distribution. The Rao-Blackwell Theorem implies that statistical estimators should depend only on sufficient statistics when they exist.,"Know the definition of a sufficient statistic. Derive an equivalent criterion in terms of a factorization of the distribution. Prove the Rao-Blackwell Theorem, which implies that estimators should be based on sufficient statistics when the exist.\nNote: the general form of the Rao-Blackwell Theorem, which applies to convex loss functions, depends on Jensen's inequality , but many texts give the special case for squared error.","random variables,conditional distributions"
sum-product on trees,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/sum_product_on_trees,"Sum-product is an algorithm for marginalization and partition function computation in graphical models. It is based on dynamic programming, and has the advantage that it reuses computations to compute marginals for all nodes in the graph. It is a generalization of the forward-backward algorithm for hidden Markov models.",,"inference in MRFs,factor graphs"
support vector machine,Machine Learning,https://metacademy.org/graphs/concepts/support_vector_machine,"The support vector machine (SVM) is a classification algorithm which tries to fit a hyperplane which maximizes the margin, or the smallest distance separating an example from the decision boundary. The main advantage is that SVMs can be kernelized, allowing them to represent complex nonlinear decision boundaries. Conveniently, the kernelized representation only requires explicitly computing kernels with a small fraction of the data points.",,"binary linear classifiers,convex optimization"
support vector regression,Machine Learning,https://metacademy.org/graphs/concepts/support_vector_regression,"Support vector regression is an analogue of the support vector machine (SVM) which is used for regression rather than classification. The loss function is hinge loss, which ignores small errors and penalizes errors linearly beyond some margin. Like with SVMs, the main selling point is the ability to represent complex nonlinear decision boundaries in terms of kernels with a small number of training examples.",,"support vector machine,linear regression,SVM optimality conditions"
surface integrals,Multivariate Calculus,https://metacademy.org/graphs/concepts/surface_integrals,"A surface integral is the integral of a function over a surface. Important cases include surface area and flux, where the function is the dot product of the surface normal with a vector field.","Know the definition of a surface integral in three dimensions.. Be able to integrate a function over a surface in three dimensions.. As a special case, be able to compute surface area.. Be able to compute the flux across a surface in three dimensions.. Derive the formula for a surface integral for a surface given as z = f(x, y).","vector fields,multiple integrals,cross product,dot product,partial derivatives"
SVM optimality conditions,Machine Learning,https://metacademy.org/graphs/concepts/svm_optimality_conditions,"Using Lagrange duality, we can formulate a set of conditions that characterize the optimal solution to the SVM objective. These conditions show that the weight vector is a linear combination of a (hopefully small) subset of the training points, those for which the margin constraint is tight.",,"support vector machine,Lagrange duality,KKT conditions"
SVM vs. logistic regression,Machine Learning,https://metacademy.org/graphs/concepts/svm_vs_logistic_regression,"Logistic regression and SVMs are closely related algorithms, even though this isn't obvious from the usual presentation. In particular, the loss functions are very similar, and linear SVMs and logistic regression can often be substituted for one another without a big difference in performance.",,"support vector machine,logistic regression"
Swedsen-Wang algorithm,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/swedsen_wang,"The Swedsen-Wang algorithm is an MCMC algorithm for sampling from Ising models. It is an auxiliary variable model, where we define a set of ""bond"" variables which determine which states are coupled, and we alternate between sampling the states and the bond variables. It mixes much faster than Gibbs sampling in models where the variables are tightly coupled.",Know the steps of the Swedsen-Wang algorithm. Know that it can be seen as MCMC in an auxiliary variable model. Why does it improve on naive Gibbs sampling when the variables are tightly coupled?,"Gibbs sampling,Markov random fields"
symplectic manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/symplectic_manifolds,"A symplectic manifold is a (differentiable) manifold endowed with a symplectic structure, or a closed, nondegenerate 2-form. The cotangent bundle of a manifold can be given a natural symplectic structure.","Know the definition of a symplectic manifold. Define the canonical symplectic structure on the cotangent bundle. Be aware of Darboux's Theorem, which shows that any symplectic manifold can be written in a standard form (the ""Darboux coordinates"")","cotangent bundle,differential forms,exterior derivative,bases,orthogonal subspaces"
tangent bundle,Differential Geometry,https://metacademy.org/graphs/concepts/tangent_bundle,"The tangent bundle of a differentiable manifold is the collection of tangent spaces at all points of the manifold. There are several equivalent definitions of tangent vectors, the elements of tangent spaces: directional derivatives, vectors in coordinate charts, and equivalence classes of curves. Pushforward is an operation which carries tangent vectors over a map between manifolds.","Define tangent vectors as directional derivative operators, and show that this constitutes a vector space.. Define tangent vectors in terms of coordinate vectors in a coordinate chart.. Define tangent vectors as equivalence classes of curves. Understand why these three definitions are equivalent, and be able to convert between the corresponding representations. Understand why the partial derivative operators in a coordinate chart give a basis for the tangent space, and be able to compute the coefficients of a vector in this basis. Know what the pushforward operator is, and be able to compute it given the coordinate representations of maps and vectors. Be able to convert between different coordinate representations of tangent vectors. Know what the tangent bundle refers to, and what structure it provides beyond simply an unrelated collection of tangent spaces. Be familiar with tangent bundle notation and terminology (e.g. base space, projection map, fibre). Define what it means for a vector field on a manifold to be smooth","differentiable manifolds,differentiable maps between manifolds,vector spaces,linear approximation,partial derivatives,bases,Chain Rule,change of basis,topology of R^n"
tangent propagation,Machine Learning,https://metacademy.org/graphs/concepts/tangent_propagation,Tangent propagation is a way of regularizing neural nets. It encourages the representation to be invariant by penalizing large changes in the representation when small transformations are applied to the inputs.,,"backpropagation,learning invariances in neural nets"
tensor fields on manifolds,Differential Geometry,https://metacademy.org/graphs/concepts/tensor_fields_on_manifolds,"A tensor field on a manifold M assigns to each point p of M a tensor defined on the tangent space at p. Important examples of tensor fields include covector fields, Riemannian metrics, differential forms, and symplectic forms.","Define tensor field and understand why it is a generalization of covector fields. Be able to compute the coordinate representation of a tensor field given a coordinate chart. Be able to compute the pullback of a tensor field. Be able to convert the coordinate representation of a tensor field to a different coordinate chart. Define contravariant tensor field, and understand why a vector field can be regarded as one","cotangent bundle,tangent bundle,tensors,bases,change of basis,differentiable maps between manifolds"
topological sort,Data Structures & Algorithms,https://metacademy.org/graphs/concepts/topological_sort,"A topological sort is an ordering of the nodes in a directed acyclic graph (DAG) such that if there is an edge A -> B in the graph, then A comes before B in the ordering. This can be used to determine a sequence of jobs which respects a prerequisite graph. The learning plans in Metacademy are generated using a topological sort.",Know what a directed acyclic graph is. Know what a topological sort is. Know how a topological sort can be computed using depth-first search. Prove the correctness and analyze the running time of this algorithm,"depth-first search,graph representations"
topology of R^n,Uncategorized,https://metacademy.org/graphs/concepts/topology_of_rn,"This node covers basic concepts of point set topology, such as limit points and open and closed sets.","Know some important definitions: ball, open interval, closed interval, convergent sequence, open set, closed set, closure. Show that the complement of an open set is closed, and vice versa",vectors
transformation method,Probability Theory,https://metacademy.org/graphs/concepts/transformation_method,The transformation method is a way of sampling from univariate probability distributions by sampling a uniform random variable and inverting the CDF.,Know what the transformation method is. Understand why it works,"cumulative distribution function,Monte Carlo estimation"
tree (data structure),"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/tree_data_structure,"A tree is one of the fundamental data structures in computer science. It consists of a set of nodes, each of which may have pointers to one or more child nodes. Binary trees are an important special case.","Know some terminology: node, edge, child node, parent node, leaf, internal node, root node, height. Know the difference between preorder, postorder and in-order traversal. Given a tree structure, be able to implement any of the traversal algorithms","C pointers,recursion (programming)"
trie (data structure),"Data Structures & Algorithms,Programming",https://metacademy.org/graphs/concepts/trie_data_structure,"A trie (pronounced ""try"") is a data structure which indexes strings by their prefixes. It can be used to efficiently implement autocomplete.","Know what a trie is and what invariants it maintains. Be able to implement insertion, deletion, and search operations. Be able to return the set of stored strings with a given prefix",binary search trees
truncated Newton,Optimization,https://metacademy.org/graphs/concepts/truncated_newton,"The truncated Newton method (also called inexact Newton, or Newton-CG), is a quasi-Newton optimization method. It works by approximately solving the Newton system in each iteration using a small number of conjugate gradient steps. It is memory efficient, as it can be implemented with implicit Hessian-vector products without having to construct the full Hessian matrix.","Know what the truncated Newton method is. What are the pitfalls of running too many or too few CG steps in each update?. Know of at least one method for computing Hessian-vector products (e.g. finite differences, automatic differentiation). Why is preconditioning important for good performance? (In particular, is it needed for the Newton or the CG aspect of the algorithm?)","preconditioned conjugate gradient,conjugate gradient,Newtons method (optimization)"
trust regions,Optimization,https://metacademy.org/graphs/concepts/trust_regions,"Trust region methods are a way of damping the updates in second-order optimization algorithms, based on the intuition that the curvature information is untrustworthy outside of a certain radius from the current point. They work by (perhaps approximately) minimizing the quadratic objective function subject to a norm constraint.",Know the optimization problem defined by the idealized trust region method. Derive the optimality conditions for this optimization problem. Know of a heuristic for adapting the radius of the trust region. Know at least one method for approximately solving the trust region optimization problem,"Newtons method (optimization),Lagrange multipliers"
Turing machines,"Logic,Theory of Computation",https://metacademy.org/graphs/concepts/turing_machines,"Turing machines are a simple theoretical model of computation involving a head which reads and writes symbols on a tape. Despite its simplicity, if the Church-Turing Thesis is true, then anything which can be computed, can be computed by a Turing machine. They are a fundamental construct in reasoning about the limits of computation.",Define a Turing machine. Be able to write Turing machines to solve simple problems. Be aware of universal Turing machines. Be aware of the Church-Turing thesis,
ultraproduct,"Logic,Set Theory",https://metacademy.org/graphs/concepts/ultraproduct,"The ultraproduct is an operation which combines a set of first-order structures into a single structure, in a way that respects the semantics of the individuals structures. This construction is used to give a purely semantic proof of the Compactness Theorem for first-order logic. It also gives a method for constructing non-standard models of first-order theories such as Peano arithmetic.","Define the ultraproduct of a set of first-order structures. Know the Fundamental Theorem of Ultraproducts, which shows that the ultraproduct construction is consistent. Prove the Fundamental Theorem of Ultraproducts. Using the ultraproduct construction, prove the Compactness Theorem of first-order logic.","semantics of first-order logic,equivalence relations,ultrafilters,structural induction,countable sets"
undefinability of truth,Logic,https://metacademy.org/graphs/concepts/undefinability_of_truth,"Tarski's Theorem states that no first-order logical theory has a predicate defining the Godel numbers of statements which are true of the natural numbers. In other words, a first-order theory which includes arithmetic can't define its own truth predicate. This is a fundamental limitation on mathematics, because it implies no formal system is powerful enough to define its own semantics.",Know the statement of Tarski's Theorem and why this implies a formal language can't define its own truth predicate. Prove Tarski's Theorem,"semantics of first-order logic,Godel numbering,representability in arithmetic"
uninformative priors,Bayesian Statistics,https://metacademy.org/graphs/concepts/uninformative_priors,"In Bayesian parameter estimation, uninformative priors are a way of making minimal assumptions about the model. They are commonly chosen to be invariant to certain transformations, such as translation or scaling. While uninformative priors are often improper, they can still lead to proper posterior distributions, and thereby be usable in posterior inference.",Know what uninformative priors are and why we care about them. Give an example where the uniform distribution is not a good uninformative prior. Be able to choose uninformative priors based on invariance arguments. Know what improper priors are and why they often still produce normalizable posteriors. Know what are good uninformative priors for location and scale parameters,"Bayesian parameter estimation,Gaussian distribution,gamma distribution"
uninformed search,Symbolic AI,https://metacademy.org/graphs/concepts/uninformed_search,"Uninformed search refers to a class of highly generic search strategies which use no problem-specific structure (other than the ability to generate successor states and evaluate costs and goal conditions). Examples include depth-first, breadth-first, and uniform cost search. These algorithms aren't very effective for complex tasks, but they're good enough for some small problems.","Know the distinction between informed and uninformed search. Know some terminology: search tree, search node, expand, fringe. Know what the search tree represents, i.e. what the nodes and edges correspond to. Know what the branching factor refers to and why it's a rough measure of problem complexity. Know what breadth-first, uniform-cost, and depth-first search refer to. Be able to implement these algorithms. Analyze the complexity of these algorithms as a function of the branching factor and the search depth. Understand the tradeoffs between these algorithms in terms of time, space, and optimality","search problems,depth-first search,breadth-first search,heap (data structure),recursive backtracking,asymptotic complexity"
unions of events,Probability Theory,https://metacademy.org/graphs/concepts/unions_of_events,It is possible to compute or bound the probability of a union of two events in terms of the probabilities of each one separately and of their intersection. The formula can be generalized to unions of more than two events.,Know the general formula for the probability of a union of events (not necessarily independent). Derive this formula from the probability axioms. Know that the probability of a union of events is bounded by the sum of the probabilities (this is a surprisingly useful fact!),probability
unit testing,Programming,https://metacademy.org/graphs/concepts/unit_testing,"In programming, a unit test is a test of a single function or data type in isolation from the rest of the system. Most modern high-level programming languages have unit testing frameworks which allow large numbers of unit tests to be run quickly and automatically.","Know what unit testing refers to (e.g. in contrast with integration testing and regression testing). Understand why it's advantageous for unit tests to be fast and to test only a single behavior. Be able to design unit tests based on the specification of a procedure. Be able to use the unit testing framework for a programming language such as Java, Python, or Ruby. Know what test-driven development refers to, and why it can be beneficial (compared to writing tests at the very end). Know what code coverage refers to and why it's important, and know of a tool for measuring it. Know the distinction between black box testing and glass box testing, and why each can be advantageous",specifications (programming)
unitary matrices,Linear Algebra,https://metacademy.org/graphs/concepts/unitary_matrices,Unitary matrices are the complex analogues of orthogonal matrices.,Know the definition of a unitary matrix. Understand how they are the complex analogue of orthogonal matrices. Show that multiplication by unitary matrices preserves the norm of a complex vector. Characterize unitary matrices in terms of their eigendecomposition,"eigenvalues and eigenvectors,orthonormal bases,complex vectors and matrices"
unsupervised pre-training,Machine Learning,https://metacademy.org/graphs/concepts/unsupervised_pre_training,"Training deep feed-forward neural networks can be difficult because of local optima in the objective function and because complex models are prone to overfitting. Unsupervised pre-training initializes a discriminative neural net from one which was trained using an unsupervised criterion, such as a deep belief network or a deep autoencoder. This method can sometimes help with both the optimization and the overfitting issues.",Understand why training a deep neural network discriminatively with backpropagation is difficult.. Know how a DBN (or a deep autoencoder) can be converted to a discriminative neural net.. Understand the justifications of generative pre-training: that it is supposed to find better local optima and prevent overfitting.\nWhat evidence supports these claims?,"feed-forward neural nets,deep belief networks,backpropagation"
value iteration,Reinforcement Learning,https://metacademy.org/graphs/concepts/value_iteration,"Value iteration is a recursive algorithm for computing the value function, and in turn the optimal policy, for a Markov decision process.",,"Markov decision process (MDP),Bellman equations"
variable elimination,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/variable_elimination,"Variable elimination is a simple algorithm for marginalization and partition function computation in graphical models. It is based on interchanging sums and products in the definitions of marginals or partition functions. While it produces exact answers, the complexity blows up exponentially in the worst case.",Does the order of elimination matter for directed graphical models? Undirected graphical models?. Is it usually necessary to keep track of the partition function when performing variable elimination in an undirected graphical model?,"Markov random fields,Bayesian networks"
variational Bayes,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/variational_bayes,"Bayesian parameter estimation often results in an intractable posterior over model parameters. Variational Bayes is an application of variational inference (in particular, mean field) to approximating the marginals over parameters as well as the marginal likelihood.",,"Bayesian parameter estimation,Bayesian model comparison,mean field approximation"
variational Bayes EM,Machine Learning,https://metacademy.org/graphs/concepts/variational_bayes_em,"Variational Bayes EM is the application of variational Bayes to latent variable models. In the approximating distribution, the latent variables and parameters are independent, and often there are additional variational approximations within either the latent variables or the parameters.",Know what objective function variational Bayes EM is optimizing. Be able to derive the update rules for the parameters and latent variables in a given probabilistic model. Why does the variational objective give a lower bound on the marginal likelihood?,"variational interpretation of EM,variational Bayes,Expectation-Maximization algorithm"
variational inference,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/variational_inference,"In most probabilistic models of interest, it's intractable to compute posterior marginals and/or normalizing constants exactly. Variational inference is a framework for approximating both. Variational inference treats inference as an optimization problem: we're trying to find a distribution (or a representation resembling a distribution) which is as close as possible to the true posterior, according to some measure.",,"multivariate distributions,KL divergence,entropy,Lagrange multipliers"
variational inference and exponential families,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/variational_exponential_family,Variational inference algorithms based on mean field turn out to have especially nice forms in exponential family models with appropriate conjugate structure.,,"mean field approximation,exponential families"
variational interpretation of EM,Probabilistic Graphical Models,https://metacademy.org/graphs/concepts/em_variational_interpretation,The expectation-maximization (EM) algorithm can be interpreted as a coordinate ascent procedure which optimizes a variational lower bound on the likelihood function. This connects it with variational inference algorithms and justifies various generalizations and approximations to the algorithm.,Show that each step of EM increases a lower bound on the likelihood. Why is the bound tight after each E step in a mixture model?,"Jensens inequality,maximum likelihood,KL divergence,Expectation-Maximization algorithm,optimization problems"
variational linear regression,Machine Learning,https://metacademy.org/graphs/concepts/variational_linear_regression,Variational linear regression is an illustrative example of variational Bayes.,Derive the variational update rules and the lower bound on the marginal likelihood,"Bayesian linear regression,variational Bayes"
variational logistic regression,Machine Learning,https://metacademy.org/graphs/concepts/variational_logistic_regression,"Variational logistic regression is an illustrative example of variational Bayes. It illustrates the use of a local variational approximation, in particular the Gaussian lower bound on the sigmoid function.",Know about the Gaussian lower bound on the sigmoid function. Derive the update rules and the lower bound on the marginal likelihood,"Bayesian logistic regression,variational Bayes"
variational mixture of Gaussians,Machine Learning,https://metacademy.org/graphs/concepts/variational_mixture_of_gaussians,"Variational Bayes EM can be applied to fitting a mixture of Gaussians model. Unlike standard mixture of Gaussians fit with EM, the variational algorithm automatically controls the model complexity and yields a lower bound on the marginal likelihood.",,"variational Bayes,mixture of Gaussians models,Bayesian parameter estimation: multivariate Gaussians,Bayesian parameter estimation: multinomial distribution"
VC dimension,"Frequentist Statistics,Machine Learning",https://metacademy.org/graphs/concepts/vc_dimension,"In statistical learning theory, VC dimension is a measure of the complexity of a continuous hypothesis class (such as linear classifiers). While it often corresponds to the number of parameters in a model, this isn't always the case. There is a general bound on the generalization error of a classifier in terms of its error on a training set and its VC dimension.",Know the definition of VC dimension.. What is the VC dimension of a linear classifier in D dimensions?. Give an example of a hypothesis class whose VC dimension is different from the number of parameters. (Hint: you can find one with a single parameter but infinite VC dimension.),binary linear classifiers
vector fields,Multivariate Calculus,https://metacademy.org/graphs/concepts/vector_fields,"A vector field is a function associating a vector with each point. Common examples include flow fields, force fields, and gradients of functions.","Understand what a vector field is. Know some common examples (gradients of functions, force fields, velocity fields). Be able to visualize vector fields graphically","functions of several variables,vectors"
vector spaces,Linear Algebra,https://metacademy.org/graphs/concepts/vector_spaces,"Vector spaces are spaces which are closed under addition and scalar multiplication. Examples include vectors, matrices, polynomials, and functions. Many core concepts of linear algebra, such as linear independence, bases, and dimension, are defined for general vector spaces.","Know what properties a vector space must satisfy. Know some important examples of vector spaces (e.g. vectors, matrices, polynomials, functions). Show that vector spaces have a unique additive identity and unique additive inverses",vectors
vectors,"Linear Algebra,Multivariate Calculus",https://metacademy.org/graphs/concepts/vectors,"A vector is is a fundamental mathematical structure that is characterized by both a direction (ordering) and a magnitude. For instance, wind has both a direction (North, South-West, etc) and a magnitude (10 km/hour) and could be represented as a vector (10 km/hour South-West). A point in Euclidean space is often represented as a vector of its coordinates and is the most common type of vector encountered. More generally, a vector is an element of a vector space -- a set that is closed under scalar multiplication and vector addition. [additional note: a vector is a very general entity that takes on many forms depending on its context, for instance, in certain vector spaces a vector could be a function such as f(x) = sin(x)]","Be able to compute various operations on vectors: addition, scalar multiplication, and linear combination. Be familiar with the geometric representation of vectors as points and arrows. Know the definition of the span of a set of vectors, and its geometric interpretation in terms of lines and planes",
Viterbi algorithm,"Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/viterbi_algorithm,The Viterbi algorithm is an algorithm for finding the most likely state sequence in the posterior for an HMM. It is based on dynamic programming and has linear time complexity in the length of the sequence.,Know the statement of the Viterbi algorithm. Understand why it is correct. Be familiar with the trellis representation of the algorithm. Analyze the computational complexity. In what situations might the marginals and the MAP solution be quite different?,"hidden Markov models,forward-backward algorithm"
weak law of large numbers,Probability Theory,https://metacademy.org/graphs/concepts/weak_law_of_large_numbers,"Roughly, the laws of large numbers state that if a random variable is sampled many times, the average of all the values approaches the expectation. In particular, the weak law states that the probability of the average of n trials differing by more than some value epsilon goes to zero as n goes to infinity. Unlike the strong law, it only requires that the variables be uncorrelated, not necessarily independent.",Know the definition of convergence in probability. Know the statement of the Weak Law of Large Numbers. Prove the Weak Law. Be aware of how this result justifies averaging the results over repeated experiments,"Markov and Chebyshev inequalities,expectation and variance,independent random variables"
weight decay in neural networks,Machine Learning,https://metacademy.org/graphs/concepts/weight_decay_neural_networks,"When training neural networks, it is common to use ""weight decay,"" where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large, and can be seen as gradient descent on a quadratic regularization term.",,"backpropagation,ridge regression"
well orderings,Set Theory,https://metacademy.org/graphs/concepts/well_orderings,A total ordering R on a set S is a well ordering if every subset of S has a smallest element. Well orderings are important because one can use a generalization of mathematical induction known as transfinite induction. The canonical example is the ordinal numbers.,Know what it means for a relation to be a well ordering. Know the transfinite induction principle and why it is justified. Be able to define functions using transfinite recursion,"order relations,set operations,natural numbers as sets"
Wishart distribution,Probability Theory,https://metacademy.org/graphs/concepts/wishart_distribution,The Wishart distribution is a distribution over positive semidefinite matrices. It is most often used as the conjugate prior for the precision matrix of a multivariate Gaussian.,Know the PDF of the Wishart distribution (don't bother with the normalizing constant),"expectation and variance,matrix inverse,gamma distribution,determinant,positive definite matrices"
Zermelo-Frankl axioms,"Logic,Set Theory",https://metacademy.org/graphs/concepts/zermelo_frankl_axioms,"The Zermelo-Frankl axioms are the standard formalization of set theory in mathematics. They include the Axiom of Extensionality (two sets are equal if they have the same elements), various comprehension axioms (which enable one to construct sets), and the Axiom of Regularity (which rules out certain pathological sets). Most of mathematics can be derived from these axioms.","Know the Axiom of Extensionality. Know the various comprehension axioms (e.g. pairing, union, power set, subset, replacement, infinity; specific presentations will differ). Know the Axiom of Foundation (or Axiom of Regularity) and what pathological sets it is meant to rule out. Be able to derive simple consequences of the axioms. What is the difference between a set and a class?","set operations,Russells Paradox,first-order logic,natural numbers as sets"
Zorn's Lemma,Set Theory,https://metacademy.org/graphs/concepts/zorns_lemma,"Zorn's Lemma states that if every chain in a partially ordered set S has an upper bound, then S has a maximal element. Zorn's Lemma is equivalent to the Axiom of Choice.","Know the statement of Zorn's Lemma. Be aware that it is equivalent to the Axiom of Choice. Prove Zorn's Lemma, assuming the Axiom of Choice. Prove the Axiom of Choice using Zorn's Lemma. Use Zorn's Lemma to show that every vector space has a basis","functions and relations as sets,Axiom of Choice,bases,order relations"
C generics,Programming,https://metacademy.org/graphs/concepts/c_generics,"The C programming language does not provide templates, which are commonly used for generic container data types in other statically typed languages. However, it is possible to implement generic containers through a clever use of pointers.","Understand why C generics can be advantageous compared with C++ templates: they don't create redundant copies of the code for different element types. Understand what the void* type is, and how to manipulate it. Understand the conventions for implementing functions (e.g. linear search) which operate on generic data. Know how the user can specify a comparator function using function pointers; what interface is typically required for such a function?. Know how memcpy and memmove can be used to move elements around; be able to determine which parts of a data structure are or are not copied using this method. As the user of a generic data structure, be able to reason about where memory does/should get allocated and freed. Be able to implement a generic container class in C","dynamic memory allocation in C,C strings,C pointers,machine representations of integers and characters,function pointers in C,C struct representation,multi-dimensional arrays in C"
C pointers,Programming,https://metacademy.org/graphs/concepts/c_pointers,"Pointers are a data type in C which refer to a location in memory. They are used to build most of the fundamental data structures in C. Pointers are worth understanding even if you're programming in a language other than C, because it gives a language for talking about memory. This is important if you want to understand what's going on beneath the hood in higher level languages.","Know what information is contained in a pointer. Know what the \u0026 and * operators do. Understand what is meant by pointer arithmetic; i.e., be able to determine what memory address a pointer arithmetic expression evaluates to. Understand how arrays in C are represented using pointers. Understand the distinction between passing by reference and passing by value. Know the pitfalls of uninitialized pointers. Know what a segmentation fault refers to",
C strings,Programming,https://metacademy.org/graphs/concepts/c_strings,"In C, strings are represented as arrays of characters, terminated by a special null character. The C standard library provides several functions for manipulating strings. Unfortunately, one needs to be very careful when using these to avoid buffer overflow bugs.","Understand how strings are represented in memory in C. Be familiar with several string processing functions in the C standard library and understand how they are implemented: strcat, strcmp, and strlen, and strcpy.. Understand why naive use of these functions can lead to buffer overflow bugs, and know of strategies for getting around this",C pointers
C struct representation,Programming,https://metacademy.org/graphs/concepts/c_struct_representation,The layout of C structs in memory is defined by the language standard. Understanding this representation can be useful for conserving memory.,Know the rules for how C structs are laid out in memory. Understand why the size of a struct may depend on the order in which the fields are declared,"C pointers,machine representations of integers and characters"
call stack,Programming,https://metacademy.org/graphs/concepts/call_stack,"All modern programming language support procedure calls, and the call stack is the data structure which contains the necessary information for this to happen. It is an important concept in C programming, since local variables are allocated on the stack (rather than the heap).","Know what the stack is. In particular, know what information is maintained in each stack frame.. Know how the stack is modified when a procedure is called, and when the procedure returns. Understand the implications for C memory management: e.g., why arrays declared locally need to be fixed size, why arrays shouldn't be allocated locally if they're needed after the procedure exits","stack,C pointers"
cardinality,Set Theory,https://metacademy.org/graphs/concepts/cardinality,"Cardinality is a way of measuring the size of a set. Two sets have the same cardinality if they are equinumerous, i.e. if there is a bijective mapping between them. A has a larger cardinality than B if there is an injective mapping from B to A. Cardinality gives a precise way to talk about different sizes of infinity.","Define the relations of equinumerosity and dominance, and show that these are equivalence and order relations, respectively. Prove the Schroeder-Bernstein Theorem: that if two sets each dominate each other, then they are equinumerous.. Show that no set is equinumerous with its power set.. Be able to manipulate cardinal numbers algebraically. Be aware of the Continuum Hypothesis","countable sets,equivalence relations,order relations,set operations,natural numbers as sets"
Central Limit Theorem,"Frequentist Statistics,Probability Theory",https://metacademy.org/graphs/concepts/central_limit_theorem,"The Central Limit Theorem states that the sum of a large number of independent, identically distributed random variables is approximately Gaussian. It can be used to approximate the probability that a sum of independent random variables lies within some range, even if the distributions are otherwise hard to work with. This theorem is one of the reasons that Gaussian distributions are so ubiquitous in statistics and probabilistic modeling.",Know the statement of the central limit theorem. Be able to use it to estimate the distribution of a sum of i.i.d. random variables. Prove the theorem,"Gaussian distribution,independent random variables,expectation and variance,moment generating functions"
Chain Rule,Multivariate Calculus,https://metacademy.org/graphs/concepts/chain_rule,The chain rule for partial derivatives gives a way of computing the partial derivatives of compositions of functions in terms of the partial derivatives of the individual functions.,Know the statement of the chain rule (in matrix form). Be able to use it to compute partial derivatives. Understand the justification in terms of linear approximations of functions,"partial derivatives,linear approximation,matrix multiplication"
change of basis,Linear Algebra,https://metacademy.org/graphs/concepts/change_of_basis,Sometimes it's convenient to perform computations in a basis other than the standard one. Change of basis matrices can be used to convert vectors and matrices from one basis to another.,Understand what it means to represent a linear transformation in a basis other than the standard one. Be able to convert matrices and vectors between bases algebraically,"bases,linear transformations as matrices,matrix multiplication,matrix inverse,vector spaces"
Chernoff bounds,Frequentist Statistics,https://metacademy.org/graphs/concepts/chernoff_bounds,"The Chernoff bounds are a way of bounding the probability that a sum of independent random variables takes on extreme values. Compared with Chebyshev's inequality, it requires a stronger assumption (independence), but is a far tighter bound. They are commonly used to analyze randomized algorithms and PAC-learning methods.",Give the general statement of Chernoff bounds in terms of moment generating functions. Derive the bound using Markov's inequality. Be able to use it to bound probabilities of extreme values. When would you use the Chernoff bound vs. the Chebyshev bound?,"independent random variables,moment generating functions,Markov and Chebyshev inequalities"
Chinese restaurant franchise,"Bayesian Statistics,Machine Learning",https://metacademy.org/graphs/concepts/chinese_restaurant_franchise,"The Chinese Restaurant Franchise (CRF) is the predictive process for a hierarchical partitioning (clustering) of grouped data -- it is a generalization of the Chinese Restaurant Process. The CRF can be used to specify a nonparametric distribution on a mixture of mixtures: each grouping of data is a draw from a mixture model, where the mixture components are shared among different groups.",,"latent Dirichlet allocation,CRP clustering"
Chinese restaurant process,"Bayesian Statistics,Machine Learning,Probability Theory",https://metacademy.org/graphs/concepts/chinese_restaurant_process,"The Chinese Restaurant Process (CRP) is a predictive rule that descripes a probability distribution on an unbounded partition (clustering). The CRP is as follows: imagine a chinese restaurant with a countably infinite number of tables, the first customer (datum) walks into a restaurant and sits at a table (cluster), the second customer walks into the restaurant and sits at the first customers table with probability 1/2 and chooses a new table with probability 1/2, the nth customer chooses a previous table with probability proportional to the number of customers at that table and chooses his own table with the remaining probability. Defining the probability from this predictive rule yields a probability distribution on an unbounded clustering.",,"Dirichlet distribution,multinomial distribution,gamma function"
Chow-Liu trees,Machine Learning,https://metacademy.org/graphs/concepts/chow_liu_trees,"While the problem of learning Bayes net structures is intractable in general, there is a polynomial time algorithm for learning the optimal tree-structured graph under various scoring criteria. In particular, it can be formulated as a maximum weight spanning tree problem. The maximum likelihood trees are known as Chow-Liu trees, after their original inventors.","Be able to formulate the problem of learning tree-structured graphical models as a maximum spanning tree problem.\nWhat assumptions does this require on the scoring function?. The algorithm only applies to trees (where a node has at most one parent) rather than polytrees (where a node can have multiple parents). Why does it fail for polytrees?. Why is the maximum likelihood score an acceptable scoring criterion for tree-based networks, unlike for general Bayes nets?. Show that the problem can be equivalently viewed as learning the structure of a tree-structured MRF.","Bayes net structure learning,Markov random fields"
Church-Turing thesis,"Logic,Theory of Computation",https://metacademy.org/graphs/concepts/church_turing_thesis,"The Church-Turing thesis is the hypothesis that any function which can be computed (by any deterministic procedure) can be computed by a Turing machine. Equivalently, it holds that a function is recursive if and only if it is computable. While this thesis is not a precise mathematical statement, and therefore cannot be proved, it is almost universally held to be true.",Know the statement of the Church-Turing thesis. Know why it is not a precise mathematical statement. Show that Turing machines and lambda calculus are equivalent. Show that a function is recursive iff it is computable (by some Turing-complete formalism),"Turing machines,decidability,recursive functions"
classes (programming),Programming,https://metacademy.org/graphs/concepts/classes_programming,"In programming, classes are kind of user-defined type with associated fields and methods. Classes are meant to provide an abstraction barrier, where they present a consistent interface to the client, while the underlying implementation is allowed to vary. They are the fundamental structure in object oriented programming.","Know some basic terminology: classes, instances, attributes (or fields or instance variables), methods, instantiation. Know the distinction between a class and an instance. In a language such as Java, Python, or C++, know the syntax for defining classes, creating instances, accessing fields, etc.. Know what constructors are, how to define them, and when they are called; know about destructors as well, if those are used in the language. Understand the basic idea of information hiding. Know the distinction between public and private fields/methods, if the language provides this; otherwise know the language's conventions for marking things as private. Understand how class methods are like ordinary functions, but with an additional ""this"" argument (which is implicit in some languages). Know the distinction between static methods (or class methods) and instance methods",
collapsed Gibbs sampling,"Bayesian Statistics,Machine Learning,Probabilistic Graphical Models",https://metacademy.org/graphs/concepts/collapsed_gibbs_sampling,"MCMC samplers can often be improved by marginalizing out a subset of the variables in closed form and performing MCMC over the remaining variables. This is more statistically efficient since each particle can cover a larger part of the distribution, and it can also improve mixing by allowing larger jumps.",Be able to derive the update rules for collapsed Gibbs sampling. Be aware of the motivations in terms of:\ngreater statistical efficiency (from the Rao-Blackwell theorem)\nfaster mixing,"Gibbs sampling,MCMC convergence,multivariate distributions"
column space and nullspace,Linear Algebra,https://metacademy.org/graphs/concepts/column_space_and_nullspace,The column space is the subspace spanned by the columns of a matrix A. The nullspace is the set of solutions to Ax = 0. Both subspaces are useful for characterizing the sets of solutions to linear systems.,Know the definitions of column space and null space. Show that the column space and null space are subspaces. Show that Ax = b is solvable iff b is in the column space of A,"subspaces,linear systems as matrices"
commuting vector fields,Differential Geometry,https://metacademy.org/graphs/concepts/commuting_vector_fields,"Two vector fields X and Y on a manifold commute if their Lie bracket is zero. Equivalently, the flows for X and Y commute, i.e. can be applied in either order. A set of n linearly independent vector fields on an n-manifold can be expressed as coordinate derivatives in some coordinate chart if and only if they commute.",Show that the flows for two vector fields commute if and only if the Lie bracket is zero. Show that a set of n linearly independent vector fields on an n-manifold can be expressed as coordinate derivatives in some chart if and only if they commute,"tangent bundle,flows on manifolds,Lie derivatives"
compactness of first-order logic,Logic,https://metacademy.org/graphs/concepts/compactness_of_first_order_logic,"The Compactness Theorem for first-order logic states that for any set of sentences S, if every finite subset of S has a model, then S has a model. This theorem can be used to show that certain sets of structures are not definable in first-order logic; examples include connected graphs and finite sets of unbounded size. It also enables the nonstandard analysis, where the real number line is extended to include infinitesimals.","Prove the Compactness Theorem of first-order logic, assuming the Completeness Theorem. Using the Compactness Theorem, show that any first-order theory with an arbitrarily large finite model has an infinite model as well","semantics of first-order logic,completeness of first-order logic"
compactness of propositional logic,Logic,https://metacademy.org/graphs/concepts/compactness_of_propositional_logic,"The Compactness Theorem of propositional logic states that if a given set of propositional sentences is contradictory, then there is also some finite subset which is contradictory.",state the Compactness Theorem of propositional logic. prove the Compactness Theorem,"propositional logic,countable sets"
comparing Gaussian mixtures and k-means,Machine Learning,https://metacademy.org/graphs/concepts/gaussian_mixtures_vs_k_means,"Gaussian mixture models and K-means are two canonical approaches to clustering, i.e. dividing data points into meaningful groups. This concept node discusses the tradeoffs between them.","Understand how k-means can be interpreted as hard-EM in a Gaussian mixture model. Understand how k-means can be interpreted as a Gaussian mixture model in the small variance limit. Be aware of the difference in motivation between the two methods (GMMs are motivated in terms of density modeling, k-means in terms of vector quantization). Be aware that k-means can be used as an initialization for fitting a GMM","k-means,Expectation-Maximization algorithm,mixture of Gaussians models"
comparing normal populations,Frequentist Statistics,https://metacademy.org/graphs/concepts/comparing_normal_populations,A common task in statistics is to determine whether two normally distributed populations have the same mean. The appropriate test can depend on factors such as the sample size and whether the populations are paired or independent.,"Know what the z-test and t-test are, and how to compute the p-values.\nWhen would you use one rather than the other?. What are paired z-tests and t-tests? Why might they be preferable to ones assuming independent populations?. What is the difference between a one-sided and a two-sided test, and why would you use one over the other?. Be aware that the assumption that the populations are normally distributed is a strong one, and often not justified.","Gaussian distribution,statistical hypothesis testing,Student-t distribution,expectation and variance"
